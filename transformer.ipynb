{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e46c7a-2510-4c90-8ff6-ba37fa9049e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 设置可视化风格\n",
    "plt.style.use('tableau-colorblind10')\n",
    "# 设置字体为SimHei(黑体)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# 解决中文字体下坐标轴负数的负号显示问题\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ad200-cbdb-4f2f-a382-8a688f2b7d60",
   "metadata": {},
   "source": [
    "### 筛选出与肾衰竭有关的代号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286c6f43-fe97-47bb-9fb3-1254b315057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id  hadm_id  icustay_id  heartrate_min  heartrate_max  \\\n",
      "0           3   145834      211552           75.0          168.0   \n",
      "1           3   145834      211552           75.0          168.0   \n",
      "2           3   145834      211552           75.0          168.0   \n",
      "3           3   145834      211552           75.0          168.0   \n",
      "4           3   145834      211552           75.0          168.0   \n",
      "\n",
      "   heartrate_mean  sysbp_min  sysbp_max  sysbp_mean  diasbp_min  ...  \\\n",
      "0      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "1      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "2      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "3      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "4      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "\n",
      "                short_title                         long_title  row_id  \\\n",
      "0  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "1  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "2  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "3  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "4  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "\n",
      "   subject_id  gender        dob        dod  dod_hosp    dod_ssn  expire_flag  \n",
      "0           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "1           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "2           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "3           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "4           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "\n",
      "[5 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "#建立数据库连接\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "#调用游标对象\n",
    "cur = con.cursor()\n",
    "\n",
    "# cur.execute('''SELECT DISTINCT d.row_id, d.subject_id, d.hadm_id, d.seq_num, d.icd9_code, lab.value, lab.valueuom, lab.charttime\n",
    "# FROM diagnoses_icd d\n",
    "# JOIN labevents lab ON d.hadm_id = lab.hadm_id\n",
    "# WHERE (d.icd9_code LIKE '584%' OR d.icd9_code LIKE '586%');''')\n",
    "\n",
    "cur.execute('''select distinct* from vitals_first_day vfd\n",
    "join kdigo_uo ku on ku.icustay_id = vfd.icustay_id\n",
    "join diagnoses_icd d on d.hadm_id=vfd.hadm_id\n",
    "join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "join patients p  on p.subject_id = d.subject_id\n",
    "where (di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%');''')\n",
    "            \n",
    "rows = cur.fetchall()\n",
    "\n",
    "# 获取列名（可选，方便 DataFrame 的列标题）\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "# 将查询结果转换为 DataFrame\n",
    "df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "# 输出查看数据\n",
    "print(df.head())\n",
    "df.to_csv(\"住院第一天的生命体征与aki肾衰竭患者尿量数据.csv\", index=False,header=True)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d288c39-1ea1-4181-94f9-4435b2b2133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "#建立数据库连接\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "#调用游标对象\n",
    "cur = con.cursor()\n",
    "\n",
    "batch_size = 10000\n",
    "offset = 0\n",
    "\n",
    "# 打开文件并写入数据\n",
    "with open(\"CHARTEVENTS.csv\", mode=\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "    first_batch = True  # 是否是第一批数据\n",
    "\n",
    "    while True:\n",
    "        # 分批查询\n",
    "        query = f'''\n",
    "        SELECT DISTINCT \n",
    "            C.charttime, C.itemid, C.value, C.valueuom,\n",
    "            d.row_id, d.icd9_code, d.hadm_id,\n",
    "            p.subject_id, p.gender, p.dob\n",
    "        FROM CHARTEVENTS C\n",
    "        JOIN diagnoses_icd d ON d.hadm_id = C.hadm_id\n",
    "        JOIN patients p ON p.subject_id = d.subject_id\n",
    "        WHERE (d.icd9_code LIKE '584%' OR d.icd9_code LIKE '586%')\n",
    "        LIMIT {batch_size} OFFSET {offset};\n",
    "        '''\n",
    "        \n",
    "        # 执行查询\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        # 如果没有更多数据，退出循环\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        # 获取列名，确保列名与数据列数匹配\n",
    "        column_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "        # 创建 DataFrame\n",
    "        df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "        # 写入 CSV 文件\n",
    "        df.to_csv(file, index=False, header=first_batch, mode=\"a\")\n",
    "        first_batch = False  # 仅第一批数据包含表头\n",
    "\n",
    "        # 更新 offset\n",
    "        offset += batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb4a10-bb78-4aba-91aa-efc7309db41d",
   "metadata": {},
   "source": [
    "### 血气"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48411dcb-9acb-4b95-8e27-21195ec70e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽样结果已保存到 blood_gas_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    bl.subject_id, bl.hadm_id, d.icd9_code, bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, p.gender, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    1 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%';\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    bl.subject_id, bl.hadm_id, d.icd9_code, bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, p.gender, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    0 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%');\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取急性肾衰竭患者数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "    \n",
    "    # 获取非急性肾衰竭患者数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "    \n",
    "    # 随机抽取与急性肾衰竭患者数量相等的非急性肾衰竭数据\n",
    "    sample_negative_df = negative_df.sample(n=len(positive_df), random_state=42)\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, sample_negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"blood_gas_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1abab4-89b6-46b5-9eb5-9efa9b31c0bd",
   "metadata": {},
   "source": [
    "### aki患者第一天实验室数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f5ddac-d59f-4ff0-9acb-bfe0734a5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽样结果已保存到 aki_patients_labs_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    lf.*, kc.*, d.icd9_code, di.long_title, p.gender,\n",
    "    1 AS match_flag\n",
    "FROM labs_first_day lf\n",
    "JOIN kdigo_creatinine kc ON kc.icustay_id = lf.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = lf.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%';\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    lf.*, kc.*, d.icd9_code, di.long_title, p.gender,\n",
    "    0 AS match_flag\n",
    "FROM labs_first_day lf\n",
    "JOIN kdigo_creatinine kc ON kc.icustay_id = lf.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = lf.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%');\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # 获取急性肾衰竭患者数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭患者数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 随机抽取与急性肾衰竭患者数量相等的非急性肾衰竭数据\n",
    "    sample_negative_df = negative_df.sample(n=len(positive_df), random_state=42)\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, sample_negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"aki_patients_labs_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a13397-334e-42c2-aac1-7b8426963c92",
   "metadata": {},
   "source": [
    "### aki患者微生物实验室结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bc7f60-48ef-467b-bb78-2b0e66843954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 20,000 条每类数据已保存到 aki_microbiology_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    mb.*, icd.*, kc.*, d.icd9_code, di.long_title, p.gender,\n",
    "    1 AS match_flag\n",
    "FROM microbiologyevents mb\n",
    "JOIN icustay_detail icd ON icd.hadm_id = mb.hadm_id\n",
    "JOIN kdigo_creatinine kc ON kc.icustay_id = icd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = mb.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    mb.*, icd.*, kc.*, d.icd9_code, di.long_title, p.gender,\n",
    "    0 AS match_flag\n",
    "FROM microbiologyevents mb\n",
    "JOIN icustay_detail icd ON icd.hadm_id = mb.hadm_id\n",
    "JOIN kdigo_creatinine kc ON kc.icustay_id = icd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = mb.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%')\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # 获取急性肾衰竭患者数据（前 20,000 条）\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭患者数据（前 20,000 条）\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"aki_microbiology_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"前 20,000 条每类数据已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc52f2-db2f-4dc8-a6e5-6c1c99f6753a",
   "metadata": {},
   "source": [
    "### aki肾衰竭患者尿量数据+第一天实验室数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6bd4474-faf9-41ee-a192-ee3bccadf72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽样结果已保存到 aki_urine_labs_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句：急性肾衰竭\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    lf.*, ku.*, d.icd9_code, di.long_title, p.gender,\n",
    "    1 AS match_flag\n",
    "FROM labs_first_day lf\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = lf.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = lf.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "# SQL 查询语句：非急性肾衰竭\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    lf.*, ku.*, d.icd9_code, di.long_title, p.gender,\n",
    "    0 AS match_flag\n",
    "FROM labs_first_day lf\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = lf.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = lf.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%')\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取急性肾衰竭患者数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭患者数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"aki_urine_labs_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"随机抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a8850-f24a-47bc-8746-bf3e5e9142c0",
   "metadata": {},
   "source": [
    "### 住院第一天的生命体征与aki肾衰竭患者尿量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d289ed-d604-4bc4-bac8-9d072851a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽样结果已保存到 vitals_aki_urine_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句：急性肾衰竭\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    vfd.*, ku.*, d.icd9_code, di.long_title, p.gender,\n",
    "    1 AS match_flag\n",
    "FROM vitals_first_day vfd\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = vfd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = vfd.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "# SQL 查询语句：非急性肾衰竭\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    vfd.*, ku.*, d.icd9_code, di.long_title, p.gender,\n",
    "    0 AS match_flag\n",
    "FROM vitals_first_day vfd\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = vfd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = vfd.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%')\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取急性肾衰竭患者数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭患者数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"vitals_aki_urine_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"随机抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0129d4-12a9-4a41-9403-3fce062dfd55",
   "metadata": {},
   "source": [
    "### 语言 宗教 婚姻 种族"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b665dba-2533-42a4-bf91-56f68ec8bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽样结果已保存到 language_religion_ethnicity_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句：急性肾衰竭\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    a.language,\n",
    "    a.religion,\n",
    "    a.marital_status,\n",
    "    a.ethnicity,\n",
    "    1 AS match_flag\n",
    "FROM admissions a\n",
    "JOIN diagnoses_icd d ON d.hadm_id = a.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "# SQL 查询语句：非急性肾衰竭\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    a.language,\n",
    "    a.religion,\n",
    "    a.marital_status,\n",
    "    a.ethnicity,\n",
    "    0 AS match_flag\n",
    "FROM admissions a\n",
    "JOIN diagnoses_icd d ON d.hadm_id = a.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%')\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取急性肾衰竭数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存为 CSV 文件\n",
    "    output_file = \"language_religion_ethnicity_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"随机抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976cd0d-36a7-4a7d-afdf-e7200e8aac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句：符合条件的数据\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    bl.subject_id, bl.hadm_id, bl.icustay_id, -- 主键\n",
    "    bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    p.gender,\n",
    "    lf.*, -- 第一日实验室数据\n",
    "    mb.*, -- 微生物事件\n",
    "    rfd.*, -- 血液净化治疗\n",
    "    uof.*, -- 第一日尿液排出量\n",
    "    vfd.*, -- 第一日生命体征\n",
    "    1 AS match_flag -- 匹配标志\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "JOIN icustay_detail icd ON icd.hadm_id = d.hadm_id\n",
    "JOIN kdigo_creatinine kdc ON kdc.icustay_id = icd.icustay_id\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = icd.icustay_id\n",
    "JOIN labs_first_day lf ON lf.icustay_id = icd.icustay_id\n",
    "JOIN microbiologyevents mb ON mb.hadm_id = d.hadm_id\n",
    "JOIN rrt_first_day rfd ON rfd.hadm_id = d.hadm_id\n",
    "JOIN urine_output_first_day uof ON uof.hadm_id = d.hadm_id\n",
    "JOIN vitals_first_day vfd ON vfd.icustay_id = icd.icustay_id\n",
    "WHERE di.long_title ILIKE '%renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 1;\n",
    "'''\n",
    "\n",
    "# SQL 查询语句：不符合条件的数据\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    bl.subject_id, bl.hadm_id, bl.icustay_id, -- 主键\n",
    "    bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    p.gender,\n",
    "    lf.*, -- 第一日实验室数据\n",
    "    mb.*, -- 微生物事件\n",
    "    rfd.*, -- 血液净化治疗\n",
    "    uof.*, -- 第一日尿液排出量\n",
    "    vfd.*, -- 第一日生命体征\n",
    "    0 AS match_flag -- 不匹配标志\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN patients p ON p.subject_id = bl.subject_id\n",
    "LEFT JOIN icustay_detail icd ON icd.hadm_id = bl.hadm_id\n",
    "LEFT JOIN kdigo_creatinine kdc ON kdc.icustay_id = icd.icustay_id\n",
    "LEFT JOIN kdigo_uo ku ON ku.icustay_id = icd.icustay_id\n",
    "LEFT JOIN labs_first_day lf ON lf.icustay_id = icd.icustay_id\n",
    "LEFT JOIN microbiologyevents mb ON mb.hadm_id = bl.hadm_id\n",
    "LEFT JOIN rrt_first_day rfd ON rfd.hadm_id = bl.hadm_id\n",
    "LEFT JOIN urine_output_first_day uof ON uof.hadm_id = bl.hadm_id\n",
    "LEFT JOIN vitals_first_day vfd ON vfd.icustay_id = icd.icustay_id\n",
    "WHERE di.icd9_code IS NULL\n",
    "LIMIT 1;\n",
    "'''\n",
    "\n",
    "output_file = \"aki_sampled_record.csv\"\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 创建一个空的 DataFrame，用于保存记录\n",
    "    columns_written = False\n",
    "    \n",
    "    for query in [query_positive, query_negative]:\n",
    "        cur.execute(query)\n",
    "        row = cur.fetchone()\n",
    "        \n",
    "        if row:\n",
    "            # 获取列名\n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            record_df = pd.DataFrame([row], columns=column_names)\n",
    "\n",
    "            # 如果是首次写入 CSV 文件，包含表头\n",
    "            if not columns_written:\n",
    "                record_df.to_csv(output_file, mode='w', index=False, header=True)\n",
    "                columns_written = True\n",
    "            else:\n",
    "                # 追加到 CSV 文件中，无需表头\n",
    "                record_df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "            \n",
    "            print(f\"已保存一条记录到 {output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e765005a-aa5b-4d34-9016-f4785fe6e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成 \n",
      "SELECT DISTINCT \n",
      "    bl.so2, bl.spo2, bl.po2, bl. 的记录写入\n",
      "完成 \n",
      "SELECT DISTINCT \n",
      "    bl.so2, bl.spo2, bl.po2, bl. 的记录写入\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    p.gender,\n",
    "    rfd.rrt,\n",
    "    uof.urineoutput,\n",
    "    vfd.*,\n",
    "    1 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "JOIN icustay_detail icd ON icd.hadm_id = d.hadm_id\n",
    "JOIN rrt_first_day rfd ON rfd.hadm_id = d.hadm_id\n",
    "JOIN urine_output_first_day uof ON uof.hadm_id = d.hadm_id\n",
    "JOIN vitals_first_day vfd ON vfd.icustay_id = icd.icustay_id\n",
    "WHERE di.long_title ILIKE '%renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 30000;\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    p.gender,\n",
    "    rfd.rrt,\n",
    "    uof.urineoutput,\n",
    "    vfd.*,\n",
    "    0 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN patients p ON p.subject_id = bl.subject_id\n",
    "LEFT JOIN icustay_detail icd ON icd.hadm_id = bl.hadm_id\n",
    "LEFT JOIN rrt_first_day rfd ON rfd.hadm_id = bl.hadm_id\n",
    "LEFT JOIN urine_output_first_day uof ON uof.hadm_id = bl.hadm_id\n",
    "LEFT JOIN vitals_first_day vfd ON vfd.icustay_id = icd.icustay_id\n",
    "WHERE di.icd9_code IS NULL\n",
    "LIMIT 30000;\n",
    "'''\n",
    "\n",
    "output_file = \"aki_data3.csv\"\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    queries = [query_positive, query_negative]\n",
    "\n",
    "    for query in queries:\n",
    "        cur.execute(query)\n",
    "        \n",
    "        # 分批提取数据\n",
    "        while True:\n",
    "            rows = cur.fetchmany(1000)  # 每次提取 1000 行\n",
    "            if not rows:\n",
    "                break\n",
    "            \n",
    "            # 获取列名并创建 DataFrame\n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            batch_df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "            # 去除重复行\n",
    "            batch_df.drop_duplicates(inplace=True)\n",
    "\n",
    "            # 写入 CSV 文件\n",
    "            batch_df.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "            \n",
    "            # 释放内存\n",
    "            del batch_df\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"完成 {query[:50]} 的记录写入\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03353954-1eb3-4bd0-9eda-9546efa60589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成 \n",
      "SELECT DISTINCT \n",
      "    mb.isolate_num, mb.dilution_ 的记录写入\n",
      "完成 \n",
      "SELECT DISTINCT \n",
      "    mb.isolate_num, mb.dilution_ 的记录写入\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    mb.isolate_num, mb.dilution_value, mb.interpretation,\n",
    "    lf.*,\n",
    "    bl.hadm_id,\n",
    "    1 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN labs_first_day lf ON lf.icustay_id = (\n",
    "    SELECT icustay_id\n",
    "    FROM icustay_detail icd\n",
    "    WHERE icd.hadm_id = d.hadm_id\n",
    "    LIMIT 1\n",
    ")\n",
    "JOIN microbiologyevents mb ON mb.hadm_id = d.hadm_id\n",
    "WHERE di.long_title ILIKE '%renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 30000;\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    mb.isolate_num, mb.dilution_value, mb.interpretation,\n",
    "    lf.*,\n",
    "    bl.hadm_id,\n",
    "    0 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN labs_first_day lf ON lf.icustay_id = (\n",
    "    SELECT icustay_id\n",
    "    FROM icustay_detail icd\n",
    "    WHERE icd.hadm_id = bl.hadm_id\n",
    "    LIMIT 1\n",
    ")\n",
    "LEFT JOIN microbiologyevents mb ON mb.hadm_id = bl.hadm_id\n",
    "WHERE di.icd9_code IS NULL\n",
    "LIMIT 30000;\n",
    "'''\n",
    "\n",
    "output_file = \"aki_data4.csv\"\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    queries = [query_positive, query_negative]\n",
    "\n",
    "    for query in queries:\n",
    "        cur.execute(query)\n",
    "        \n",
    "        # 分批提取数据\n",
    "        while True:\n",
    "            rows = cur.fetchmany(1000)  # 每次提取 1000 行\n",
    "            if not rows:\n",
    "                break\n",
    "            \n",
    "            # 获取列名并创建 DataFrame\n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            batch_df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "            # 去除重复行\n",
    "            batch_df.drop_duplicates(inplace=True)\n",
    "\n",
    "            # 写入 CSV 文件\n",
    "            batch_df.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "            \n",
    "            # 释放内存\n",
    "            del batch_df\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"完成 {query[:50]} 的记录写入\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e675d9-18d8-4dbe-90ea-d0ed92b6e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后的数据已保存到 cleaned_aki_data4.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'aki_data4.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'charttime',\n",
    "                     'icd9_code', 'long_title', ]  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "# 2. 去除重复列\n",
    "df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.duplicated()]\n",
    "\n",
    "# 3. 将性别转换为数值类型\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "# 4. 对非文本类型的分类特征进行编码\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# 5. 去除缺失值比例超过 50% 的列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 6. 填充剩余的缺失值\n",
    "imputer = SimpleImputer(strategy='median')  # 使用中位数填充\n",
    "df_cleaned[df_cleaned.columns] = imputer.fit_transform(df_cleaned)\n",
    "\n",
    "# 7. 提取数值特征并标准化\n",
    "features = df_cleaned.drop(columns=['match_flag'])  # 假设 'match_flag' 是标签列\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 进一步归一化到 [0, 1] 范围\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "# 转换为 DataFrame 并添加回标签列\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final['match_flag'] = df_cleaned['match_flag'].values\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_aki_data4.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2173e5e-3e9d-40ab-b9c1-e6255342b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 1991 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 2534 条\n",
      "数据集已保存到 ./data/weight_first_day.csv\n",
      "总样本数：4525（阳性 1991，阴性 2534）\n",
      "清洗后的数据已保存到 ./data/cleaned_weight_first_day.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# 阳性样本查询\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    wfd.weight,\n",
    "    p.gender,\n",
    "    1 AS match_flag\n",
    "FROM weight_first_day wfd\n",
    "JOIN icustays ic ON ic.icustay_id = wfd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = ic.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE (di.long_title ILIKE '% renal fail%' \n",
    "    OR di.long_title ILIKE '%kidney fail%'\n",
    "    OR di.long_title ILIKE '%liver fail%'\n",
    "    OR di.long_title ILIKE '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 优化后的阴性样本查询\n",
    "query_negative = '''\n",
    "SELECT * FROM (\n",
    "    SELECT DISTINCT \n",
    "        wfd.weight,\n",
    "        p.gender,\n",
    "        0 AS match_flag\n",
    "    FROM weight_first_day wfd\n",
    "    JOIN icustays ic ON ic.icustay_id = wfd.icustay_id\n",
    "    JOIN diagnoses_icd d ON d.hadm_id = ic.hadm_id\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    JOIN patients p ON p.subject_id = d.subject_id\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM diagnoses_icd d2\n",
    "        JOIN d_icd_diagnoses di2 ON di2.icd9_code = d2.icd9_code\n",
    "        WHERE d2.hadm_id = d.hadm_id\n",
    "          AND (di2.long_title ILIKE '% renal fail%' \n",
    "              OR di2.long_title ILIKE '%kidney fail%'\n",
    "              OR di2.long_title ILIKE '%liver fail%'\n",
    "              OR di2.long_title ILIKE '%spleen fail%')\n",
    "    )\n",
    ") AS sub\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 保存结果\n",
    "    output_file = \"./data/weight_first_day.csv\"\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"数据集已保存到 {output_file}\")\n",
    "    print(f\"总样本数：{len(combined_df)}（阳性 {len(positive_df)}，阴性 {len(negative_df)}）\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "        \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  # 启用 IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "file_path = './data/weight_first_day.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['row_id', 'subject_id',  'icustay_id', 'charttime',\n",
    "                     'icd9_code', 'long_title', 'subject_id1', 'hadm_id1', 'expire_flag', 'short_title', 'row_id1', 'icd9_code1'\n",
    "                    ,'dod','dob','dod_ssn','dod_hosp']  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "# 2. 去除重复列\n",
    "df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.duplicated()]\n",
    "\n",
    "# 3. 将性别转换为数值类型\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "# 4. 对非文本类型的分类特征进行编码\n",
    "categorical_cols = []  # 记录分类列用于后续处理\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['intime', 'outtime']:  # 跳过时间列\n",
    "        categorical_cols.append(col)\n",
    "        label_encoder = LabelEncoder()\n",
    "        df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# 5. 将时间列转换为Unix时间戳并处理缺失\n",
    "time_cols = ['intime', 'outtime']\n",
    "for col in time_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        # 转换为datetime并处理无效值\n",
    "        df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')\n",
    "        # 转换为Unix时间戳（秒）\n",
    "        df_cleaned[col] = (df_cleaned[col].astype('int64') // 10**9).replace(-9223372036854775808, np.nan)\n",
    "\n",
    "# 6. 删除高缺失率列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 7. 使用XGBoost进行缺失值填补\n",
    "from sklearn.impute import IterativeImputer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 对所有列进行填补（包括时间列和标签列）\n",
    "columns_to_impute = df_cleaned.columns.tolist()\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=100, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df_cleaned[columns_to_impute] = imputer.fit_transform(df_cleaned[columns_to_impute])\n",
    "\n",
    "# 8. 后处理：分类列和标签列取整\n",
    "for col in categorical_cols + ['match_flag']:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].round().astype(int)\n",
    "\n",
    "# 时间列取整（确保为整数时间戳）\n",
    "for col in time_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].round().astype('int64')\n",
    "\n",
    "# 9. 标准化处理（排除指定列）\n",
    "# features = df_cleaned.drop(columns=['match_flag', 'intime', 'outtime',])\n",
    "features = df_cleaned.drop(columns=['match_flag'])\n",
    "# 标准化+归一化\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "# for col in ['intime', 'outtime','match_flag']:\n",
    "for col in ['match_flag']:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_final[col] = df_cleaned[col].values\n",
    "\n",
    "# 保存结果\n",
    "output_file = './data/cleaned_weight_first_day.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6ba7b1-897e-4e2b-ba00-c1b266f03b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 12888 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 12888 条\n",
      "动态保留的列：['hadm_id', 'aniongap_min', 'aniongap_max', 'albumin_min', 'albumin_max', 'bands_min', 'bands_max', 'bicarbonate_min', 'bicarbonate_max', 'bilirubin_min', 'bilirubin_max', 'creatinine_min', 'creatinine_max', 'chloride_min', 'chloride_max', 'glucose_min', 'glucose_max', 'hematocrit_min', 'hematocrit_max', 'hemoglobin_min', 'hemoglobin_max', 'lactate_min', 'lactate_max', 'platelet_min', 'platelet_max', 'potassium_min', 'potassium_max', 'ptt_min', 'ptt_max', 'inr_min', 'inr_max', 'pt_min', 'pt_max', 'sodium_min', 'sodium_max', 'bun_min', 'bun_max', 'wbc_min', 'wbc_max', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/labs_first_day_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 ./data/cleaned_labs_first_day.csv\n",
      "数据特征分布：\n",
      "              hadm_id  aniongap_min  aniongap_max   albumin_min   albumin_max  \\\n",
      "count   24087.000000  24087.000000  24087.000000  24087.000000  24087.000000   \n",
      "mean   149804.298252      0.291386      0.258762      0.337905      0.350923   \n",
      "std     28875.239187      0.081605      0.089616      0.113909      0.117026   \n",
      "min    100001.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    124775.500000      0.232558      0.196429      0.256857      0.264151   \n",
      "50%    149804.000000      0.279070      0.250000      0.322706      0.337007   \n",
      "75%    174792.500000      0.325581      0.285714      0.410794      0.429168   \n",
      "max    199999.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "          bands_min     bands_max  bicarbonate_min  bicarbonate_max  \\\n",
      "count  24087.000000  24087.000000     24087.000000     24087.000000   \n",
      "mean       0.127821      0.137647         0.401015         0.408652   \n",
      "std        0.088750      0.103496         0.103129         0.096324   \n",
      "min        0.000000      0.000000         0.000000         0.000000   \n",
      "25%        0.063157      0.059493         0.340000         0.354167   \n",
      "50%        0.103902      0.105932         0.413780         0.398553   \n",
      "75%        0.164606      0.179331         0.460000         0.458333   \n",
      "max        1.000000      1.000000         1.000000         1.000000   \n",
      "\n",
      "       bilirubin_min  ...        pt_min        pt_max    sodium_min  \\\n",
      "count   24087.000000  ...  24087.000000  24087.000000  24087.000000   \n",
      "mean        0.028126  ...      0.145541      0.156661      0.764243   \n",
      "std         0.043117  ...      0.208303      0.218070      0.030042   \n",
      "min         0.000000  ...      0.000000      0.000000      0.000000   \n",
      "25%         0.010171  ...      0.035915      0.034763      0.751117   \n",
      "50%         0.016942  ...      0.045775      0.048420      0.762430   \n",
      "75%         0.032267  ...      0.100000      0.132223      0.779399   \n",
      "max         1.000000  ...      1.000000      1.000000      1.000000   \n",
      "\n",
      "         sodium_max       bun_min       bun_max       wbc_min       wbc_max  \\\n",
      "count  24087.000000  24087.000000  24087.000000  24087.000000  24087.000000   \n",
      "mean       0.502754      0.109493      0.117097      0.019010      0.016591   \n",
      "std        0.061202      0.091658      0.098549      0.015749      0.015105   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.470588      0.051383      0.051852      0.011812      0.010395   \n",
      "50%        0.505882      0.075099      0.081481      0.016849      0.014292   \n",
      "75%        0.529412      0.138340      0.151852      0.022929      0.019962   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "             gender    match_flag  \n",
      "count  24087.000000  24087.000000  \n",
      "mean       0.563914      0.472537  \n",
      "std        0.495908      0.499256  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      0.000000  \n",
      "50%        1.000000      0.000000  \n",
      "75%        1.000000      1.000000  \n",
      "max        1.000000      1.000000  \n",
      "\n",
      "[8 rows x 41 columns]\n",
      "\n",
      "验证结果：\n",
      "总样本数：24087\n",
      "阳性样本数：11382\n",
      "阴性样本数：12705\n",
      "唯一hadm_id数量：24087\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "SELECT DISTINCT lfd.*,\n",
    "                p.gender,\n",
    "                1 AS match_flag\n",
    "FROM labs_first_day lfd\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = lfd.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE (di.long_title ILIKE '% renal fail%' \n",
    "    OR di.long_title ILIKE '%kidney fail%'\n",
    "    OR di.long_title ILIKE '%liver fail%'\n",
    "    OR di.long_title ILIKE '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%' \n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT \n",
    "    lfd.*,\n",
    "    p.gender,\n",
    "    0 AS match_flag \n",
    "FROM labs_first_day lfd\n",
    "LEFT JOIN patients p ON p.subject_id = lfd.subject_id\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1 \n",
    "    FROM positive_hadm ph \n",
    "    WHERE ph.hadm_id = lfd.hadm_id\n",
    ")\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "'''\n",
    "\n",
    "# ================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "    \n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "    \n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "    \n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "    \n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/labs_first_day_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/labs_first_day_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_labs_first_day.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81288f52-35d1-43b5-a4f2-17dca02b1262",
   "metadata": {},
   "source": [
    "#### LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23229842-6b2f-4ade-bb75-608a5fd0038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 12888 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 12888 条\n",
      "动态保留的列：['hadm_id', 'aniongap_min', 'aniongap_max', 'albumin_min', 'albumin_max', 'bands_min', 'bands_max', 'bicarbonate_min', 'bicarbonate_max', 'bilirubin_min', 'bilirubin_max', 'creatinine_min', 'creatinine_max', 'chloride_min', 'chloride_max', 'glucose_min', 'glucose_max', 'hematocrit_min', 'hematocrit_max', 'hemoglobin_min', 'hemoglobin_max', 'lactate_min', 'lactate_max', 'platelet_min', 'platelet_max', 'potassium_min', 'potassium_max', 'ptt_min', 'ptt_max', 'inr_min', 'inr_max', 'pt_min', 'pt_max', 'sodium_min', 'sodium_max', 'bun_min', 'bun_max', 'wbc_min', 'wbc_max', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/labs_first_day_raw.csv\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5834\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5581\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5581\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5642\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5669\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5582\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5582\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5576\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5576\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5756\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5740\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5750\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5748\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5740\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5745\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5760\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5760\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5762\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5751\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5864\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5864\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5961\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5941\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5910\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5895\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5813\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5815\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5990\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5944\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5850\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5848\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5868\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5811\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6300\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6298\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5912\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5912\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6696\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000846 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6689\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7970\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7660\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7660\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7676\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7703\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7613\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7613\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7557\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7557\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7572\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7556\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7500\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7498\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7420\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7375\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7375\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7192\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7181\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7137\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7137\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7137\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7117\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7085\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7070\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6833\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6835\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7008\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6962\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6871\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6869\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6785\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6728\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7292\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7290\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6399\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6399\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7279\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7272\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8333\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8024\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8024\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8060\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8087\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8007\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8007\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7971\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7971\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7657\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7641\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7578\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7576\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7492\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7497\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7458\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7458\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7278\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7267\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7223\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7223\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7217\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7197\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7167\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7152\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6829\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6831\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7006\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6960\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6865\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6863\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6804\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6747\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7366\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7364\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6421\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6421\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7917\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7910\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8608\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8318\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8318\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8359\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8386\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8296\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8296\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8268\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8268\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7678\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7662\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7598\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7596\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7518\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7523\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7487\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7487\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7306\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7295\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7246\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7246\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7232\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7212\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7186\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7171\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6828\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6830\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7004\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6958\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6865\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6863\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6803\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6746\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7436\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7434\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6432\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6432\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8450\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8443\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8916\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8611\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8611\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8682\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8592\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8592\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8579\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8579\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7693\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7677\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7618\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7616\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7539\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7544\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7506\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7506\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7311\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7258\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7258\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7252\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7232\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7201\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7186\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6827\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6829\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7003\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6957\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6862\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6860\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6806\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6749\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7458\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7456\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6431\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6431\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8617\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8610\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9028\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8728\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8728\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8777\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8804\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8711\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8711\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8701\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8701\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7694\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7678\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7624\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7622\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7533\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7538\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7504\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7504\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7311\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7265\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7265\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7258\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7238\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7210\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7195\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6822\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6824\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7002\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6956\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6861\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6859\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6822\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6765\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7490\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7488\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6442\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6442\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8733\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8726\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9093\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8811\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8811\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8854\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8881\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8795\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8795\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8783\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8783\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7707\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7691\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7629\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7627\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7546\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7551\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7514\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7514\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7333\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7272\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7272\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7258\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7238\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7211\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7196\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6828\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6830\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7003\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6957\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6863\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6861\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001943 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6808\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6751\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7489\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7487\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6434\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6434\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8823\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8816\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9151\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8865\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8865\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8916\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8943\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8856\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8856\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8847\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8847\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7705\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7689\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7630\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7628\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7547\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7552\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7513\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7513\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7333\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7262\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7262\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7253\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7233\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7210\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7195\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6821\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6823\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6996\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6950\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6854\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6852\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6815\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6758\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7517\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7515\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8844\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8837\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9169\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8879\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8879\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8936\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8963\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8878\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8878\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8863\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8863\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7701\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7685\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7622\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7620\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7545\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7550\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7507\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7507\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7329\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7257\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7257\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7255\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7235\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7206\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7191\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6825\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6827\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7004\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6958\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6856\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6854\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6811\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6754\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7534\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7532\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6445\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6445\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8876\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8869\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9186\n",
      "[LightGBM] [Info] Number of data points in the train set: 24117, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.561139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8902\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.947292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8902\n",
      "[LightGBM] [Info] Number of data points in the train set: 23537, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.202256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8958\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.436194\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8985\n",
      "[LightGBM] [Info] Number of data points in the train set: 23465, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.034635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8891\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 210.403420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8891\n",
      "[LightGBM] [Info] Number of data points in the train set: 23452, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 249.966527\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8885\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.018214\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8885\n",
      "[LightGBM] [Info] Number of data points in the train set: 23420, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.191319\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7700\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.847598\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7684\n",
      "[LightGBM] [Info] Number of data points in the train set: 22348, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.771521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7621\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.481135\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7619\n",
      "[LightGBM] [Info] Number of data points in the train set: 22316, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.165204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7540\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.204131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7545\n",
      "[LightGBM] [Info] Number of data points in the train set: 22272, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.901356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7507\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.899180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7507\n",
      "[LightGBM] [Info] Number of data points in the train set: 22203, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.645332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.419281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7315\n",
      "[LightGBM] [Info] Number of data points in the train set: 21959, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.630994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7265\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.473368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7265\n",
      "[LightGBM] [Info] Number of data points in the train set: 21835, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.854820\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7262\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.549888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7242\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.880512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7215\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 29.969478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7200\n",
      "[LightGBM] [Info] Number of data points in the train set: 21820, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.413566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6820\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.434271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6822\n",
      "[LightGBM] [Info] Number of data points in the train set: 19559, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.874769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6996\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.431531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6950\n",
      "[LightGBM] [Info] Number of data points in the train set: 19555, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.773796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6851\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.523256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6849\n",
      "[LightGBM] [Info] Number of data points in the train set: 19463, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.573092\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6822\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.859264\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6765\n",
      "[LightGBM] [Info] Number of data points in the train set: 14014, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.164122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7535\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.094702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7533\n",
      "[LightGBM] [Info] Number of data points in the train set: 11967, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.423431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6447\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.052805\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000921 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6447\n",
      "[LightGBM] [Info] Number of data points in the train set: 9126, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.149627\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8861\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.478425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8854\n",
      "[LightGBM] [Info] Number of data points in the train set: 4394, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.230906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "插值模型已保存！\n",
      "标准化模型已保存至 ./models/labs_first_day_lgbm_standard_scaler.pkl\n",
      "处理后的数据已保存到 ./data/cleaned_labs_first_day_lgbm.csv\n",
      "数据特征分布：\n",
      "              hadm_id  aniongap_min  aniongap_max   albumin_min   albumin_max  \\\n",
      "count   24117.000000  2.411700e+04  2.411700e+04  2.411700e+04  2.411700e+04   \n",
      "mean   149951.696189 -3.016941e-16 -1.508471e-16  4.336853e-16  5.091089e-16   \n",
      "std     28840.616513  1.000021e+00  1.000021e+00  1.000021e+00  1.000021e+00   \n",
      "min    100001.000000 -3.581268e+00 -2.310412e+00 -4.306053e+00 -4.537382e+00   \n",
      "25%    125056.000000 -7.356458e-01 -7.130917e-01 -4.457192e-01 -4.420069e-01   \n",
      "50%    149947.000000 -1.665215e-01 -1.140965e-01  2.897292e-02  3.738769e-03   \n",
      "75%    174893.000000  4.811374e-01  3.527051e-01  6.074105e-01  6.084877e-01   \n",
      "max    199999.000000  8.654906e+00  8.471500e+00  5.177099e+00  5.291518e+00   \n",
      "\n",
      "          bands_min     bands_max  bicarbonate_min  bicarbonate_max  \\\n",
      "count  2.411700e+04  2.411700e+04     2.411700e+04     2.411700e+04   \n",
      "mean  -9.899339e-17 -9.427942e-18    -2.168427e-16     4.855390e-16   \n",
      "std    1.000021e+00  1.000021e+00     1.000021e+00     1.000021e+00   \n",
      "min   -1.580355e+00 -1.506717e+00    -3.680489e+00    -4.195988e+00   \n",
      "25%   -4.350557e-01 -4.836509e-01    -5.647871e-01    -5.337953e-01   \n",
      "50%   -2.823573e-01 -2.897767e-01     1.940691e-02    -1.029492e-01   \n",
      "75%    1.780850e-01  1.808578e-01     6.036010e-01     5.433200e-01   \n",
      "max    1.246993e+01  1.159188e+01     5.471885e+00     5.498051e+00   \n",
      "\n",
      "       bilirubin_min  ...        pt_min        pt_max    sodium_min  \\\n",
      "count   2.411700e+04  ...  2.411700e+04  2.411700e+04  2.411700e+04   \n",
      "mean   -1.178493e-17  ...  9.899339e-17  5.185368e-17 -1.253916e-15   \n",
      "std     1.000021e+00  ...  1.000021e+00  1.000021e+00  1.000021e+00   \n",
      "min    -5.663268e-01  ... -1.423638e+00 -9.722049e-01 -2.370947e+01   \n",
      "25%    -3.867449e-01  ... -5.201295e-01 -5.095935e-01 -5.035428e-01   \n",
      "50%    -1.428558e-01  ... -2.721076e-01 -3.361142e-01  7.181071e-02   \n",
      "75%     3.122386e-02  ...  3.302314e-01  5.802012e-02  4.553797e-01   \n",
      "max     2.304870e+01  ...  2.373287e+01  1.075830e+01  7.934975e+00   \n",
      "\n",
      "         sodium_max       bun_min       bun_max       wbc_min       wbc_max  \\\n",
      "count  2.411700e+04  2.411700e+04  2.411700e+04  2.411700e+04  2.411700e+04   \n",
      "mean   1.124282e-15  3.771177e-17 -7.542354e-17 -1.437761e-16 -1.319912e-16   \n",
      "std    1.000021e+00  1.000021e+00  1.000021e+00  1.000021e+00  1.000021e+00   \n",
      "min   -8.623636e+00 -1.266466e+00 -1.274251e+00 -1.254575e+00 -1.140611e+00   \n",
      "25%   -4.149207e-01 -7.001962e-01 -7.001388e-01 -4.711239e-01 -4.148399e-01   \n",
      "50%   -1.449559e-02 -2.646038e-01 -2.408487e-01 -1.485262e-01 -1.538886e-01   \n",
      "75%    3.859295e-01  2.581071e-01  2.949897e-01  2.662423e-01  2.293836e-01   \n",
      "max    8.394432e+00  9.754022e+00  9.059775e+00  6.507381e+01  6.789732e+01   \n",
      "\n",
      "             gender    match_flag  \n",
      "count  2.411700e+04  24117.000000  \n",
      "mean  -7.954826e-17      0.471949  \n",
      "std    1.000021e+00      0.499223  \n",
      "min   -1.130764e+00      0.000000  \n",
      "25%   -1.130764e+00      0.000000  \n",
      "50%    8.843575e-01      0.000000  \n",
      "75%    8.843575e-01      1.000000  \n",
      "max    8.843575e-01      1.000000  \n",
      "\n",
      "[8 rows x 41 columns]\n",
      "\n",
      "验证结果：\n",
      "总样本数：24117\n",
      "阳性样本数：11382\n",
      "阴性样本数：12735\n",
      "唯一hadm_id数量：24117\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "SELECT DISTINCT lfd.*,\n",
    "                p.gender,\n",
    "                1 AS match_flag\n",
    "FROM labs_first_day lfd\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = lfd.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE (di.long_title ILIKE '% renal fail%'\n",
    "    OR di.long_title ILIKE '%kidney fail%'\n",
    "    OR di.long_title ILIKE '%liver fail%'\n",
    "    OR di.long_title ILIKE '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%'\n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT\n",
    "    lfd.*,\n",
    "    p.gender,\n",
    "    0 AS match_flag\n",
    "FROM labs_first_day lfd\n",
    "LEFT JOIN patients p ON p.subject_id = lfd.subject_id\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM positive_hadm ph\n",
    "    WHERE ph.hadm_id = lfd.hadm_id\n",
    ")\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "'''\n",
    "\n",
    "\n",
    "#================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "\n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "\n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "\n",
    "    return keep_columns\n",
    "\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "\n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "\n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "\n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "\n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/labs_first_day_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/labs_first_day_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=LGBMRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "# 保存插值模型\n",
    "joblib.dump(imputer, \"./models/labs_first_day_lgbm_xgb.pkl\")\n",
    "print(\"插值模型已保存！\")\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "# 保存标准化模型\n",
    "scaler_path = \"./models/labs_first_day_lgbm_standard_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"标准化模型已保存至 {scaler_path}\")\n",
    "\n",
    "features_scaled = scaler.transform(features)\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_labs_first_day_lgbm.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41556db3-849f-4ac6-8127-b5932c28f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 148751 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 148751 条\n",
      "动态保留的列：['hadm_id', 'spec_itemid', 'org_itemid', 'isolate_num', 'ab_itemid', 'dilution_text', 'dilution_value', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/microbiologyevents_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 ./data/cleaned_microbiologyevents.csv\n",
      "数据特征分布：\n",
      "              hadm_id   spec_itemid    org_itemid   isolate_num     ab_itemid  \\\n",
      "count   40786.000000  40786.000000  40786.000000  40786.000000  40786.000000   \n",
      "mean   149950.636689      0.474512      0.361828      0.050977      0.512851   \n",
      "std     28847.476642      0.361430      0.243739      0.044876      0.206961   \n",
      "min    100001.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    124962.250000      0.109890      0.158974      0.036316      0.342567   \n",
      "50%    150013.500000      0.483516      0.352154      0.037114      0.474153   \n",
      "75%    174886.750000      0.846154      0.421053      0.060519      0.695523   \n",
      "max    199999.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "       dilution_text  dilution_value        gender    match_flag  \n",
      "count   40786.000000    40786.000000  40786.000000  40786.000000  \n",
      "mean        0.004159        0.004464      0.548178      0.267567  \n",
      "std         0.017880        0.017844      0.497680      0.442696  \n",
      "min         0.000000        0.000000      0.000000      0.000000  \n",
      "25%         0.001505        0.001953      0.000000      0.000000  \n",
      "50%         0.001618        0.001953      1.000000      0.000000  \n",
      "75%         0.001702        0.001953      1.000000      1.000000  \n",
      "max         1.000000        1.000000      1.000000      1.000000  \n",
      "\n",
      "验证结果：\n",
      "总样本数：40786\n",
      "阳性样本数：10913\n",
      "阴性样本数：29873\n",
      "唯一hadm_id数量：40786\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "select  distinct m.hadm_id,\n",
    "\t\t\t\tm.spec_itemid,\n",
    "\t\t\t\tm.org_itemid,\n",
    "\t\t\t\tm.isolate_num,\n",
    "\t\t\t\tm.ab_itemid,\n",
    "\t\t\t\tm.dilution_text,\n",
    "\t\t\t\tm.dilution_value,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t1 AS match_flag\n",
    "from microbiologyevents m\n",
    "left join labevents l on l.hadm_id=m.hadm_id\n",
    "left join diagnoses_icd d on d.hadm_id=m.hadm_id\n",
    "left join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "left join patients p  on p.subject_id = d.subject_id\n",
    "where l.itemid = 50912\n",
    "\tAND l.valuenum <= 150\n",
    "\tand(di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%' \n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT * FROM (\n",
    "    SELECT \n",
    "        DISTINCT m.hadm_id,\n",
    "        m.spec_itemid,\n",
    "        m.org_itemid,\n",
    "        m.isolate_num,\n",
    "        m.ab_itemid,\n",
    "        m.dilution_text,\n",
    "        m.dilution_value,\n",
    "        p.gender,\n",
    "        0 AS match_flag \n",
    "    FROM microbiologyevents m\n",
    "    LEFT JOIN patients p ON p.subject_id = m.subject_id\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM positive_hadm ph \n",
    "        WHERE ph.hadm_id = m.hadm_id\n",
    "    )\n",
    ") AS subquery\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "\n",
    "'''\n",
    "\n",
    "# ================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "    \n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "    \n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "    \n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "    \n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/microbiologyevents_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/microbiologyevents_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "df['dilution_text'] = df['dilution_text'].str.extract(r'([\\d\\.]+)').astype(float)\n",
    "\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag','dilution_comparison']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_microbiologyevents.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc73961-e6c9-4cf7-b27e-75e0ad78159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 12637 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 12637 条\n",
      "动态保留的列：['hadm_id', 'heartrate_min', 'heartrate_max', 'heartrate_mean', 'sysbp_min', 'sysbp_max', 'sysbp_mean', 'diasbp_min', 'diasbp_max', 'diasbp_mean', 'meanbp_min', 'meanbp_max', 'meanbp_mean', 'resprate_min', 'resprate_max', 'resprate_mean', 'tempc_min', 'tempc_max', 'tempc_mean', 'spo2_min', 'spo2_max', 'spo2_mean', 'glucose_min', 'glucose_max', 'glucose_mean', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/vitals_first_day_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "插值模型已保存！\n",
      "标准化模型已保存至 ./models/vitals_first_day_standard_scaler.pkl\n",
      "处理后的数据已保存到 ./data/cleaned_vitals_first_day.csv\n",
      "数据特征分布：\n",
      "              hadm_id  heartrate_min  heartrate_max  heartrate_mean  \\\n",
      "count   23680.000000   2.368000e+04   2.368000e+04    2.368000e+04   \n",
      "mean   149928.050127   3.072617e-16  -7.681543e-17   -4.800964e-16   \n",
      "std     28858.657107   1.000021e+00   1.000021e+00    1.000021e+00   \n",
      "min    100001.000000  -3.475526e+00  -3.060160e+00   -2.790386e+00   \n",
      "25%    124811.750000  -6.655692e-01  -7.002555e-01   -6.875893e-01   \n",
      "50%    149996.000000  -1.670532e-01  -1.496112e-01   -1.671795e-01   \n",
      "75%    174913.500000   4.000370e-01   5.190283e-01    4.690276e-01   \n",
      "max    199999.000000   5.311091e+00   5.396164e+00    4.841986e+00   \n",
      "\n",
      "          sysbp_min     sysbp_max    sysbp_mean    diasbp_min    diasbp_max  \\\n",
      "count  2.368000e+04  2.368000e+04  2.368000e+04  2.368000e+04  2.368000e+04   \n",
      "mean  -5.185042e-16  2.856574e-16  6.025210e-16 -1.488299e-16 -1.344270e-16   \n",
      "std    1.000021e+00  1.000021e+00  1.000021e+00  1.000021e+00  1.000021e+00   \n",
      "min   -4.714571e+00 -4.281930e+00 -4.204966e+00 -3.528953e+00 -2.994815e+00   \n",
      "25%   -6.120240e-01 -6.874216e-01 -6.938739e-01 -6.045598e-01 -6.910533e-01   \n",
      "50%   -2.972698e-02 -1.089951e-01 -1.765330e-01 -1.428441e-02 -1.017190e-01   \n",
      "75%    5.525700e-01  5.520639e-01  5.811960e-01  5.759910e-01  5.411912e-01   \n",
      "max    4.363969e+00  7.162653e+00  4.589327e+00  5.129544e+00  1.141709e+01   \n",
      "\n",
      "        diasbp_mean  ...     tempc_max    tempc_mean      spo2_min  \\\n",
      "count  2.368000e+04  ...  2.368000e+04  2.368000e+04  2.368000e+04   \n",
      "mean   6.337273e-16  ... -1.317865e-14  4.224849e-16  6.541314e-16   \n",
      "std    1.000021e+00  ...  1.000021e+00  1.000021e+00  1.000021e+00   \n",
      "min   -3.669565e+00  ... -8.136755e+00 -9.440927e+00 -9.028500e+00   \n",
      "25%   -6.700642e-01  ... -6.603358e-01 -6.071434e-01 -1.171294e-01   \n",
      "50%   -4.598206e-02  ... -1.149013e-01 -2.852294e-02  2.856443e-01   \n",
      "75%    5.468208e-01  ...  6.034716e-01  5.972728e-01  4.870312e-01   \n",
      "max    6.189799e+00  ...  6.323863e+00  5.031591e+00  9.904985e-01   \n",
      "\n",
      "           spo2_max     spo2_mean   glucose_min   glucose_max  glucose_mean  \\\n",
      "count  2.368000e+04  2.368000e+04  2.368000e+04  2.368000e+04  2.368000e+04   \n",
      "mean  -2.959194e-15  8.455699e-16  4.392882e-16 -1.152231e-16 -1.920386e-17   \n",
      "std    1.000021e+00  1.000021e+00  1.000021e+00  1.000021e+00  1.000021e+00   \n",
      "min   -3.151781e+01 -1.819709e+01 -2.915193e+00 -1.535980e+00 -2.740449e+00   \n",
      "25%   -3.585149e-01 -3.440487e-01 -5.933575e-01 -5.040002e-01 -6.055146e-01   \n",
      "50%    3.833730e-01  1.713759e-01 -1.519964e-01 -1.600069e-01 -1.971205e-01   \n",
      "75%    3.833730e-01  5.891730e-01  4.272900e-01  2.016270e-01  3.559129e-01   \n",
      "max    1.268435e+00  1.052413e+00  1.030274e+01  8.656158e+01  1.370043e+01   \n",
      "\n",
      "             gender    match_flag  \n",
      "count  2.368000e+04  23680.000000  \n",
      "mean  -7.081423e-17      0.473311  \n",
      "std    1.000021e+00      0.499298  \n",
      "min   -1.132532e+00      0.000000  \n",
      "25%   -1.132532e+00      0.000000  \n",
      "50%    8.829769e-01      0.000000  \n",
      "75%    8.829769e-01      1.000000  \n",
      "max    8.829769e-01      1.000000  \n",
      "\n",
      "[8 rows x 27 columns]\n",
      "\n",
      "验证结果：\n",
      "总样本数：23680\n",
      "阳性样本数：11208\n",
      "阴性样本数：12472\n",
      "唯一hadm_id数量：23680\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "select  distinct vfd.hadm_id,\n",
    "\t\t\t\tvfd.heartrate_min,vfd.heartrate_max,vfd.heartrate_mean,\n",
    "\t\t\t\tvfd.sysbp_min,vfd.sysbp_max,vfd.sysbp_mean,\n",
    "\t\t\t\tvfd.diasbp_min,vfd.diasbp_max,vfd.diasbp_mean,\n",
    "\t\t\t\tvfd.meanbp_min,vfd.meanbp_max,vfd.meanbp_mean,\n",
    "\t\t\t\tvfd.resprate_min,vfd.resprate_max,vfd.resprate_mean,\n",
    "\t\t\t\tvfd.tempc_min,vfd.tempc_max,vfd.tempc_mean,\n",
    "\t\t\t\tvfd.spo2_min,vfd.spo2_max,vfd.spo2_mean,\n",
    "\t\t\t\tvfd.glucose_min,vfd.glucose_max,vfd.glucose_mean,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t1 AS match_flag\n",
    "from vitals_first_day vfd\n",
    "left join labevents l on l.hadm_id=vfd.hadm_id\n",
    "left join diagnoses_icd d on d.hadm_id=vfd.hadm_id\n",
    "left join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "left join patients p  on p.subject_id = d.subject_id\n",
    "where l.itemid = 50912\n",
    "\tAND l.valuenum <= 150\n",
    "\tand(di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%' \n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT * FROM (\n",
    "select  distinct vfd.hadm_id,\n",
    "\t\t\t\tvfd.heartrate_min,vfd.heartrate_max,vfd.heartrate_mean,\n",
    "\t\t\t\tvfd.sysbp_min,vfd.sysbp_max,vfd.sysbp_mean,\n",
    "\t\t\t\tvfd.diasbp_min,vfd.diasbp_max,vfd.diasbp_mean,\n",
    "\t\t\t\tvfd.meanbp_min,vfd.meanbp_max,vfd.meanbp_mean,\n",
    "\t\t\t\tvfd.resprate_min,vfd.resprate_max,vfd.resprate_mean,\n",
    "\t\t\t\tvfd.tempc_min,vfd.tempc_max,vfd.tempc_mean,\n",
    "\t\t\t\tvfd.spo2_min,vfd.spo2_max,vfd.spo2_mean,\n",
    "\t\t\t\tvfd.glucose_min,vfd.glucose_max,vfd.glucose_mean,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t0 AS match_flag\n",
    "from vitals_first_day vfd\n",
    "    LEFT JOIN patients p ON p.subject_id = vfd.subject_id\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM positive_hadm ph \n",
    "        WHERE ph.hadm_id = vfd.hadm_id\n",
    "    )\n",
    ") AS subquery\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "\n",
    "'''\n",
    "\n",
    "# ================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "    \n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "    \n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "    \n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "    \n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/vitals_first_day_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/vitals_first_day_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "# df['dilution_text'] = df['dilution_text'].str.extract(r'([\\d\\.]+)').astype(float)\n",
    "\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 保存插值模型\n",
    "joblib.dump(imputer, \"./models/microbiologyevents_plus_xgb.pkl\")\n",
    "print(\"插值模型已保存！\")\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "# 保存标准化模型\n",
    "scaler_path = \"./models/vitals_first_day_standard_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"标准化模型已保存至 {scaler_path}\")\n",
    "\n",
    "features_scaled = scaler.transform(features)\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_vitals_first_day.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ee02c3-6879-4ca3-ad11-4c11e49ee582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 175293 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 175293 条\n",
      "动态保留的列：['hadm_id', 'spec_itemid', 'org_itemid', 'isolate_num', 'ab_itemid', 'dilution_text', 'dilution_value', 'urineoutput', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/microbiologyevents_plus_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "插值模型已保存！\n",
      "标准化模型已保存至 ./models/microbiologyevents_plus_standard_scaler.pkl\n",
      "处理后的数据已保存到 ./data/cleaned_microbiologyevents_plus.csv\n",
      "数据特征分布：\n",
      "              hadm_id   spec_itemid    org_itemid   isolate_num     ab_itemid  \\\n",
      "count   41765.000000  4.176500e+04  4.176500e+04  4.176500e+04  4.176500e+04   \n",
      "mean   149927.860840 -9.032748e-14  3.831300e-14 -6.018305e-16  8.359753e-14   \n",
      "std     28887.730593  1.000012e+00  1.000012e+00  1.000012e+00  1.000012e+00   \n",
      "min    100001.000000 -1.318042e+00 -1.730640e+00 -1.407520e+00 -3.267225e+00   \n",
      "25%    124925.000000 -1.014602e+00 -6.531906e-01 -2.777443e-01 -6.116099e-01   \n",
      "50%    149918.000000  4.743961e-02  7.943077e-02 -2.202983e-01  2.764951e-02   \n",
      "75%    174934.000000  1.018449e+00  3.356062e-01 -2.106527e-02  5.996970e-01   \n",
      "max    199999.000000  1.443266e+00  3.281220e+00  2.140600e+01  2.946137e+00   \n",
      "\n",
      "       dilution_text  dilution_value   urineoutput        gender    match_flag  \n",
      "count   4.176500e+04    4.176500e+04  4.176500e+04  4.176500e+04  41765.000000  \n",
      "mean   -2.041545e-18    3.470627e-17  7.621768e-17 -5.860936e-17      0.261295  \n",
      "std     1.000012e+00    1.000012e+00  1.000012e+00  1.000012e+00      0.439346  \n",
      "min    -2.209844e-01   -2.404932e-01 -2.884513e+00 -1.100861e+00      0.000000  \n",
      "25%    -1.441147e-01   -1.343020e-01 -6.182906e-01 -1.100861e+00      0.000000  \n",
      "50%    -1.379906e-01   -1.343020e-01 -1.158097e-01  9.083799e-01      0.000000  \n",
      "75%    -1.215371e-01   -1.343020e-01  4.305118e-01  9.083799e-01      1.000000  \n",
      "max     5.402749e+01    5.412937e+01  6.515612e+01  9.083799e-01      1.000000  \n",
      "\n",
      "验证结果：\n",
      "总样本数：41765\n",
      "阳性样本数：10913\n",
      "阴性样本数：30852\n",
      "唯一hadm_id数量：41765\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "select  distinct m.hadm_id,\n",
    "\t\t\t\tm.spec_itemid,\n",
    "\t\t\t\tm.org_itemid,\n",
    "\t\t\t\tm.isolate_num,\n",
    "\t\t\t\tm.ab_itemid,\n",
    "\t\t\t\tm.dilution_text,\n",
    "\t\t\t\tm.dilution_value,\n",
    "    \t\t\tuofd.urineoutput,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t1 AS match_flag\n",
    "from microbiologyevents m\n",
    "left join labevents l on l.hadm_id=m.hadm_id\n",
    "left join urine_output_first_day uofd on uofd.hadm_id = m.hadm_id\n",
    "left join diagnoses_icd d on d.hadm_id=m.hadm_id\n",
    "left join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "left join patients p  on p.subject_id = d.subject_id\n",
    "where l.itemid = 50912\n",
    "\tAND l.valuenum <= 150\n",
    "\tand(di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%' \n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT * FROM (\n",
    "    SELECT \n",
    "        DISTINCT m.hadm_id,\n",
    "        m.spec_itemid,\n",
    "        m.org_itemid,\n",
    "        m.isolate_num,\n",
    "        m.ab_itemid,\n",
    "        m.dilution_text,\n",
    "        m.dilution_value,\n",
    "        uofd.urineoutput,\n",
    "        p.gender,\n",
    "        0 AS match_flag \n",
    "    FROM microbiologyevents m\n",
    "    left join urine_output_first_day uofd on uofd.hadm_id = m.hadm_id\n",
    "    LEFT JOIN patients p ON p.subject_id = m.subject_id\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM positive_hadm ph \n",
    "        WHERE ph.hadm_id = m.hadm_id\n",
    "    )\n",
    ") AS subquery\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "\n",
    "'''\n",
    "\n",
    "# ================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "    \n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "    \n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "    \n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "    \n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/microbiologyevents_plus_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/microbiologyevents_plus_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "df['dilution_text'] = df['dilution_text'].str.extract(r'([\\d\\.]+)').astype(float)\n",
    "\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag','dilution_comparison']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 保存插值模型\n",
    "joblib.dump(imputer, \"./models/microbiologyevents_plus_xgb.pkl\")\n",
    "print(\"插值模型已保存！\")\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "# 保存标准化模型\n",
    "scaler_path = \"./models/microbiologyevents_plus_standard_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"标准化模型已保存至 {scaler_path}\")\n",
    "\n",
    "features_scaled = scaler.transform(features)\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_microbiologyevents_plus.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca84bbd-f6d8-4787-bfec-2e4a2eab2e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 16127 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 16127 条\n",
      "动态保留的列：['hadm_id', 'aniongap_min', 'aniongap_max', 'albumin_min', 'albumin_max', 'bands_min', 'bands_max', 'bicarbonate_min', 'bicarbonate_max', 'bilirubin_min', 'bilirubin_max', 'creatinine_min', 'creatinine_max', 'chloride_min', 'chloride_max', 'glucose_min', 'glucose_max', 'hematocrit_min', 'hematocrit_max', 'hemoglobin_min', 'hemoglobin_max', 'lactate_min', 'lactate_max', 'platelet_min', 'platelet_max', 'potassium_min', 'potassium_max', 'ptt_min', 'ptt_max', 'inr_min', 'inr_max', 'pt_min', 'pt_max', 'sodium_min', 'sodium_max', 'bun_min', 'bun_max', 'wbc_min', 'wbc_max', 'hadm_id', 'heartrate_min', 'heartrate_max', 'heartrate_mean', 'sysbp_min', 'sysbp_max', 'sysbp_mean', 'diasbp_min', 'diasbp_max', 'diasbp_mean', 'meanbp_min', 'meanbp_max', 'meanbp_mean', 'resprate_min', 'resprate_max', 'resprate_mean', 'tempc_min', 'tempc_max', 'tempc_mean', 'spo2_min', 'spo2_max', 'spo2_mean', 'glucose_min', 'glucose_max', 'glucose_mean', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/labs_first_day_plus_vitals_first_day_raw.csv\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11955\n",
      "[LightGBM] [Info] Number of data points in the train set: 26076, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 149998.006558\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11955\n",
      "[LightGBM] [Info] Number of data points in the train set: 26076, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 149998.006558\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11955\n",
      "[LightGBM] [Info] Number of data points in the train set: 26076, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 149998.006558\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 26076, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 0.564465\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12045\n",
      "[LightGBM] [Info] Number of data points in the train set: 26009, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 75.703338\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12044\n",
      "[LightGBM] [Info] Number of data points in the train set: 26009, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 108.053828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11950\n",
      "[LightGBM] [Info] Number of data points in the train set: 26009, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 90.012937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11979\n",
      "[LightGBM] [Info] Number of data points in the train set: 25645, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 30.877567\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11979\n",
      "[LightGBM] [Info] Number of data points in the train set: 25645, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 36.197450\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12041\n",
      "[LightGBM] [Info] Number of data points in the train set: 25574, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 10.418495\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12069\n",
      "[LightGBM] [Info] Number of data points in the train set: 25574, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 12.037276\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11982\n",
      "[LightGBM] [Info] Number of data points in the train set: 25558, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 210.720381\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11982\n",
      "[LightGBM] [Info] Number of data points in the train set: 25558, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 250.183309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11975\n",
      "[LightGBM] [Info] Number of data points in the train set: 25524, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 10.940322\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11975\n",
      "[LightGBM] [Info] Number of data points in the train set: 25524, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 14.135123\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12151\n",
      "[LightGBM] [Info] Number of data points in the train set: 24526, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 3.839490\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12130\n",
      "[LightGBM] [Info] Number of data points in the train set: 24526, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 4.763260\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12143\n",
      "[LightGBM] [Info] Number of data points in the train set: 24504, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 136.515426\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12142\n",
      "[LightGBM] [Info] Number of data points in the train set: 24504, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 140.150057\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12131\n",
      "[LightGBM] [Info] Number of data points in the train set: 24466, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 102.196657\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12136\n",
      "[LightGBM] [Info] Number of data points in the train set: 24466, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 106.829641\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12138\n",
      "[LightGBM] [Info] Number of data points in the train set: 24379, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 22.076299\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12138\n",
      "[LightGBM] [Info] Number of data points in the train set: 24379, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 24.772136\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12138\n",
      "[LightGBM] [Info] Number of data points in the train set: 24131, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 13.335585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12127\n",
      "[LightGBM] [Info] Number of data points in the train set: 24131, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 16.463968\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12423\n",
      "[LightGBM] [Info] Number of data points in the train set: 24104, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 107.007846\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12423\n",
      "[LightGBM] [Info] Number of data points in the train set: 24104, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 184.481165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12423\n",
      "[LightGBM] [Info] Number of data points in the train set: 24104, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 107.007846\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12423\n",
      "[LightGBM] [Info] Number of data points in the train set: 24104, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 184.481165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12423\n",
      "[LightGBM] [Info] Number of data points in the train set: 24104, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 140.122534\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12261\n",
      "[LightGBM] [Info] Number of data points in the train set: 23940, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 112.112663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004614 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12261\n",
      "[LightGBM] [Info] Number of data points in the train set: 23940, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 185.462164\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12261\n",
      "[LightGBM] [Info] Number of data points in the train set: 23940, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 112.112663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12261\n",
      "[LightGBM] [Info] Number of data points in the train set: 23940, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 185.462164\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12331\n",
      "[LightGBM] [Info] Number of data points in the train set: 23938, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 1.512056\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12312\n",
      "[LightGBM] [Info] Number of data points in the train set: 23938, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 1.826610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12293\n",
      "[LightGBM] [Info] Number of data points in the train set: 23936, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 28.856116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12276\n",
      "[LightGBM] [Info] Number of data points in the train set: 23936, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 34.121867\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12441\n",
      "[LightGBM] [Info] Number of data points in the train set: 23923, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 57.281720\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12413\n",
      "[LightGBM] [Info] Number of data points in the train set: 23923, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 104.422549\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12410\n",
      "[LightGBM] [Info] Number of data points in the train set: 23923, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 77.175208\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12507\n",
      "[LightGBM] [Info] Number of data points in the train set: 23911, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 91.113566\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12480\n",
      "[LightGBM] [Info] Number of data points in the train set: 23911, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 148.912216\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12405\n",
      "[LightGBM] [Info] Number of data points in the train set: 23911, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 118.171055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12567\n",
      "[LightGBM] [Info] Number of data points in the train set: 23910, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 42.845601\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12509\n",
      "[LightGBM] [Info] Number of data points in the train set: 23910, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 83.641029\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12405\n",
      "[LightGBM] [Info] Number of data points in the train set: 23910, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 60.122914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12554\n",
      "[LightGBM] [Info] Number of data points in the train set: 23895, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 91.008809\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12625\n",
      "[LightGBM] [Info] Number of data points in the train set: 23895, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 99.538171\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12392\n",
      "[LightGBM] [Info] Number of data points in the train set: 23895, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 97.022938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12643\n",
      "[LightGBM] [Info] Number of data points in the train set: 23893, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 12.568508\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12617\n",
      "[LightGBM] [Info] Number of data points in the train set: 23893, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 27.551835\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12422\n",
      "[LightGBM] [Info] Number of data points in the train set: 23893, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 19.173751\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12435\n",
      "[LightGBM] [Info] Number of data points in the train set: 23519, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 36.059242\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12422\n",
      "[LightGBM] [Info] Number of data points in the train set: 23519, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 37.441628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12389\n",
      "[LightGBM] [Info] Number of data points in the train set: 23519, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 36.780060\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12344\n",
      "[LightGBM] [Info] Number of data points in the train set: 21423, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 15.346259\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12346\n",
      "[LightGBM] [Info] Number of data points in the train set: 21423, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 17.724380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12513\n",
      "[LightGBM] [Info] Number of data points in the train set: 21422, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 1.420599\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12460\n",
      "[LightGBM] [Info] Number of data points in the train set: 21422, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 1.757984\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12367\n",
      "[LightGBM] [Info] Number of data points in the train set: 21321, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 32.479614\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 21321, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 45.226397\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12491\n",
      "[LightGBM] [Info] Number of data points in the train set: 15212, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 1.843783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12433\n",
      "[LightGBM] [Info] Number of data points in the train set: 15212, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 3.125640\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13349\n",
      "[LightGBM] [Info] Number of data points in the train set: 12960, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 2.075478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13347\n",
      "[LightGBM] [Info] Number of data points in the train set: 12960, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 2.401489\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12244\n",
      "[LightGBM] [Info] Number of data points in the train set: 9952, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 3.063012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12243\n",
      "[LightGBM] [Info] Number of data points in the train set: 9952, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 3.156340\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13598\n",
      "[LightGBM] [Info] Number of data points in the train set: 4597, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 7.350141\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13589\n",
      "[LightGBM] [Info] Number of data points in the train set: 4597, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 9.081379\n",
      "插值模型已保存！\n",
      "标准化模型已保存至 ./models/labs_first_day_plus_vitals_first_day_standard_scaler.pkl\n",
      "处理后的数据已保存到 ./data/cleaned_labs_first_day_plus_vitals_first_day_raw.csv\n",
      "数据特征分布：\n",
      "              hadm_id     hadm_id.1  aniongap_min  aniongap_max   albumin_min  \\\n",
      "count   26076.000000  2.607600e+04  2.607600e+04  2.607600e+04  2.607600e+04   \n",
      "mean   149998.006558 -4.546482e-16 -1.159714e-15 -7.411705e-17  2.125415e-16   \n",
      "std     28933.758532  1.000019e+00  1.000019e+00  1.000019e+00  1.000019e+00   \n",
      "min    100001.000000 -1.728015e+00 -3.619071e+00 -2.874639e+00 -4.720388e+00   \n",
      "25%    124784.500000 -8.714385e-01 -6.970739e-01 -6.649142e-01 -7.797875e-02   \n",
      "50%    150093.500000  3.300479e-03 -1.126746e-01 -2.631461e-01  1.118065e-01   \n",
      "75%    175115.500000  8.681200e-01  4.717248e-01  3.395061e-01  1.469602e-01   \n",
      "max    199999.000000  1.728153e+00  8.945515e+00  8.374869e+00  7.110272e+00   \n",
      "\n",
      "        albumin_max     bands_min     bands_max  bicarbonate_min  \\\n",
      "count  2.607600e+04  2.607600e+04  2.607600e+04     2.607600e+04   \n",
      "mean  -8.283670e-17 -1.046358e-16 -2.397905e-17    -7.193714e-16   \n",
      "std    1.000019e+00  1.000019e+00  1.000019e+00     1.000019e+00   \n",
      "min   -4.967192e+00 -1.900912e+00 -1.795615e+00    -3.923022e+00   \n",
      "25%   -1.609104e-01 -8.992927e-02 -2.395641e-01    -6.226366e-01   \n",
      "50%    5.640050e-02  3.315520e-02 -8.947817e-02     1.539248e-01   \n",
      "75%    2.136420e-01  1.141139e-01  1.325056e-01     5.422055e-01   \n",
      "max    6.993323e+00  1.698237e+01  1.422892e+01     5.395714e+00   \n",
      "\n",
      "       bicarbonate_max  ...      spo2_min      spo2_max     spo2_mean  \\\n",
      "count     2.607600e+04  ...  2.607600e+04  2.607600e+04  2.607600e+04   \n",
      "mean      1.765730e-16  ... -4.076438e-16  1.366915e-14 -8.092928e-16   \n",
      "std       1.000019e+00  ...  1.000019e+00  1.000019e+00  1.000019e+00   \n",
      "min      -4.297124e+00  ... -1.042776e+01 -3.378478e+01 -2.532357e+01   \n",
      "25%      -6.007630e-01  ... -1.240400e-01  2.275115e-01 -3.234205e-01   \n",
      "50%      -3.742485e-02  ...  2.213361e-01  3.685405e-01  7.204474e-02   \n",
      "75%       4.864022e-01  ...  4.515868e-01  3.685405e-01  5.916677e-01   \n",
      "max       6.139661e+00  ...  1.027213e+00  5.717334e-01  1.157275e+00   \n",
      "\n",
      "       glucose_min.2  glucose_min.3  glucose_max.2  glucose_max.3  \\\n",
      "count   2.607600e+04   2.607600e+04   2.607600e+04   2.607600e+04   \n",
      "mean    4.945678e-17   4.051914e-16  -2.054568e-16  -1.035459e-16   \n",
      "std     1.000019e+00   1.000019e+00   1.000019e+00   1.000019e+00   \n",
      "min    -2.864730e+00  -2.992818e+00  -1.498653e+00  -1.468037e+00   \n",
      "25%    -5.699367e-01  -5.921269e-01  -5.261382e-01  -5.070000e-01   \n",
      "50%    -1.058212e-01  -5.657291e-02  -1.929619e-01  -1.567154e-01   \n",
      "75%     3.067260e-01   3.944199e-01   1.312098e-01   1.935692e-01   \n",
      "max     1.340510e+01   1.293766e+01   2.030189e+01   8.815093e+01   \n",
      "\n",
      "       glucose_mean        gender    match_flag  \n",
      "count  2.607600e+04  2.607600e+04  26076.000000  \n",
      "mean   2.179913e-17 -1.455092e-16      0.429821  \n",
      "std    1.000019e+00  1.000019e+00      0.495060  \n",
      "min   -2.573676e+00 -1.138433e+00      0.000000  \n",
      "25%   -6.128899e-01 -1.138433e+00      0.000000  \n",
      "50%   -1.469606e-01  8.784007e-01      0.000000  \n",
      "75%    3.275971e-01  8.784007e-01      1.000000  \n",
      "max    1.393278e+01  8.784007e-01      1.000000  \n",
      "\n",
      "[8 rows x 72 columns]\n",
      "\n",
      "验证结果：\n",
      "总样本数：26076\n",
      "阳性样本数：11208\n",
      "阴性样本数：14868\n",
      "唯一hadm_id数量：26076\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "select  distinct lfd.*,\n",
    "\t\t\t\tvfd.hadm_id,\n",
    "\t\t\t\tvfd.heartrate_min,vfd.heartrate_max,vfd.heartrate_mean,\n",
    "\t\t\t\tvfd.sysbp_min,vfd.sysbp_max,vfd.sysbp_mean,\n",
    "\t\t\t\tvfd.diasbp_min,vfd.diasbp_max,vfd.diasbp_mean,\n",
    "\t\t\t\tvfd.meanbp_min,vfd.meanbp_max,vfd.meanbp_mean,\n",
    "\t\t\t\tvfd.resprate_min,vfd.resprate_max,vfd.resprate_mean,\n",
    "\t\t\t\tvfd.tempc_min,vfd.tempc_max,vfd.tempc_mean,\n",
    "\t\t\t\tvfd.spo2_min,vfd.spo2_max,vfd.spo2_mean,\n",
    "\t\t\t\tvfd.glucose_min,vfd.glucose_max,vfd.glucose_mean,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t1 AS match_flag\n",
    "from vitals_first_day vfd\n",
    "left join labs_first_day lfd on lfd.hadm_id = vfd.hadm_id\n",
    "left join labevents l on l.hadm_id=vfd.hadm_id\n",
    "left join diagnoses_icd d on d.hadm_id=vfd.hadm_id\n",
    "left join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "left join patients p  on p.subject_id = d.subject_id\n",
    "where l.itemid = 50912\n",
    "\tAND l.valuenum <= 150\n",
    "\tand(di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%'\n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT\n",
    "    lfd.*,\n",
    "    vfd.hadm_id,\n",
    "    vfd.heartrate_min,vfd.heartrate_max,vfd.heartrate_mean,\n",
    "    vfd.sysbp_min,vfd.sysbp_max,vfd.sysbp_mean,\n",
    "    vfd.diasbp_min,vfd.diasbp_max,vfd.diasbp_mean,\n",
    "    vfd.meanbp_min,vfd.meanbp_max,vfd.meanbp_mean,\n",
    "    vfd.resprate_min,vfd.resprate_max,vfd.resprate_mean,\n",
    "    vfd.tempc_min,vfd.tempc_max,vfd.tempc_mean,\n",
    "    vfd.spo2_min,vfd.spo2_max,vfd.spo2_mean,\n",
    "    vfd.glucose_min,vfd.glucose_max,vfd.glucose_mean,\n",
    "    p.gender,\n",
    "    0 AS match_flag\n",
    "FROM vitals_first_day vfd\n",
    "LEFT JOIN labs_first_day lfd on lfd.hadm_id = vfd.hadm_id\n",
    "LEFT JOIN patients p ON p.subject_id = lfd.subject_id\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM positive_hadm ph\n",
    "    WHERE ph.hadm_id = lfd.hadm_id\n",
    ")\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "'''\n",
    "\n",
    "\n",
    "#================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "\n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "\n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "\n",
    "    return keep_columns\n",
    "\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "\n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "\n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "\n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "\n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/labs_first_day_plus_vitals_first_day_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/labs_first_day_plus_vitals_first_day_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=LGBMRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 保存插值模型\n",
    "joblib.dump(imputer, \"./models/labs_first_day_plus_vitals_first_day_lgbm.pkl\")\n",
    "print(\"插值模型已保存！\")\n",
    "\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "# 保存标准化模型\n",
    "scaler_path = \"./models/labs_first_day_plus_vitals_first_day_standard_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"标准化模型已保存至 {scaler_path}\")\n",
    "\n",
    "\n",
    "# features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "features_scaled = scaler.transform(features)\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_labs_first_day_plus_vitals_first_day_raw.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80231455-e6fe-4bf9-9641-159dddc39547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning] *",
   "language": "python",
   "name": "conda-env-.conda-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
