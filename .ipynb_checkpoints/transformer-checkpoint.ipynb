{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e46c7a-2510-4c90-8ff6-ba37fa9049e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 设置可视化风格\n",
    "plt.style.use('tableau-colorblind10')\n",
    "# 设置字体为SimHei(黑体)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# 解决中文字体下坐标轴负数的负号显示问题\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ad200-cbdb-4f2f-a382-8a688f2b7d60",
   "metadata": {},
   "source": [
    "### 筛选出与肾衰竭有关的代号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286c6f43-fe97-47bb-9fb3-1254b315057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id  hadm_id  icustay_id  heartrate_min  heartrate_max  \\\n",
      "0           3   145834      211552           75.0          168.0   \n",
      "1           3   145834      211552           75.0          168.0   \n",
      "2           3   145834      211552           75.0          168.0   \n",
      "3           3   145834      211552           75.0          168.0   \n",
      "4           3   145834      211552           75.0          168.0   \n",
      "\n",
      "   heartrate_mean  sysbp_min  sysbp_max  sysbp_mean  diasbp_min  ...  \\\n",
      "0      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "1      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "2      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "3      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "4      111.785714       64.0      217.0      102.96        28.0  ...   \n",
      "\n",
      "                short_title                         long_title  row_id  \\\n",
      "0  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "1  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "2  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "3  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "4  Acute kidney failure NOS  Acute kidney failure, unspecified       2   \n",
      "\n",
      "   subject_id  gender        dob        dod  dod_hosp    dod_ssn  expire_flag  \n",
      "0           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "1           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "2           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "3           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "4           3       M 2025-04-11 2102-06-14       NaT 2102-06-14            1  \n",
      "\n",
      "[5 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "#建立数据库连接\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "#调用游标对象\n",
    "cur = con.cursor()\n",
    "\n",
    "# cur.execute('''SELECT DISTINCT d.row_id, d.subject_id, d.hadm_id, d.seq_num, d.icd9_code, lab.value, lab.valueuom, lab.charttime\n",
    "# FROM diagnoses_icd d\n",
    "# JOIN labevents lab ON d.hadm_id = lab.hadm_id\n",
    "# WHERE (d.icd9_code LIKE '584%' OR d.icd9_code LIKE '586%');''')\n",
    "\n",
    "cur.execute('''select distinct* from vitals_first_day vfd\n",
    "join kdigo_uo ku on ku.icustay_id = vfd.icustay_id\n",
    "join diagnoses_icd d on d.hadm_id=vfd.hadm_id\n",
    "join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "join patients p  on p.subject_id = d.subject_id\n",
    "where (di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%');''')\n",
    "            \n",
    "rows = cur.fetchall()\n",
    "\n",
    "# 获取列名（可选，方便 DataFrame 的列标题）\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "# 将查询结果转换为 DataFrame\n",
    "df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "# 输出查看数据\n",
    "print(df.head())\n",
    "df.to_csv(\"住院第一天的生命体征与aki肾衰竭患者尿量数据.csv\", index=False,header=True)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d288c39-1ea1-4181-94f9-4435b2b2133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "#建立数据库连接\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "#调用游标对象\n",
    "cur = con.cursor()\n",
    "\n",
    "batch_size = 10000\n",
    "offset = 0\n",
    "\n",
    "# 打开文件并写入数据\n",
    "with open(\"CHARTEVENTS.csv\", mode=\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "    first_batch = True  # 是否是第一批数据\n",
    "\n",
    "    while True:\n",
    "        # 分批查询\n",
    "        query = f'''\n",
    "        SELECT DISTINCT \n",
    "            C.charttime, C.itemid, C.value, C.valueuom,\n",
    "            d.row_id, d.icd9_code, d.hadm_id,\n",
    "            p.subject_id, p.gender, p.dob\n",
    "        FROM CHARTEVENTS C\n",
    "        JOIN diagnoses_icd d ON d.hadm_id = C.hadm_id\n",
    "        JOIN patients p ON p.subject_id = d.subject_id\n",
    "        WHERE (d.icd9_code LIKE '584%' OR d.icd9_code LIKE '586%')\n",
    "        LIMIT {batch_size} OFFSET {offset};\n",
    "        '''\n",
    "        \n",
    "        # 执行查询\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        # 如果没有更多数据，退出循环\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        # 获取列名，确保列名与数据列数匹配\n",
    "        column_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "        # 创建 DataFrame\n",
    "        df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "        # 写入 CSV 文件\n",
    "        df.to_csv(file, index=False, header=first_batch, mode=\"a\")\n",
    "        first_batch = False  # 仅第一批数据包含表头\n",
    "\n",
    "        # 更新 offset\n",
    "        offset += batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb4a10-bb78-4aba-91aa-efc7309db41d",
   "metadata": {},
   "source": [
    "### 血气"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48411dcb-9acb-4b95-8e27-21195ec70e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽样结果已保存到 blood_gas_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    bl.subject_id, bl.hadm_id, d.icd9_code, bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, p.gender, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    1 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%';\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    bl.subject_id, bl.hadm_id, d.icd9_code, bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, p.gender, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    0 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%');\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取急性肾衰竭患者数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "    \n",
    "    # 获取非急性肾衰竭患者数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "    \n",
    "    # 随机抽取与急性肾衰竭患者数量相等的非急性肾衰竭数据\n",
    "    sample_negative_df = negative_df.sample(n=len(positive_df), random_state=42)\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, sample_negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"blood_gas_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1abab4-89b6-46b5-9eb5-9efa9b31c0bd",
   "metadata": {},
   "source": [
    "### aki患者第一天实验室数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f5ddac-d59f-4ff0-9acb-bfe0734a5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽样结果已保存到 aki_patients_labs_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    lf.*, kc.*, d.icd9_code, di.long_title, p.gender,\n",
    "    1 AS match_flag\n",
    "FROM labs_first_day lf\n",
    "JOIN kdigo_creatinine kc ON kc.icustay_id = lf.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = lf.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%';\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    lf.*, kc.*, d.icd9_code, di.long_title, p.gender,\n",
    "    0 AS match_flag\n",
    "FROM labs_first_day lf\n",
    "JOIN kdigo_creatinine kc ON kc.icustay_id = lf.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = lf.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%');\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # 获取急性肾衰竭患者数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭患者数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 随机抽取与急性肾衰竭患者数量相等的非急性肾衰竭数据\n",
    "    sample_negative_df = negative_df.sample(n=len(positive_df), random_state=42)\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, sample_negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"aki_patients_labs_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a13397-334e-42c2-aac1-7b8426963c92",
   "metadata": {},
   "source": [
    "### aki患者微生物实验室结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bc7f60-48ef-467b-bb78-2b0e66843954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 20,000 条每类数据已保存到 aki_microbiology_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    mb.*, icd.*, kc.*, d.icd9_code, di.long_title, p.gender,\n",
    "    1 AS match_flag\n",
    "FROM microbiologyevents mb\n",
    "JOIN icustay_detail icd ON icd.hadm_id = mb.hadm_id\n",
    "JOIN kdigo_creatinine kc ON kc.icustay_id = icd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = mb.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    mb.*, icd.*, kc.*, d.icd9_code, di.long_title, p.gender,\n",
    "    0 AS match_flag\n",
    "FROM microbiologyevents mb\n",
    "JOIN icustay_detail icd ON icd.hadm_id = mb.hadm_id\n",
    "JOIN kdigo_creatinine kc ON kc.icustay_id = icd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = mb.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%')\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # 获取急性肾衰竭患者数据（前 20,000 条）\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭患者数据（前 20,000 条）\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"aki_microbiology_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"前 20,000 条每类数据已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc52f2-db2f-4dc8-a6e5-6c1c99f6753a",
   "metadata": {},
   "source": [
    "### aki肾衰竭患者尿量数据+第一天实验室数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6bd4474-faf9-41ee-a192-ee3bccadf72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽样结果已保存到 aki_urine_labs_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句：急性肾衰竭\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    lf.*, ku.*, d.icd9_code, di.long_title, p.gender,\n",
    "    1 AS match_flag\n",
    "FROM labs_first_day lf\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = lf.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = lf.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "# SQL 查询语句：非急性肾衰竭\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    lf.*, ku.*, d.icd9_code, di.long_title, p.gender,\n",
    "    0 AS match_flag\n",
    "FROM labs_first_day lf\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = lf.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = lf.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%')\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取急性肾衰竭患者数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭患者数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"aki_urine_labs_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"随机抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a8850-f24a-47bc-8746-bf3e5e9142c0",
   "metadata": {},
   "source": [
    "### 住院第一天的生命体征与aki肾衰竭患者尿量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d289ed-d604-4bc4-bac8-9d072851a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽样结果已保存到 vitals_aki_urine_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句：急性肾衰竭\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    vfd.*, ku.*, d.icd9_code, di.long_title, p.gender,\n",
    "    1 AS match_flag\n",
    "FROM vitals_first_day vfd\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = vfd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = vfd.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "# SQL 查询语句：非急性肾衰竭\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    vfd.*, ku.*, d.icd9_code, di.long_title, p.gender,\n",
    "    0 AS match_flag\n",
    "FROM vitals_first_day vfd\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = vfd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = vfd.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%')\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取急性肾衰竭患者数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭患者数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    output_file = \"vitals_aki_urine_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"随机抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0129d4-12a9-4a41-9403-3fce062dfd55",
   "metadata": {},
   "source": [
    "### 语言 宗教 婚姻 种族"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b665dba-2533-42a4-bf91-56f68ec8bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽样结果已保存到 language_religion_ethnicity_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句：急性肾衰竭\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    a.language,\n",
    "    a.religion,\n",
    "    a.marital_status,\n",
    "    a.ethnicity,\n",
    "    1 AS match_flag\n",
    "FROM admissions a\n",
    "JOIN diagnoses_icd d ON d.hadm_id = a.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "WHERE di.long_title ILIKE '% renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "# SQL 查询语句：非急性肾衰竭\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    a.language,\n",
    "    a.religion,\n",
    "    a.marital_status,\n",
    "    a.ethnicity,\n",
    "    0 AS match_flag\n",
    "FROM admissions a\n",
    "JOIN diagnoses_icd d ON d.hadm_id = a.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "WHERE NOT (di.long_title ILIKE '% renal fail%' \n",
    "           OR di.long_title ILIKE '%kidney fail%'\n",
    "           OR di.long_title ILIKE '%liver fail%'\n",
    "           OR di.long_title ILIKE '%spleen fail%')\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取急性肾衰竭数据\n",
    "    cur.execute(query_positive)\n",
    "    positive_rows = cur.fetchall()\n",
    "    positive_df = pd.DataFrame(positive_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 获取非急性肾衰竭数据\n",
    "    cur.execute(query_negative)\n",
    "    negative_rows = cur.fetchall()\n",
    "    negative_df = pd.DataFrame(negative_rows, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 保存为 CSV 文件\n",
    "    output_file = \"language_religion_ethnicity_sampled_with_flags.csv\"\n",
    "    combined_df.to_csv(output_file, index=False, header=True)\n",
    "    print(f\"随机抽样结果已保存到 {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976cd0d-36a7-4a7d-afdf-e7200e8aac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# SQL 查询语句：符合条件的数据\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    bl.subject_id, bl.hadm_id, bl.icustay_id, -- 主键\n",
    "    bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    p.gender,\n",
    "    lf.*, -- 第一日实验室数据\n",
    "    mb.*, -- 微生物事件\n",
    "    rfd.*, -- 血液净化治疗\n",
    "    uof.*, -- 第一日尿液排出量\n",
    "    vfd.*, -- 第一日生命体征\n",
    "    1 AS match_flag -- 匹配标志\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "JOIN icustay_detail icd ON icd.hadm_id = d.hadm_id\n",
    "JOIN kdigo_creatinine kdc ON kdc.icustay_id = icd.icustay_id\n",
    "JOIN kdigo_uo ku ON ku.icustay_id = icd.icustay_id\n",
    "JOIN labs_first_day lf ON lf.icustay_id = icd.icustay_id\n",
    "JOIN microbiologyevents mb ON mb.hadm_id = d.hadm_id\n",
    "JOIN rrt_first_day rfd ON rfd.hadm_id = d.hadm_id\n",
    "JOIN urine_output_first_day uof ON uof.hadm_id = d.hadm_id\n",
    "JOIN vitals_first_day vfd ON vfd.icustay_id = icd.icustay_id\n",
    "WHERE di.long_title ILIKE '%renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 1;\n",
    "'''\n",
    "\n",
    "# SQL 查询语句：不符合条件的数据\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    bl.subject_id, bl.hadm_id, bl.icustay_id, -- 主键\n",
    "    bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    p.gender,\n",
    "    lf.*, -- 第一日实验室数据\n",
    "    mb.*, -- 微生物事件\n",
    "    rfd.*, -- 血液净化治疗\n",
    "    uof.*, -- 第一日尿液排出量\n",
    "    vfd.*, -- 第一日生命体征\n",
    "    0 AS match_flag -- 不匹配标志\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN patients p ON p.subject_id = bl.subject_id\n",
    "LEFT JOIN icustay_detail icd ON icd.hadm_id = bl.hadm_id\n",
    "LEFT JOIN kdigo_creatinine kdc ON kdc.icustay_id = icd.icustay_id\n",
    "LEFT JOIN kdigo_uo ku ON ku.icustay_id = icd.icustay_id\n",
    "LEFT JOIN labs_first_day lf ON lf.icustay_id = icd.icustay_id\n",
    "LEFT JOIN microbiologyevents mb ON mb.hadm_id = bl.hadm_id\n",
    "LEFT JOIN rrt_first_day rfd ON rfd.hadm_id = bl.hadm_id\n",
    "LEFT JOIN urine_output_first_day uof ON uof.hadm_id = bl.hadm_id\n",
    "LEFT JOIN vitals_first_day vfd ON vfd.icustay_id = icd.icustay_id\n",
    "WHERE di.icd9_code IS NULL\n",
    "LIMIT 1;\n",
    "'''\n",
    "\n",
    "output_file = \"aki_sampled_record.csv\"\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 创建一个空的 DataFrame，用于保存记录\n",
    "    columns_written = False\n",
    "    \n",
    "    for query in [query_positive, query_negative]:\n",
    "        cur.execute(query)\n",
    "        row = cur.fetchone()\n",
    "        \n",
    "        if row:\n",
    "            # 获取列名\n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            record_df = pd.DataFrame([row], columns=column_names)\n",
    "\n",
    "            # 如果是首次写入 CSV 文件，包含表头\n",
    "            if not columns_written:\n",
    "                record_df.to_csv(output_file, mode='w', index=False, header=True)\n",
    "                columns_written = True\n",
    "            else:\n",
    "                # 追加到 CSV 文件中，无需表头\n",
    "                record_df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "            \n",
    "            print(f\"已保存一条记录到 {output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e765005a-aa5b-4d34-9016-f4785fe6e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成 \n",
      "SELECT DISTINCT \n",
      "    bl.so2, bl.spo2, bl.po2, bl. 的记录写入\n",
      "完成 \n",
      "SELECT DISTINCT \n",
      "    bl.so2, bl.spo2, bl.po2, bl. 的记录写入\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    p.gender,\n",
    "    rfd.rrt,\n",
    "    uof.urineoutput,\n",
    "    vfd.*,\n",
    "    1 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "JOIN icustay_detail icd ON icd.hadm_id = d.hadm_id\n",
    "JOIN rrt_first_day rfd ON rfd.hadm_id = d.hadm_id\n",
    "JOIN urine_output_first_day uof ON uof.hadm_id = d.hadm_id\n",
    "JOIN vitals_first_day vfd ON vfd.icustay_id = icd.icustay_id\n",
    "WHERE di.long_title ILIKE '%renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 30000;\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    bl.so2, bl.spo2, bl.po2, bl.pco2, bl.fio2, bl.aado2, bl.pao2fio2,\n",
    "    bl.ph, bl.totalco2, bl.temperature, bl.requiredo2, bl.tidalvolume, bl.calcium,\n",
    "    p.gender,\n",
    "    rfd.rrt,\n",
    "    uof.urineoutput,\n",
    "    vfd.*,\n",
    "    0 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN patients p ON p.subject_id = bl.subject_id\n",
    "LEFT JOIN icustay_detail icd ON icd.hadm_id = bl.hadm_id\n",
    "LEFT JOIN rrt_first_day rfd ON rfd.hadm_id = bl.hadm_id\n",
    "LEFT JOIN urine_output_first_day uof ON uof.hadm_id = bl.hadm_id\n",
    "LEFT JOIN vitals_first_day vfd ON vfd.icustay_id = icd.icustay_id\n",
    "WHERE di.icd9_code IS NULL\n",
    "LIMIT 30000;\n",
    "'''\n",
    "\n",
    "output_file = \"aki_data3.csv\"\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    queries = [query_positive, query_negative]\n",
    "\n",
    "    for query in queries:\n",
    "        cur.execute(query)\n",
    "        \n",
    "        # 分批提取数据\n",
    "        while True:\n",
    "            rows = cur.fetchmany(1000)  # 每次提取 1000 行\n",
    "            if not rows:\n",
    "                break\n",
    "            \n",
    "            # 获取列名并创建 DataFrame\n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            batch_df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "            # 去除重复行\n",
    "            batch_df.drop_duplicates(inplace=True)\n",
    "\n",
    "            # 写入 CSV 文件\n",
    "            batch_df.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "            \n",
    "            # 释放内存\n",
    "            del batch_df\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"完成 {query[:50]} 的记录写入\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03353954-1eb3-4bd0-9eda-9546efa60589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成 \n",
      "SELECT DISTINCT \n",
      "    mb.isolate_num, mb.dilution_ 的记录写入\n",
      "完成 \n",
      "SELECT DISTINCT \n",
      "    mb.isolate_num, mb.dilution_ 的记录写入\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(database=\"mimiciii\",\n",
    "                       user=\"postgres\",\n",
    "                       password=\"123456\",\n",
    "                       host=\"localhost\",\n",
    "                       port=\"5433\")\n",
    "\n",
    "# 查询语句\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    mb.isolate_num, mb.dilution_value, mb.interpretation,\n",
    "    lf.*,\n",
    "    bl.hadm_id,\n",
    "    1 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN labs_first_day lf ON lf.icustay_id = (\n",
    "    SELECT icustay_id\n",
    "    FROM icustay_detail icd\n",
    "    WHERE icd.hadm_id = d.hadm_id\n",
    "    LIMIT 1\n",
    ")\n",
    "JOIN microbiologyevents mb ON mb.hadm_id = d.hadm_id\n",
    "WHERE di.long_title ILIKE '%renal fail%' \n",
    "   OR di.long_title ILIKE '%kidney fail%'\n",
    "   OR di.long_title ILIKE '%liver fail%'\n",
    "   OR di.long_title ILIKE '%spleen fail%'\n",
    "LIMIT 30000;\n",
    "'''\n",
    "\n",
    "query_negative = '''\n",
    "SELECT DISTINCT \n",
    "    mb.isolate_num, mb.dilution_value, mb.interpretation,\n",
    "    lf.*,\n",
    "    bl.hadm_id,\n",
    "    0 AS match_flag\n",
    "FROM blood_gas_first_day_arterial bl\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = bl.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN labs_first_day lf ON lf.icustay_id = (\n",
    "    SELECT icustay_id\n",
    "    FROM icustay_detail icd\n",
    "    WHERE icd.hadm_id = bl.hadm_id\n",
    "    LIMIT 1\n",
    ")\n",
    "LEFT JOIN microbiologyevents mb ON mb.hadm_id = bl.hadm_id\n",
    "WHERE di.icd9_code IS NULL\n",
    "LIMIT 30000;\n",
    "'''\n",
    "\n",
    "output_file = \"aki_data4.csv\"\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    queries = [query_positive, query_negative]\n",
    "\n",
    "    for query in queries:\n",
    "        cur.execute(query)\n",
    "        \n",
    "        # 分批提取数据\n",
    "        while True:\n",
    "            rows = cur.fetchmany(1000)  # 每次提取 1000 行\n",
    "            if not rows:\n",
    "                break\n",
    "            \n",
    "            # 获取列名并创建 DataFrame\n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            batch_df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "            # 去除重复行\n",
    "            batch_df.drop_duplicates(inplace=True)\n",
    "\n",
    "            # 写入 CSV 文件\n",
    "            batch_df.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "            \n",
    "            # 释放内存\n",
    "            del batch_df\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"完成 {query[:50]} 的记录写入\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e675d9-18d8-4dbe-90ea-d0ed92b6e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后的数据已保存到 cleaned_aki_data4.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'aki_data4.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'charttime',\n",
    "                     'icd9_code', 'long_title', ]  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "# 2. 去除重复列\n",
    "df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.duplicated()]\n",
    "\n",
    "# 3. 将性别转换为数值类型\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "# 4. 对非文本类型的分类特征进行编码\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# 5. 去除缺失值比例超过 50% 的列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 6. 填充剩余的缺失值\n",
    "imputer = SimpleImputer(strategy='median')  # 使用中位数填充\n",
    "df_cleaned[df_cleaned.columns] = imputer.fit_transform(df_cleaned)\n",
    "\n",
    "# 7. 提取数值特征并标准化\n",
    "features = df_cleaned.drop(columns=['match_flag'])  # 假设 'match_flag' 是标签列\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 进一步归一化到 [0, 1] 范围\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "# 转换为 DataFrame 并添加回标签列\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final['match_flag'] = df_cleaned['match_flag'].values\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_aki_data4.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2173e5e-3e9d-40ab-b9c1-e6255342b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 1991 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 2534 条\n",
      "数据集已保存到 ./data/weight_first_day.csv\n",
      "总样本数：4525（阳性 1991，阴性 2534）\n",
      "清洗后的数据已保存到 ./data/cleaned_weight_first_day.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# 阳性样本查询\n",
    "query_positive = '''\n",
    "SELECT DISTINCT \n",
    "    wfd.weight,\n",
    "    p.gender,\n",
    "    1 AS match_flag\n",
    "FROM weight_first_day wfd\n",
    "JOIN icustays ic ON ic.icustay_id = wfd.icustay_id\n",
    "JOIN diagnoses_icd d ON d.hadm_id = ic.hadm_id\n",
    "JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE (di.long_title ILIKE '% renal fail%' \n",
    "    OR di.long_title ILIKE '%kidney fail%'\n",
    "    OR di.long_title ILIKE '%liver fail%'\n",
    "    OR di.long_title ILIKE '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 优化后的阴性样本查询\n",
    "query_negative = '''\n",
    "SELECT * FROM (\n",
    "    SELECT DISTINCT \n",
    "        wfd.weight,\n",
    "        p.gender,\n",
    "        0 AS match_flag\n",
    "    FROM weight_first_day wfd\n",
    "    JOIN icustays ic ON ic.icustay_id = wfd.icustay_id\n",
    "    JOIN diagnoses_icd d ON d.hadm_id = ic.hadm_id\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    JOIN patients p ON p.subject_id = d.subject_id\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM diagnoses_icd d2\n",
    "        JOIN d_icd_diagnoses di2 ON di2.icd9_code = d2.icd9_code\n",
    "        WHERE d2.hadm_id = d.hadm_id\n",
    "          AND (di2.long_title ILIKE '% renal fail%' \n",
    "              OR di2.long_title ILIKE '%kidney fail%'\n",
    "              OR di2.long_title ILIKE '%liver fail%'\n",
    "              OR di2.long_title ILIKE '%spleen fail%')\n",
    "    )\n",
    ") AS sub\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 20000;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 保存结果\n",
    "    output_file = \"./data/weight_first_day.csv\"\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"数据集已保存到 {output_file}\")\n",
    "    print(f\"总样本数：{len(combined_df)}（阳性 {len(positive_df)}，阴性 {len(negative_df)}）\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "        \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  # 启用 IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "file_path = './data/weight_first_day.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['row_id', 'subject_id',  'icustay_id', 'charttime',\n",
    "                     'icd9_code', 'long_title', 'subject_id1', 'hadm_id1', 'expire_flag', 'short_title', 'row_id1', 'icd9_code1'\n",
    "                    ,'dod','dob','dod_ssn','dod_hosp']  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "# 2. 去除重复列\n",
    "df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.duplicated()]\n",
    "\n",
    "# 3. 将性别转换为数值类型\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "# 4. 对非文本类型的分类特征进行编码\n",
    "categorical_cols = []  # 记录分类列用于后续处理\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['intime', 'outtime']:  # 跳过时间列\n",
    "        categorical_cols.append(col)\n",
    "        label_encoder = LabelEncoder()\n",
    "        df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# 5. 将时间列转换为Unix时间戳并处理缺失\n",
    "time_cols = ['intime', 'outtime']\n",
    "for col in time_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        # 转换为datetime并处理无效值\n",
    "        df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')\n",
    "        # 转换为Unix时间戳（秒）\n",
    "        df_cleaned[col] = (df_cleaned[col].astype('int64') // 10**9).replace(-9223372036854775808, np.nan)\n",
    "\n",
    "# 6. 删除高缺失率列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 7. 使用XGBoost进行缺失值填补\n",
    "from sklearn.impute import IterativeImputer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 对所有列进行填补（包括时间列和标签列）\n",
    "columns_to_impute = df_cleaned.columns.tolist()\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=100, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df_cleaned[columns_to_impute] = imputer.fit_transform(df_cleaned[columns_to_impute])\n",
    "\n",
    "# 8. 后处理：分类列和标签列取整\n",
    "for col in categorical_cols + ['match_flag']:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].round().astype(int)\n",
    "\n",
    "# 时间列取整（确保为整数时间戳）\n",
    "for col in time_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].round().astype('int64')\n",
    "\n",
    "# 9. 标准化处理（排除指定列）\n",
    "# features = df_cleaned.drop(columns=['match_flag', 'intime', 'outtime',])\n",
    "features = df_cleaned.drop(columns=['match_flag'])\n",
    "# 标准化+归一化\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "# for col in ['intime', 'outtime','match_flag']:\n",
    "for col in ['match_flag']:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_final[col] = df_cleaned[col].values\n",
    "\n",
    "# 保存结果\n",
    "output_file = './data/cleaned_weight_first_day.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6ba7b1-897e-4e2b-ba00-c1b266f03b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 12888 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 12888 条\n",
      "动态保留的列：['hadm_id', 'aniongap_min', 'aniongap_max', 'albumin_min', 'albumin_max', 'bands_min', 'bands_max', 'bicarbonate_min', 'bicarbonate_max', 'bilirubin_min', 'bilirubin_max', 'creatinine_min', 'creatinine_max', 'chloride_min', 'chloride_max', 'glucose_min', 'glucose_max', 'hematocrit_min', 'hematocrit_max', 'hemoglobin_min', 'hemoglobin_max', 'lactate_min', 'lactate_max', 'platelet_min', 'platelet_max', 'potassium_min', 'potassium_max', 'ptt_min', 'ptt_max', 'inr_min', 'inr_max', 'pt_min', 'pt_max', 'sodium_min', 'sodium_max', 'bun_min', 'bun_max', 'wbc_min', 'wbc_max', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/labs_first_day_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 ./data/cleaned_labs_first_day.csv\n",
      "数据特征分布：\n",
      "              hadm_id  aniongap_min  aniongap_max   albumin_min   albumin_max  \\\n",
      "count   24087.000000  24087.000000  24087.000000  24087.000000  24087.000000   \n",
      "mean   149804.298252      0.291386      0.258762      0.337905      0.350923   \n",
      "std     28875.239187      0.081605      0.089616      0.113909      0.117026   \n",
      "min    100001.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    124775.500000      0.232558      0.196429      0.256857      0.264151   \n",
      "50%    149804.000000      0.279070      0.250000      0.322706      0.337007   \n",
      "75%    174792.500000      0.325581      0.285714      0.410794      0.429168   \n",
      "max    199999.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "          bands_min     bands_max  bicarbonate_min  bicarbonate_max  \\\n",
      "count  24087.000000  24087.000000     24087.000000     24087.000000   \n",
      "mean       0.127821      0.137647         0.401015         0.408652   \n",
      "std        0.088750      0.103496         0.103129         0.096324   \n",
      "min        0.000000      0.000000         0.000000         0.000000   \n",
      "25%        0.063157      0.059493         0.340000         0.354167   \n",
      "50%        0.103902      0.105932         0.413780         0.398553   \n",
      "75%        0.164606      0.179331         0.460000         0.458333   \n",
      "max        1.000000      1.000000         1.000000         1.000000   \n",
      "\n",
      "       bilirubin_min  ...        pt_min        pt_max    sodium_min  \\\n",
      "count   24087.000000  ...  24087.000000  24087.000000  24087.000000   \n",
      "mean        0.028126  ...      0.145541      0.156661      0.764243   \n",
      "std         0.043117  ...      0.208303      0.218070      0.030042   \n",
      "min         0.000000  ...      0.000000      0.000000      0.000000   \n",
      "25%         0.010171  ...      0.035915      0.034763      0.751117   \n",
      "50%         0.016942  ...      0.045775      0.048420      0.762430   \n",
      "75%         0.032267  ...      0.100000      0.132223      0.779399   \n",
      "max         1.000000  ...      1.000000      1.000000      1.000000   \n",
      "\n",
      "         sodium_max       bun_min       bun_max       wbc_min       wbc_max  \\\n",
      "count  24087.000000  24087.000000  24087.000000  24087.000000  24087.000000   \n",
      "mean       0.502754      0.109493      0.117097      0.019010      0.016591   \n",
      "std        0.061202      0.091658      0.098549      0.015749      0.015105   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.470588      0.051383      0.051852      0.011812      0.010395   \n",
      "50%        0.505882      0.075099      0.081481      0.016849      0.014292   \n",
      "75%        0.529412      0.138340      0.151852      0.022929      0.019962   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "             gender    match_flag  \n",
      "count  24087.000000  24087.000000  \n",
      "mean       0.563914      0.472537  \n",
      "std        0.495908      0.499256  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      0.000000  \n",
      "50%        1.000000      0.000000  \n",
      "75%        1.000000      1.000000  \n",
      "max        1.000000      1.000000  \n",
      "\n",
      "[8 rows x 41 columns]\n",
      "\n",
      "验证结果：\n",
      "总样本数：24087\n",
      "阳性样本数：11382\n",
      "阴性样本数：12705\n",
      "唯一hadm_id数量：24087\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "SELECT DISTINCT lfd.*,\n",
    "                p.gender,\n",
    "                1 AS match_flag\n",
    "FROM labs_first_day lfd\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = lfd.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE (di.long_title ILIKE '% renal fail%' \n",
    "    OR di.long_title ILIKE '%kidney fail%'\n",
    "    OR di.long_title ILIKE '%liver fail%'\n",
    "    OR di.long_title ILIKE '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%' \n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT \n",
    "    lfd.*,\n",
    "    p.gender,\n",
    "    0 AS match_flag \n",
    "FROM labs_first_day lfd\n",
    "LEFT JOIN patients p ON p.subject_id = lfd.subject_id\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1 \n",
    "    FROM positive_hadm ph \n",
    "    WHERE ph.hadm_id = lfd.hadm_id\n",
    ")\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "'''\n",
    "\n",
    "# ================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "    \n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "    \n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "    \n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "    \n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/labs_first_day_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/labs_first_day_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_labs_first_day.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81288f52-35d1-43b5-a4f2-17dca02b1262",
   "metadata": {},
   "source": [
    "#### LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23229842-6b2f-4ade-bb75-608a5fd0038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 12888 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 12888 条\n",
      "动态保留的列：['hadm_id', 'aniongap_min', 'aniongap_max', 'albumin_min', 'albumin_max', 'bands_min', 'bands_max', 'bicarbonate_min', 'bicarbonate_max', 'bilirubin_min', 'bilirubin_max', 'creatinine_min', 'creatinine_max', 'chloride_min', 'chloride_max', 'glucose_min', 'glucose_max', 'hematocrit_min', 'hematocrit_max', 'hemoglobin_min', 'hemoglobin_max', 'lactate_min', 'lactate_max', 'platelet_min', 'platelet_max', 'potassium_min', 'potassium_max', 'ptt_min', 'ptt_max', 'inr_min', 'inr_max', 'pt_min', 'pt_max', 'sodium_min', 'sodium_max', 'bun_min', 'bun_max', 'wbc_min', 'wbc_max', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/labs_first_day_raw.csv\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5827\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5572\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5572\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5632\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5661\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5571\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5571\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5570\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5570\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5750\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5731\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5743\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5741\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5732\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5736\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5751\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5750\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5751\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5740\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5872\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5872\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5941\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5921\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5900\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5884\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5811\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5814\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5982\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5931\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5825\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5823\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5892\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5835\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6292\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6288\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5925\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5925\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6698\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6690\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7919\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7614\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7614\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7636\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7665\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7542\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7542\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7511\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7511\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7553\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7534\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7514\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7512\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7437\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7441\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7371\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7370\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7199\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7188\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7178\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7178\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7146\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7126\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7106\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7090\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6838\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6841\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7006\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6955\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6846\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6844\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6760\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6703\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7260\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7256\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6347\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6347\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7094\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7086\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8244\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7942\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7942\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8001\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8030\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7911\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7911\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7913\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7913\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7575\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7556\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7533\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7531\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7459\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7463\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7394\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7218\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7197\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7197\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7155\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7135\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7114\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7098\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6836\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7003\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6952\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6836\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6834\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6771\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6714\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7381\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7377\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6374\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6374\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7486\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7478\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8155\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8155\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8222\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8251\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8153\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8153\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8140\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8140\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7625\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7606\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7592\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7590\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7506\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7510\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7444\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7443\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7266\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7255\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7217\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7217\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7173\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7153\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7133\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7117\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001683 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6831\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6834\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7003\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6952\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6837\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6779\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6722\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7435\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7431\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6390\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6390\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7845\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7837\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8569\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8231\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8231\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8268\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8297\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8201\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8201\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8195\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8195\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7633\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7614\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7597\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7595\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7513\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7517\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7463\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7462\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7285\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7274\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7239\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7239\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7193\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7173\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7153\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7137\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6828\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6831\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7004\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6953\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6833\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6831\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001520 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6781\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6724\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7456\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7452\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6389\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6389\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8190\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8182\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8761\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8423\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8423\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8464\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8493\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8399\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8399\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8392\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8392\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7638\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7619\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7598\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7596\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7524\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7528\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7469\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7468\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7292\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7281\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7241\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7241\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7195\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7175\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7154\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7138\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6825\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6828\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6999\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6948\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6836\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6834\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001666 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6808\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6751\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7483\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7479\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6405\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6405\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8358\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002846 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8570\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8570\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8618\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8647\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8546\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8546\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8542\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8542\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7651\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7632\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7611\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7609\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7535\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7539\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7482\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7481\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7306\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7295\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7261\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7261\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7203\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7183\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7161\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7145\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6816\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6819\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6989\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6938\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6833\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6831\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6813\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6756\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7498\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7494\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6421\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6421\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8592\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8584\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9032\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8719\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8719\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8772\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8801\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8705\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8705\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8700\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8700\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7654\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7635\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7616\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7614\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7540\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7544\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7490\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7489\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7313\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7302\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7264\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7264\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7224\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7204\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7182\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7166\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6827\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6830\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6992\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6941\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6838\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6836\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6816\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6759\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7500\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7496\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6423\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6423\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8690\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8682\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9099\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8780\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8780\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8828\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8857\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8763\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8763\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8758\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8758\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7657\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7638\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7619\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7617\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7542\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7546\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7488\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7487\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7312\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7301\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7265\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7265\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7220\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7200\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7182\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7166\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6818\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6821\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6987\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6936\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6835\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6833\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6829\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6772\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7518\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7514\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8717\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8709\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9106\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.565818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8795\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 31.002918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8795\n",
      "[LightGBM] [Info] Number of data points in the train set: 23543, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 36.229351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8842\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.455962\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8871\n",
      "[LightGBM] [Info] Number of data points in the train set: 23473, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 12.049461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8783\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 211.728919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8783\n",
      "[LightGBM] [Info] Number of data points in the train set: 23445, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 251.112949\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8771\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.050143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8771\n",
      "[LightGBM] [Info] Number of data points in the train set: 23430, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 14.241927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7666\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.851573\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7647\n",
      "[LightGBM] [Info] Number of data points in the train set: 22303, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.769988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7626\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 136.505061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7624\n",
      "[LightGBM] [Info] Number of data points in the train set: 22290, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 140.159937\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7551\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 102.158069\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7555\n",
      "[LightGBM] [Info] Number of data points in the train set: 22246, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 106.864964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7494\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 21.928921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7493\n",
      "[LightGBM] [Info] Number of data points in the train set: 22157, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 24.675403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7319\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 13.429301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7308\n",
      "[LightGBM] [Info] Number of data points in the train set: 21931, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 16.628015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7288\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 112.216913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7288\n",
      "[LightGBM] [Info] Number of data points in the train set: 21806, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 186.608053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7232\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.561225\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7212\n",
      "[LightGBM] [Info] Number of data points in the train set: 21795, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.889309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7191\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 30.088281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7175\n",
      "[LightGBM] [Info] Number of data points in the train set: 21794, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 35.535881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6812\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 15.439367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6815\n",
      "[LightGBM] [Info] Number of data points in the train set: 19546, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 17.893431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6985\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.430524\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6934\n",
      "[LightGBM] [Info] Number of data points in the train set: 19541, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.777715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6819\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 32.542075\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6817\n",
      "[LightGBM] [Info] Number of data points in the train set: 19457, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 45.310223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6816\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1.861628\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6759\n",
      "[LightGBM] [Info] Number of data points in the train set: 13995, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.165362\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7520\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.080329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7516\n",
      "[LightGBM] [Info] Number of data points in the train set: 12048, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2.409396\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6425\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.059531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6425\n",
      "[LightGBM] [Info] Number of data points in the train set: 9296, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3.155207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8726\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.387408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8718\n",
      "[LightGBM] [Info] Number of data points in the train set: 4360, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 9.165459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 ./data/cleaned_labs_first_day_lgbm.csv\n",
      "数据特征分布：\n",
      "              hadm_id  aniongap_min  aniongap_max   albumin_min   albumin_max  \\\n",
      "count   24112.000000  24112.000000  24112.000000  24112.000000  24112.000000   \n",
      "mean   149939.064947      0.293499      0.203785      0.460860      0.468080   \n",
      "std     28874.018860      0.082349      0.096612      0.107144      0.103541   \n",
      "min    100001.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    124940.750000      0.232558      0.134615      0.414522      0.424700   \n",
      "50%    149953.500000      0.279070      0.192308      0.464006      0.464475   \n",
      "75%    174925.250000      0.347406      0.245201      0.524816      0.528467   \n",
      "max    199999.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "          bands_min     bands_max  bicarbonate_min  bicarbonate_max  \\\n",
      "count  24112.000000  24112.000000     24112.000000     24112.000000   \n",
      "mean       0.116819      0.120521         0.404246         0.407786   \n",
      "std        0.071711      0.077247         0.109473         0.096334   \n",
      "min        0.000000      0.000000         0.000000         0.000000   \n",
      "25%        0.078341      0.078445         0.340426         0.354167   \n",
      "50%        0.095905      0.096268         0.414623         0.395833   \n",
      "75%        0.146901      0.153955         0.468085         0.458333   \n",
      "max        1.000000      1.000000         1.000000         1.000000   \n",
      "\n",
      "       bilirubin_min  ...        pt_min        pt_max    sodium_min  \\\n",
      "count   24112.000000  ...  24112.000000  24112.000000  24112.000000   \n",
      "mean        0.024125  ...      0.091358      0.071279      0.763043   \n",
      "std         0.042143  ...      0.038444      0.082461      0.030121   \n",
      "min         0.000000  ...      0.000000      0.000000      0.000000   \n",
      "25%         0.007605  ...      0.071879      0.031938      0.747417   \n",
      "50%         0.017744  ...      0.082041      0.046842      0.764638   \n",
      "75%         0.026616  ...      0.102413      0.076683      0.781860   \n",
      "max         1.000000  ...      1.000000      1.000000      1.000000   \n",
      "\n",
      "         sodium_max       bun_min       bun_max       wbc_min       wbc_max  \\\n",
      "count  24112.000000  24112.000000  24112.000000  24112.000000  24112.000000   \n",
      "mean       0.434271      0.115302      0.123710      0.019013      0.016617   \n",
      "std        0.067165      0.090617      0.096609      0.015182      0.014736   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.405405      0.051383      0.055556      0.011985      0.010513   \n",
      "50%        0.432432      0.090909      0.100000      0.016849      0.014411   \n",
      "75%        0.459459      0.138340      0.151852      0.023102      0.020080   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "             gender    match_flag  \n",
      "count  24112.000000  24112.000000  \n",
      "mean       0.565818      0.472047  \n",
      "std        0.495659      0.499228  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      0.000000  \n",
      "50%        1.000000      0.000000  \n",
      "75%        1.000000      1.000000  \n",
      "max        1.000000      1.000000  \n",
      "\n",
      "[8 rows x 41 columns]\n",
      "\n",
      "验证结果：\n",
      "总样本数：24112\n",
      "阳性样本数：11382\n",
      "阴性样本数：12730\n",
      "唯一hadm_id数量：24112\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "SELECT DISTINCT lfd.*,\n",
    "                p.gender,\n",
    "                1 AS match_flag\n",
    "FROM labs_first_day lfd\n",
    "LEFT JOIN diagnoses_icd d ON d.hadm_id = lfd.hadm_id\n",
    "LEFT JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "LEFT JOIN patients p ON p.subject_id = d.subject_id\n",
    "WHERE (di.long_title ILIKE '% renal fail%'\n",
    "    OR di.long_title ILIKE '%kidney fail%'\n",
    "    OR di.long_title ILIKE '%liver fail%'\n",
    "    OR di.long_title ILIKE '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%'\n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT\n",
    "    lfd.*,\n",
    "    p.gender,\n",
    "    0 AS match_flag\n",
    "FROM labs_first_day lfd\n",
    "LEFT JOIN patients p ON p.subject_id = lfd.subject_id\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM positive_hadm ph\n",
    "    WHERE ph.hadm_id = lfd.hadm_id\n",
    ")\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "'''\n",
    "\n",
    "\n",
    "#================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "\n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "\n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "\n",
    "    return keep_columns\n",
    "\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "\n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "\n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "\n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "\n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/labs_first_day_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/labs_first_day_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=LGBMRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_labs_first_day_lgbm.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41556db3-849f-4ac6-8127-b5932c28f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 148751 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 148751 条\n",
      "动态保留的列：['hadm_id', 'spec_itemid', 'org_itemid', 'isolate_num', 'ab_itemid', 'dilution_text', 'dilution_value', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/microbiologyevents_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 ./data/cleaned_microbiologyevents.csv\n",
      "数据特征分布：\n",
      "              hadm_id   spec_itemid    org_itemid   isolate_num     ab_itemid  \\\n",
      "count   40786.000000  40786.000000  40786.000000  40786.000000  40786.000000   \n",
      "mean   149950.636689      0.474512      0.361828      0.050977      0.512851   \n",
      "std     28847.476642      0.361430      0.243739      0.044876      0.206961   \n",
      "min    100001.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    124962.250000      0.109890      0.158974      0.036316      0.342567   \n",
      "50%    150013.500000      0.483516      0.352154      0.037114      0.474153   \n",
      "75%    174886.750000      0.846154      0.421053      0.060519      0.695523   \n",
      "max    199999.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "       dilution_text  dilution_value        gender    match_flag  \n",
      "count   40786.000000    40786.000000  40786.000000  40786.000000  \n",
      "mean        0.004159        0.004464      0.548178      0.267567  \n",
      "std         0.017880        0.017844      0.497680      0.442696  \n",
      "min         0.000000        0.000000      0.000000      0.000000  \n",
      "25%         0.001505        0.001953      0.000000      0.000000  \n",
      "50%         0.001618        0.001953      1.000000      0.000000  \n",
      "75%         0.001702        0.001953      1.000000      1.000000  \n",
      "max         1.000000        1.000000      1.000000      1.000000  \n",
      "\n",
      "验证结果：\n",
      "总样本数：40786\n",
      "阳性样本数：10913\n",
      "阴性样本数：29873\n",
      "唯一hadm_id数量：40786\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "select  distinct m.hadm_id,\n",
    "\t\t\t\tm.spec_itemid,\n",
    "\t\t\t\tm.org_itemid,\n",
    "\t\t\t\tm.isolate_num,\n",
    "\t\t\t\tm.ab_itemid,\n",
    "\t\t\t\tm.dilution_text,\n",
    "\t\t\t\tm.dilution_value,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t1 AS match_flag\n",
    "from microbiologyevents m\n",
    "left join labevents l on l.hadm_id=m.hadm_id\n",
    "left join diagnoses_icd d on d.hadm_id=m.hadm_id\n",
    "left join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "left join patients p  on p.subject_id = d.subject_id\n",
    "where l.itemid = 50912\n",
    "\tAND l.valuenum <= 150\n",
    "\tand(di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%' \n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT * FROM (\n",
    "    SELECT \n",
    "        DISTINCT m.hadm_id,\n",
    "        m.spec_itemid,\n",
    "        m.org_itemid,\n",
    "        m.isolate_num,\n",
    "        m.ab_itemid,\n",
    "        m.dilution_text,\n",
    "        m.dilution_value,\n",
    "        p.gender,\n",
    "        0 AS match_flag \n",
    "    FROM microbiologyevents m\n",
    "    LEFT JOIN patients p ON p.subject_id = m.subject_id\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM positive_hadm ph \n",
    "        WHERE ph.hadm_id = m.hadm_id\n",
    "    )\n",
    ") AS subquery\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "\n",
    "'''\n",
    "\n",
    "# ================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "    \n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "    \n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "    \n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "    \n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/microbiologyevents_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/microbiologyevents_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "df['dilution_text'] = df['dilution_text'].str.extract(r'([\\d\\.]+)').astype(float)\n",
    "\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag','dilution_comparison']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_microbiologyevents.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc73961-e6c9-4cf7-b27e-75e0ad78159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 12637 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 12637 条\n",
      "动态保留的列：['hadm_id', 'heartrate_min', 'heartrate_max', 'heartrate_mean', 'sysbp_min', 'sysbp_max', 'sysbp_mean', 'diasbp_min', 'diasbp_max', 'diasbp_mean', 'meanbp_min', 'meanbp_max', 'meanbp_mean', 'resprate_min', 'resprate_max', 'resprate_mean', 'tempc_min', 'tempc_max', 'tempc_mean', 'spo2_min', 'spo2_max', 'spo2_mean', 'glucose_min', 'glucose_max', 'glucose_mean', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/vitals_first_day_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 ./data/cleaned_vitals_first_day.csv\n",
      "数据特征分布：\n",
      "              hadm_id  heartrate_min  heartrate_max  heartrate_mean  \\\n",
      "count   23663.000000   23663.000000   23663.000000    23663.000000   \n",
      "mean   150002.875248       0.420695       0.340941        0.390233   \n",
      "std     28846.078833       0.120508       0.122430        0.146731   \n",
      "min    100001.000000       0.000000       0.000000        0.000000   \n",
      "25%    124933.500000       0.339491       0.254808        0.289033   \n",
      "50%    150140.000000       0.401064       0.322115        0.365700   \n",
      "75%    175018.000000       0.468234       0.408654        0.458193   \n",
      "max    199999.000000       1.000000       1.000000        1.000000   \n",
      "\n",
      "          sysbp_min     sysbp_max    sysbp_mean    diasbp_min    diasbp_max  \\\n",
      "count  23663.000000  23663.000000  23663.000000  23663.000000  23663.000000   \n",
      "mean       0.490771      0.372994      0.468835      0.364095      0.207300   \n",
      "std        0.107429      0.087617      0.114200      0.109250      0.069781   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.424301      0.314079      0.386719      0.296270      0.159851   \n",
      "50%        0.490728      0.364621      0.446860      0.366643      0.200743   \n",
      "75%        0.551619      0.422383      0.536493      0.428220      0.243140   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "        diasbp_mean  ...     tempc_max    tempc_mean      spo2_min  \\\n",
      "count  23663.000000  ...  23663.000000  23663.000000  23663.000000   \n",
      "mean       0.360757  ...      0.563669      0.652674      0.900662   \n",
      "std        0.099413  ...      0.068954      0.068289      0.102438   \n",
      "min        0.000000  ...      0.000000      0.000000      0.000000   \n",
      "25%        0.296005  ...      0.517019      0.611354      0.889447   \n",
      "50%        0.352265  ...      0.558418      0.651528      0.929648   \n",
      "75%        0.415740  ...      0.604416      0.694306      0.949749   \n",
      "max        1.000000  ...      1.000000      1.000000      1.000000   \n",
      "\n",
      "           spo2_max     spo2_mean   glucose_min   glucose_max  glucose_mean  \\\n",
      "count  23663.000000  23663.000000  23663.000000  23663.000000  23663.000000   \n",
      "mean       0.952259      0.944203      0.220443      0.017537      0.167909   \n",
      "std        0.032100      0.053904      0.074261      0.011291      0.059889   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.942053      0.927130      0.177745      0.011813      0.131406   \n",
      "50%        0.965605      0.954472      0.211136      0.015817      0.157162   \n",
      "75%        0.965605      0.975935      0.252875      0.019722      0.189081   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "             gender    match_flag  \n",
      "count  23663.000000  23663.000000  \n",
      "mean       0.565778      0.473651  \n",
      "std        0.495665      0.499316  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      0.000000  \n",
      "50%        1.000000      0.000000  \n",
      "75%        1.000000      1.000000  \n",
      "max        1.000000      1.000000  \n",
      "\n",
      "[8 rows x 27 columns]\n",
      "\n",
      "验证结果：\n",
      "总样本数：23663\n",
      "阳性样本数：11208\n",
      "阴性样本数：12455\n",
      "唯一hadm_id数量：23663\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "select  distinct vfd.hadm_id,\n",
    "\t\t\t\tvfd.heartrate_min,vfd.heartrate_max,vfd.heartrate_mean,\n",
    "\t\t\t\tvfd.sysbp_min,vfd.sysbp_max,vfd.sysbp_mean,\n",
    "\t\t\t\tvfd.diasbp_min,vfd.diasbp_max,vfd.diasbp_mean,\n",
    "\t\t\t\tvfd.meanbp_min,vfd.meanbp_max,vfd.meanbp_mean,\n",
    "\t\t\t\tvfd.resprate_min,vfd.resprate_max,vfd.resprate_mean,\n",
    "\t\t\t\tvfd.tempc_min,vfd.tempc_max,vfd.tempc_mean,\n",
    "\t\t\t\tvfd.spo2_min,vfd.spo2_max,vfd.spo2_mean,\n",
    "\t\t\t\tvfd.glucose_min,vfd.glucose_max,vfd.glucose_mean,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t1 AS match_flag\n",
    "from vitals_first_day vfd\n",
    "left join labevents l on l.hadm_id=vfd.hadm_id\n",
    "left join diagnoses_icd d on d.hadm_id=vfd.hadm_id\n",
    "left join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "left join patients p  on p.subject_id = d.subject_id\n",
    "where l.itemid = 50912\n",
    "\tAND l.valuenum <= 150\n",
    "\tand(di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%' \n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT * FROM (\n",
    "select  distinct vfd.hadm_id,\n",
    "\t\t\t\tvfd.heartrate_min,vfd.heartrate_max,vfd.heartrate_mean,\n",
    "\t\t\t\tvfd.sysbp_min,vfd.sysbp_max,vfd.sysbp_mean,\n",
    "\t\t\t\tvfd.diasbp_min,vfd.diasbp_max,vfd.diasbp_mean,\n",
    "\t\t\t\tvfd.meanbp_min,vfd.meanbp_max,vfd.meanbp_mean,\n",
    "\t\t\t\tvfd.resprate_min,vfd.resprate_max,vfd.resprate_mean,\n",
    "\t\t\t\tvfd.tempc_min,vfd.tempc_max,vfd.tempc_mean,\n",
    "\t\t\t\tvfd.spo2_min,vfd.spo2_max,vfd.spo2_mean,\n",
    "\t\t\t\tvfd.glucose_min,vfd.glucose_max,vfd.glucose_mean,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t0 AS match_flag\n",
    "from vitals_first_day vfd\n",
    "    LEFT JOIN patients p ON p.subject_id = vfd.subject_id\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM positive_hadm ph \n",
    "        WHERE ph.hadm_id = vfd.hadm_id\n",
    "    )\n",
    ") AS subquery\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "\n",
    "'''\n",
    "\n",
    "# ================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "    \n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "    \n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "    \n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "    \n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/vitals_first_day_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/vitals_first_day_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "# df['dilution_text'] = df['dilution_text'].str.extract(r'([\\d\\.]+)').astype(float)\n",
    "\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_vitals_first_day.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ee02c3-6879-4ca3-ad11-4c11e49ee582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n",
      "阳性样本获取完成，共 175293 条\n",
      "正在获取阴性样本...\n",
      "阴性样本获取完成，共 175293 条\n",
      "动态保留的列：['hadm_id', 'spec_itemid', 'org_itemid', 'isolate_num', 'ab_itemid', 'dilution_text', 'dilution_value', 'urineoutput', 'gender', 'match_flag']\n",
      "原始数据已保存到 ./data/microbiologyevents_plus_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\impute\\_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 ./data/cleaned_microbiologyevents_plus.csv\n",
      "数据特征分布：\n",
      "              hadm_id   spec_itemid    org_itemid   isolate_num     ab_itemid  \\\n",
      "count   41669.000000  41669.000000  41669.000000  41669.000000  41669.000000   \n",
      "mean   149998.834697      0.476091      0.364639      0.058443      0.535133   \n",
      "std     28895.529377      0.361755      0.224140      0.042242      0.149725   \n",
      "min    100001.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    125011.000000      0.109890      0.235013      0.047250      0.439572   \n",
      "50%    150004.000000      0.483516      0.349648      0.050433      0.525839   \n",
      "75%    175023.000000      0.846154      0.439701      0.058955      0.639695   \n",
      "max    199999.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "       dilution_text  dilution_value   urineoutput        gender    match_flag  \n",
      "count   41669.000000    41669.000000  41669.000000  41669.000000  41669.000000  \n",
      "mean        0.004188        0.004361      0.042469      0.546089      0.261897  \n",
      "std         0.017917        0.017886      0.014416      0.497877      0.439672  \n",
      "min         0.000000        0.000000      0.000000      0.000000      0.000000  \n",
      "25%         0.001579        0.001953      0.033366      0.000000      0.000000  \n",
      "50%         0.001735        0.001953      0.040494      1.000000      0.000000  \n",
      "75%         0.001974        0.001953      0.048473      1.000000      1.000000  \n",
      "max         1.000000        1.000000      1.000000      1.000000      1.000000  \n",
      "\n",
      "验证结果：\n",
      "总样本数：41669\n",
      "阳性样本数：10913\n",
      "阴性样本数：30756\n",
      "唯一hadm_id数量：41669\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "select  distinct m.hadm_id,\n",
    "\t\t\t\tm.spec_itemid,\n",
    "\t\t\t\tm.org_itemid,\n",
    "\t\t\t\tm.isolate_num,\n",
    "\t\t\t\tm.ab_itemid,\n",
    "\t\t\t\tm.dilution_text,\n",
    "\t\t\t\tm.dilution_value,\n",
    "    \t\t\tuofd.urineoutput,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t1 AS match_flag\n",
    "from microbiologyevents m\n",
    "left join labevents l on l.hadm_id=m.hadm_id\n",
    "left join urine_output_first_day uofd on uofd.hadm_id = m.hadm_id\n",
    "left join diagnoses_icd d on d.hadm_id=m.hadm_id\n",
    "left join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "left join patients p  on p.subject_id = d.subject_id\n",
    "where l.itemid = 50912\n",
    "\tAND l.valuenum <= 150\n",
    "\tand(di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%' \n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT * FROM (\n",
    "    SELECT \n",
    "        DISTINCT m.hadm_id,\n",
    "        m.spec_itemid,\n",
    "        m.org_itemid,\n",
    "        m.isolate_num,\n",
    "        m.ab_itemid,\n",
    "        m.dilution_text,\n",
    "        m.dilution_value,\n",
    "        uofd.urineoutput,\n",
    "        p.gender,\n",
    "        0 AS match_flag \n",
    "    FROM microbiologyevents m\n",
    "    left join urine_output_first_day uofd on uofd.hadm_id = m.hadm_id\n",
    "    LEFT JOIN patients p ON p.subject_id = m.subject_id\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM positive_hadm ph \n",
    "        WHERE ph.hadm_id = m.hadm_id\n",
    "    )\n",
    ") AS subquery\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "\n",
    "'''\n",
    "\n",
    "# ================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "    \n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "    \n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "    \n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "    \n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "    \n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "    \n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "    \n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "    \n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/microbiologyevents_plus_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/microbiologyevents_plus_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "df['dilution_text'] = df['dilution_text'].str.extract(r'([\\d\\.]+)').astype(float)\n",
    "\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag','dilution_comparison']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_microbiologyevents_plus.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3ee57-f63d-4f21-af32-1c430afe87bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取阳性样本...\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ================== 数据库配置部分 ==================\n",
    "# 数据库连接配置\n",
    "con = psycopg2.connect(\n",
    "    database=\"mimiciii\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\"\n",
    ")\n",
    "\n",
    "# ================== 修改后的SQL查询 ==================\n",
    "# 新的阳性样本查询\n",
    "query_positive = '''\n",
    "select  distinct lfd.*,\n",
    "\t\t\t\tvfd.hadm_id,\n",
    "\t\t\t\tvfd.heartrate_min,vfd.heartrate_max,vfd.heartrate_mean,\n",
    "\t\t\t\tvfd.sysbp_min,vfd.sysbp_max,vfd.sysbp_mean,\n",
    "\t\t\t\tvfd.diasbp_min,vfd.diasbp_max,vfd.diasbp_mean,\n",
    "\t\t\t\tvfd.meanbp_min,vfd.meanbp_max,vfd.meanbp_mean,\n",
    "\t\t\t\tvfd.resprate_min,vfd.resprate_max,vfd.resprate_mean,\n",
    "\t\t\t\tvfd.tempc_min,vfd.tempc_max,vfd.tempc_mean,\n",
    "\t\t\t\tvfd.spo2_min,vfd.spo2_max,vfd.spo2_mean,\n",
    "\t\t\t\tvfd.glucose_min,vfd.glucose_max,vfd.glucose_mean,\n",
    "\t\t\t\tp.gender,\n",
    "\t\t\t\t1 AS match_flag\n",
    "from vitals_first_day vfd\n",
    "left join labs_first_day lfd on lfd.hadm_id = vfd.hadm_id\n",
    "left join labevents l on l.hadm_id=vfd.hadm_id\n",
    "left join diagnoses_icd d on d.hadm_id=vfd.hadm_id\n",
    "left join d_icd_diagnoses di on di.icd9_code=d.icd9_code\n",
    "left join patients p  on p.subject_id = d.subject_id\n",
    "where l.itemid = 50912\n",
    "\tAND l.valuenum <= 150\n",
    "\tand(di.long_title ilike '% renal fail%' \n",
    "\tor di.long_title ilike '%kidney fail%'\n",
    "\tor di.long_title ilike '%liver fail%'\n",
    "\tor di.long_title ilike '%spleen fail%')\n",
    "'''\n",
    "\n",
    "# 新的阴性样本查询（动态匹配阳性样本数量）\n",
    "query_negative = '''\n",
    "WITH positive_hadm AS (\n",
    "    SELECT DISTINCT d.hadm_id\n",
    "    FROM diagnoses_icd d\n",
    "    JOIN d_icd_diagnoses di ON di.icd9_code = d.icd9_code\n",
    "    WHERE di.long_title ILIKE '% renal fail%'\n",
    "        OR di.long_title ILIKE '%kidney fail%'\n",
    "        OR di.long_title ILIKE '%liver fail%'\n",
    "        OR di.long_title ILIKE '%spleen fail%'\n",
    ")\n",
    "SELECT\n",
    "    lfd.*,\n",
    "    vfd.hadm_id,\n",
    "    vfd.heartrate_min,vfd.heartrate_max,vfd.heartrate_mean,\n",
    "    vfd.sysbp_min,vfd.sysbp_max,vfd.sysbp_mean,\n",
    "    vfd.diasbp_min,vfd.diasbp_max,vfd.diasbp_mean,\n",
    "    vfd.meanbp_min,vfd.meanbp_max,vfd.meanbp_mean,\n",
    "    vfd.resprate_min,vfd.resprate_max,vfd.resprate_mean,\n",
    "    vfd.tempc_min,vfd.tempc_max,vfd.tempc_mean,\n",
    "    vfd.spo2_min,vfd.spo2_max,vfd.spo2_mean,\n",
    "    vfd.glucose_min,vfd.glucose_max,vfd.glucose_mean,\n",
    "    p.gender,\n",
    "    0 AS match_flag\n",
    "FROM vitals_first_day vfd\n",
    "LEFT JOIN labs_first_day lfd on lfd.hadm_id = vfd.hadm_id\n",
    "LEFT JOIN patients p ON p.subject_id = lfd.subject_id\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM positive_hadm ph\n",
    "    WHERE ph.hadm_id = lfd.hadm_id\n",
    ")\n",
    "ORDER BY RANDOM()\n",
    "LIMIT (SELECT COUNT(*) FROM positive_hadm);\n",
    "'''\n",
    "\n",
    "\n",
    "#================== 数据获取部分 ==================\n",
    "def get_dynamic_keep_columns(cursor_description):\n",
    "    \"\"\"\n",
    "    从数据库查询结果中动态提取列名，并确保保留关键列。\n",
    "    :param cursor_description: 数据库查询结果的description属性\n",
    "    :return: 动态生成的keep_columns列表\n",
    "    \"\"\"\n",
    "    # 提取所有列名\n",
    "    all_columns = [desc[0] for desc in cursor_description]\n",
    "\n",
    "    # 确保关键列存在\n",
    "    required_columns = ['hadm_id', 'match_flag']\n",
    "    for col in required_columns:\n",
    "        if col not in all_columns:\n",
    "            raise ValueError(f\"关键列 {col} 不在查询结果中！\")\n",
    "\n",
    "    # 动态构建keep_columns\n",
    "    # 排除不需要的列（可根据需要调整）\n",
    "    exclude_columns = ['row_id', 'subject_id', 'icustay_id']  # 示例：排除这些列\n",
    "    keep_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "\n",
    "    return keep_columns\n",
    "\n",
    "\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # 获取阳性样本\n",
    "    print(\"正在获取阳性样本...\")\n",
    "    cur.execute(query_positive)\n",
    "    positive_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    positive_df = pd.DataFrame(cur.fetchall(), columns=positive_columns)\n",
    "    print(f\"阳性样本获取完成，共 {len(positive_df)} 条\")\n",
    "\n",
    "    # 动态更新阴性样本查询\n",
    "    query_negative = query_negative.replace('SELECT COUNT(*) FROM positive_hadm', str(len(positive_df)))\n",
    "\n",
    "    # 获取阴性样本\n",
    "    print(\"正在获取阴性样本...\")\n",
    "    cur.execute(query_negative)\n",
    "    negative_columns = [desc[0] for desc in cur.description]  # 提取列名\n",
    "    negative_df = pd.DataFrame(cur.fetchall(), columns=negative_columns)\n",
    "    print(f\"阴性样本获取完成，共 {len(negative_df)} 条\")\n",
    "\n",
    "    # 合并数据\n",
    "    combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "    # 动态生成keep_columns\n",
    "    keep_columns = get_dynamic_keep_columns(cur.description)\n",
    "    print(f\"动态保留的列：{keep_columns}\")\n",
    "\n",
    "    # 仅保留需要的列\n",
    "    combined_df = combined_df[keep_columns]\n",
    "\n",
    "    # 保存原始数据\n",
    "    raw_output = \"./data/labs_first_day_plus_vitals_first_day_raw.csv\"\n",
    "    combined_df.to_csv(raw_output, index=False)\n",
    "    print(f\"原始数据已保存到 {raw_output}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"数据库操作失败：{str(e)}\")\n",
    "finally:\n",
    "    if con:\n",
    "        cur.close()\n",
    "        con.close()\n",
    "\n",
    "# ================== 数据预处理部分 ==================\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/labs_first_day_plus_vitals_first_day_raw.csv')\n",
    "\n",
    "# 1. 动态保留列（基于查询结果）\n",
    "keep_columns = get_dynamic_keep_columns([(col,) for col in df.columns])  # 模拟cursor.description\n",
    "df = df[keep_columns]\n",
    "\n",
    "# 2. 去除重复记录（基于hadm_id）\n",
    "df = df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
    "\n",
    "# 3. 性别编码\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0}).fillna(-1).astype(int)\n",
    "\n",
    "# 4. 缺失值处理（保留hadm_id）\n",
    "impute_cols = [col for col in df.columns if col not in ['hadm_id', 'match_flag']]  # 动态选择需要填补的列\n",
    "imputer = IterativeImputer(\n",
    "    estimator=LGBMRegressor(n_estimators=50, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "df[impute_cols] = imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "# 5. 标准化处理（排除hadm_id和标签列）\n",
    "features = df.drop(columns=['hadm_id', 'match_flag'])\n",
    "scaler = StandardScaler().fit(features)\n",
    "features_scaled = MinMaxScaler().fit_transform(scaler.transform(features))\n",
    "\n",
    "# 重组最终DataFrame\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final = pd.concat([\n",
    "    df[['hadm_id']].reset_index(drop=True),\n",
    "    df_final,\n",
    "    df['match_flag'].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 保存最终数据\n",
    "final_output = './data/cleaned_labs_first_day_plus_vitals_first_day_raw.csv'\n",
    "df_final.to_csv(final_output, index=False)\n",
    "print(f\"处理后的数据已保存到 {final_output}\")\n",
    "print(\"数据特征分布：\\n\", df_final.describe())\n",
    "\n",
    "# ================== 新增验证部分 ==================\n",
    "# 验证hadm_id保留情况\n",
    "assert 'hadm_id' in df_final.columns, \"hadm_id列丢失！\"\n",
    "print(\"\\n验证结果：\")\n",
    "print(f\"总样本数：{len(df_final)}\")\n",
    "print(f\"阳性样本数：{df_final.match_flag.sum()}\")\n",
    "print(f\"阴性样本数：{len(df_final) - df_final.match_flag.sum()}\")\n",
    "print(f\"唯一hadm_id数量：{df_final.hadm_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2490a5db-d0f4-4ec8-bd09-5a14054e1e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning] *",
   "language": "python",
   "name": "conda-env-.conda-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
