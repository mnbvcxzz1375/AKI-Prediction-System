{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3386cf9-ca06-46ea-8fb2-c94c19427d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 设置可视化风格\n",
    "plt.style.use('tableau-colorblind10')\n",
    "# 设置字体为SimHei(黑体)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# 解决中文字体下坐标轴负数的负号显示问题\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f88db3b-6bc0-46a4-8e15-7cabaaead1d1",
   "metadata": {},
   "source": [
    "### 血气数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3806438e-9419-4397-b0dc-dd48426334b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_25768\\862766434.py:7: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后的数据已保存到 cleaned_blood_gas_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler ,MinMaxScaler\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'blood_gas_sampled_with_flags.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['subject_id', 'hadm_id', 'icd9_code']  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "\n",
    "# 2. 去除缺失值比例超过 50% 的列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 3. 填充剩余的缺失值\n",
    "imputer = SimpleImputer(strategy='median')  # 使用中位数填充\n",
    "df_cleaned[df_cleaned.columns] = imputer.fit_transform(df_cleaned)\n",
    "\n",
    "# 4. 提取数值特征并标准化\n",
    "features = df_cleaned.drop(columns=['match_flag'])  # 假设 'match_flag' 是标签列\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "scaler = MinMaxScaler()  \n",
    "features_scaled = scaler.fit_transform(features_scaled) \n",
    "\n",
    "# 转换为 DataFrame 并添加回标签列\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final['match_flag'] = df_cleaned['match_flag'].values\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_blood_gas_data.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dbc71-381d-477c-8daa-86083beb45bb",
   "metadata": {},
   "source": [
    "### aki患者肌酐水平+第一天实验室数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6874d3e3-10c7-4326-9c90-bdb8fc5f5dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_25768\\2036710154.py:7: DtypeWarning: Columns (46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后的数据已保存到 cleaned_aki_patients_labs_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler ,MinMaxScaler\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'aki_patients_labs_sampled_with_flags.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['subject_id', 'hadm_id', 'icustay_id',  'charttime','icd9_code','long_title',]  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "# 2. 去除缺失值比例超过 50% 的列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 3. 填充剩余的缺失值\n",
    "imputer = SimpleImputer(strategy='median')  # 使用中位数填充\n",
    "df_cleaned[df_cleaned.columns] = imputer.fit_transform(df_cleaned)\n",
    "\n",
    "# 4. 提取数值特征并标准化\n",
    "features = df_cleaned.drop(columns=['match_flag'])  # 假设 'match_flag' 是标签列\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "scaler = MinMaxScaler()  \n",
    "features_scaled = scaler.fit_transform(features_scaled) \n",
    "\n",
    "# 转换为 DataFrame 并添加回标签列\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final['match_flag'] = df_cleaned['match_flag'].values\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_aki_patients_labs_sampled_with_flags.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de732c4-a23c-45ad-9f2f-94c333dfbf8b",
   "metadata": {},
   "source": [
    "### aki患者肌酐水平+微生物实验室数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eed4f890-0eae-43a8-8a09-8ba2ec29ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_25768\\317449091.py:8: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后的数据已保存到 cleaned_aki_microbiology_sampled_with_flags_encoded_unique.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'aki_microbiology_sampled_with_flags.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'charttime',\n",
    "                     'icd9_code', 'long_title', 'chartdate', 'spec_itemid', 'spec_type_desc',\n",
    "                     'org_itemid', 'org_name', 'ab_itemid', 'ab_name', 'dilution_text',\n",
    "                     'dilution_comparison', 'dod', 'admittime', 'dischtime', 'intime',\n",
    "                     'outtime', 'icustay_seq', 'long_title']  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "# 2. 去除重复列\n",
    "df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.duplicated()]\n",
    "\n",
    "# 3. 将性别转换为数值类型\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "# 4. 对非文本类型的分类特征进行编码\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# 5. 去除缺失值比例超过 50% 的列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 6. 填充剩余的缺失值\n",
    "imputer = SimpleImputer(strategy='median')  # 使用中位数填充\n",
    "df_cleaned[df_cleaned.columns] = imputer.fit_transform(df_cleaned)\n",
    "\n",
    "# 7. 提取数值特征并标准化\n",
    "features = df_cleaned.drop(columns=['match_flag'])  # 假设 'match_flag' 是标签列\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 进一步归一化到 [0, 1] 范围\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "# 转换为 DataFrame 并添加回标签列\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final['match_flag'] = df_cleaned['match_flag'].values\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_aki_microbiology_sampled_with_flags_encoded_unique.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ac5c5-5e5e-421c-9bad-c4cfe0882426",
   "metadata": {},
   "source": [
    "### aki肾衰竭患者尿量数据+第一天实验室数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46158126-25a2-4490-b6ff-80fc13060dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_25768\\1969900157.py:8: DtypeWarning: Columns (53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后的数据已保存到 cleaned_aki_urine_labs_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'aki_urine_labs_sampled_with_flags.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'charttime',\n",
    "                     'icd9_code', 'long_title', ]  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "# 2. 去除重复列\n",
    "df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.duplicated()]\n",
    "\n",
    "# 3. 将性别转换为数值类型\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "# 4. 对非文本类型的分类特征进行编码\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# 5. 去除缺失值比例超过 50% 的列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 6. 填充剩余的缺失值\n",
    "imputer = SimpleImputer(strategy='median')  # 使用中位数填充\n",
    "df_cleaned[df_cleaned.columns] = imputer.fit_transform(df_cleaned)\n",
    "\n",
    "# 7. 提取数值特征并标准化\n",
    "features = df_cleaned.drop(columns=['match_flag'])  # 假设 'match_flag' 是标签列\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 进一步归一化到 [0, 1] 范围\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "# 转换为 DataFrame 并添加回标签列\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final['match_flag'] = df_cleaned['match_flag'].values\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_aki_urine_labs_sampled_with_flags.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9ddc5-60bf-4020-9a88-b320ae96d124",
   "metadata": {},
   "source": [
    "### 住院第一天的生命体征与aki肾衰竭患者尿量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6976ad3d-d5fc-4be7-894a-cf48e974099d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_25768\\3589550008.py:8: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后的数据已保存到 cleaned_vitals_aki_urine_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'vitals_aki_urine_sampled_with_flags.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'charttime',\n",
    "                     'icd9_code', 'long_title', ]  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "# 2. 去除重复列\n",
    "df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.duplicated()]\n",
    "\n",
    "# 3. 将性别转换为数值类型\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "# 4. 对非文本类型的分类特征进行编码\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# 5. 去除缺失值比例超过 50% 的列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 6. 填充剩余的缺失值\n",
    "imputer = SimpleImputer(strategy='median')  # 使用中位数填充\n",
    "df_cleaned[df_cleaned.columns] = imputer.fit_transform(df_cleaned)\n",
    "\n",
    "# 7. 提取数值特征并标准化\n",
    "features = df_cleaned.drop(columns=['match_flag'])  # 假设 'match_flag' 是标签列\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 进一步归一化到 [0, 1] 范围\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "# 转换为 DataFrame 并添加回标签列\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final['match_flag'] = df_cleaned['match_flag'].values\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_vitals_aki_urine_sampled_with_flags.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa129f60-7544-48e4-bd1c-c8c029df59e6",
   "metadata": {},
   "source": [
    "### 语言 宗教 婚姻 种族"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a539d8b-1e37-45d5-a2b1-341222a26030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后的数据已保存到 cleaned_language_religion_ethnicity_sampled_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'language_religion_ethnicity_sampled_with_flags.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. 删除 ID 类和文本描述相关的列\n",
    "columns_to_remove = ['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'charttime',\n",
    "                     'icd9_code', 'long_title', ]  # 添加需要移除的列名\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "# 2. 去除重复列\n",
    "df_cleaned = df_cleaned.loc[:, ~df_cleaned.columns.duplicated()]\n",
    "\n",
    "# 3. 将性别转换为数值类型\n",
    "if 'gender' in df_cleaned.columns:\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "# 4. 对非文本类型的分类特征进行编码\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# 5. 去除缺失值比例超过 50% 的列\n",
    "missing_threshold = 0.5\n",
    "df_cleaned = df_cleaned.loc[:, df_cleaned.isnull().mean() <= missing_threshold]\n",
    "\n",
    "# 6. 填充剩余的缺失值\n",
    "imputer = SimpleImputer(strategy='median')  # 使用中位数填充\n",
    "df_cleaned[df_cleaned.columns] = imputer.fit_transform(df_cleaned)\n",
    "\n",
    "# 7. 提取数值特征并标准化\n",
    "features = df_cleaned.drop(columns=['match_flag'])  # 假设 'match_flag' 是标签列\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 进一步归一化到 [0, 1] 范围\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "# 转换为 DataFrame 并添加回标签列\n",
    "df_final = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_final['match_flag'] = df_cleaned['match_flag'].values\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_language_religion_ethnicity_sampled_with_flags.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"清洗后的数据已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9778121b-3e10-4272-ad44-2282315c3c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spo2</th>\n",
       "      <th>po2</th>\n",
       "      <th>pco2</th>\n",
       "      <th>pao2fio2</th>\n",
       "      <th>ph</th>\n",
       "      <th>totalco2</th>\n",
       "      <th>match_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.098208</td>\n",
       "      <td>0.650350</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.134771</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.107512</td>\n",
       "      <td>0.657343</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.115903</td>\n",
       "      <td>0.102679</td>\n",
       "      <td>0.093039</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.185984</td>\n",
       "      <td>0.084821</td>\n",
       "      <td>0.116196</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.373315</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>0.112474</td>\n",
       "      <td>0.559441</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spo2       po2      pco2  pao2fio2        ph  totalco2  match_flag\n",
       "0  0.979798  0.122642  0.089286  0.098208  0.650350  0.192771         1.0\n",
       "1  0.979798  0.134771  0.093750  0.107512  0.657343  0.204819         1.0\n",
       "2  0.979798  0.115903  0.102679  0.093039  0.636364  0.204819         1.0\n",
       "3  0.979798  0.185984  0.084821  0.116196  0.671329  0.192771         1.0\n",
       "4  0.979798  0.373315  0.116071  0.112474  0.559441  0.168675         1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'cleaned_blood_gas_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18829eeb-8555-41f4-9afe-e5600fcfbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  \n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "X=df.iloc[:,:-1]\n",
    "Y=df['match_flag']\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40c4f3-1630-49c8-a5ef-68040eca7b6d",
   "metadata": {},
   "source": [
    "## cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff569b61-1ab1-4718-a2fd-a8be3127aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------第1轮训练-------\n",
      "训练损失: 28101.7449\n",
      "测试损失: 11623.4030, 准确率: 0.7496\n",
      "------第2轮训练-------\n",
      "训练损失: 27771.1330\n",
      "测试损失: 11600.9753, 准确率: 0.7496\n",
      "------第3轮训练-------\n",
      "训练损失: 27636.6002\n",
      "测试损失: 11549.7414, 准确率: 0.7495\n",
      "------第4轮训练-------\n",
      "训练损失: 27540.9744\n",
      "测试损失: 11455.4247, 准确率: 0.7496\n",
      "------第5轮训练-------\n",
      "训练损失: 27480.3246\n",
      "测试损失: 11481.5648, 准确率: 0.7495\n",
      "------第6轮训练-------\n",
      "训练损失: 27318.6247\n",
      "测试损失: 11466.1227, 准确率: 0.7494\n",
      "------第7轮训练-------\n",
      "训练损失: 27113.3984\n",
      "测试损失: 11498.3413, 准确率: 0.7495\n",
      "------第8轮训练-------\n",
      "训练损失: 26995.6864\n",
      "测试损失: 11405.2650, 准确率: 0.7495\n",
      "------第9轮训练-------\n",
      "训练损失: 26948.7613\n",
      "测试损失: 11391.7469, 准确率: 0.7495\n",
      "------第10轮训练-------\n",
      "训练损失: 26913.7181\n",
      "测试损失: 11533.4576, 准确率: 0.7495\n",
      "模型已保存到 cnn_model1.pth\n",
      "总训练时间: 2186.01 秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 检测设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 读取数据\n",
    "file_path = './data/cleaned_kdigo_uo.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "X = df.iloc[:, :-1].values  # 特征\n",
    "Y = df['match_flag'].values  # 标签\n",
    "\n",
    "# 数据分割\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # 添加通道维度\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)  # 转换为浮点型\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)  # 转换为浮点型\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# 定义 CNN 模型\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=2, padding=3),\n",
    "            nn.BatchNorm1d(16),  # 增加 Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(1),\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=5),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(1),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(1),\n",
    "            # nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.model2 = None  # Placeholder\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.model1(input)\n",
    "\n",
    "        # 动态初始化 model2\n",
    "        if self.model2 is None:\n",
    "            self.model2 = nn.Sequential(\n",
    "                nn.Linear(in_features=x.shape[1], out_features=128, bias=True),\n",
    "                nn.BatchNorm1d(128),  # 增加 Batch Normalization\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.4),  # 增加 Dropout\n",
    "                nn.Linear(in_features=128, out_features=1, bias=True),\n",
    "            )\n",
    "            self.model2 = self.model2.to(input.device)  # 确保在正确的设备上\n",
    "\n",
    "        x = self.model2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 实例化模型并移动到设备\n",
    "cnn = CNN().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# 超参数设置\n",
    "epoch = 10\n",
    "\n",
    "# TensorBoard 日志记录\n",
    "writer = SummaryWriter('loss_train')\n",
    "start_time = time.time()\n",
    "\n",
    "# 开始训练\n",
    "for i in range(epoch):\n",
    "    print(f'------第{i + 1}轮训练-------')\n",
    "\n",
    "    cnn.train()\n",
    "    total_train_loss = 0\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = cnn(batch_data).squeeze(1)  # 去掉通道维度\n",
    "        loss = loss_fn(outputs, batch_labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    print(f'训练损失: {total_train_loss:.4f}')\n",
    "    writer.add_scalar('Train Loss', total_train_loss, i)\n",
    "\n",
    "    # 测试阶段\n",
    "    cnn.eval()\n",
    "    total_test_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "            outputs = cnn(batch_data).squeeze(1)  # 去掉通道维度\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            predictions = torch.sigmoid(outputs) > 0.5  # 转为概率并判断大于0.5\n",
    "            total_correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(test_dataset)\n",
    "    print(f'测试损失: {total_test_loss:.4f}, 准确率: {accuracy:.4f}')\n",
    "    writer.add_scalar('Test Loss', total_test_loss, i)\n",
    "    writer.add_scalar('Accuracy', accuracy, i)\n",
    "\n",
    "# 保存模型\n",
    "model_path = 'cnn_model1.pth'\n",
    "torch.save(cnn.state_dict(), model_path)\n",
    "print(f'模型已保存到 {model_path}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'总训练时间: {end_time - start_time:.2f} 秒')\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc18042-501d-4f41-a403-748a0f8366bd",
   "metadata": {},
   "source": [
    "## TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb2a05f-fb1b-4d62-a4db-405f6e594267",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 64 and 18 (The offending index is 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 151\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 144\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# 训练和评估\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 144\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     val_loss, accuracy, precision, recall, f1, auc \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader1, test_loader2, criterion, device)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader1, train_loader2, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     74\u001b[0m X1_batch, X2_batch, y1_batch \u001b[38;5;241m=\u001b[39m X1_batch\u001b[38;5;241m.\u001b[39mto(device), X2_batch\u001b[38;5;241m.\u001b[39mto(device), y1_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y1_batch)\n\u001b[0;32m     78\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m, in \u001b[0;36mDualTabTransformer.forward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     63\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer1(x1)\n\u001b[0;32m     64\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer2(x2)\n\u001b[1;32m---> 65\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 拼接两个特征\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(combined)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 64 and 18 (The offending index is 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df, target_column):\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "    return X, y\n",
    "\n",
    "# 加载数据\n",
    "def load_data(file_path, target_column, batch_size=64):\n",
    "    df = pd.read_csv(file_path)\n",
    "    X, y = preprocess_data(df, target_column)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# TabTransformer 模型\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, embed_dim)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        x = x.transpose(0, 1)  # (sequence_length=1, batch_size, embed_dim)\n",
    "        x = self.transformer(x)  # (sequence_length=1, batch_size, embed_dim)\n",
    "        x = x.squeeze(0)  # (batch_size, embed_dim)\n",
    "        return x\n",
    "\n",
    "# 联合 TabTransformer 模型\n",
    "class DualTabTransformer(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(DualTabTransformer, self).__init__()\n",
    "        self.transformer1 = TabTransformer(input_dim1, embed_dim, num_heads, num_layers, dropout)\n",
    "        self.transformer2 = TabTransformer(input_dim2, embed_dim, num_heads, num_layers, dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 128),  # 拼接两个特征\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)  # 输出单个值\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.transformer1(x1)\n",
    "        x2 = self.transformer2(x2)\n",
    "        combined = torch.cat((x1, x2), dim=1)  # 拼接两个特征\n",
    "        return self.fc(combined).squeeze(1)  # 输出 (batch_size, )\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader1, train_loader2, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (X1_batch, y1_batch), (X2_batch, _) in zip(train_loader1, train_loader2):\n",
    "        X1_batch, X2_batch, y1_batch = X1_batch.to(device), X2_batch.to(device), y1_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X1_batch, X2_batch)\n",
    "        loss = criterion(outputs, y1_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader1)\n",
    "\n",
    "# 评估模型\n",
    "def evaluate_model(model, test_loader1, test_loader2, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (X1_batch, y1_batch), (X2_batch, _) in zip(test_loader1, test_loader2):\n",
    "            X1_batch, X2_batch, y1_batch = X1_batch.to(device), X2_batch.to(device), y1_batch.to(device)\n",
    "            outputs = model(X1_batch, X2_batch)\n",
    "            loss = criterion(outputs, y1_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            y_true.extend(y1_batch.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_pred_proba.extend(probs.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    return total_loss / len(test_loader1), accuracy, precision, recall, f1, auc\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    file_path1 = './data/cleaned_urine_output_first_day.csv'\n",
    "    file_path2 = './data/cleaned_vitals_first_day.csv'\n",
    "    target_column = 'match_flag'\n",
    "\n",
    "    # 加载两个文件的数据\n",
    "    train_loader1, test_loader1 = load_data(file_path1, target_column)\n",
    "    train_loader2, test_loader2 = load_data(file_path2, target_column)\n",
    "\n",
    "    # 模型参数\n",
    "    input_dim1 = next(iter(train_loader1))[0].shape[1]\n",
    "    input_dim2 = next(iter(train_loader2))[0].shape[1]\n",
    "    embed_dim = 64\n",
    "    num_heads = 4\n",
    "    num_layers = 2\n",
    "    dropout = 0.1\n",
    "    epochs = 30\n",
    "\n",
    "    # 模型初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DualTabTransformer(input_dim1, input_dim2, embed_dim, num_heads, num_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # 替换为 BCEWithLogitsLoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 训练和评估\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_loader1, train_loader2, criterion, optimizer, device)\n",
    "        val_loss, accuracy, precision, recall, f1, auc = evaluate_model(model, test_loader1, test_loader2, criterion, device)\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85622e87-1018-474b-aa79-2cc29eab85d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'urine_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 155\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# 主程序\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# 加载数据\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     urine_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murine_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     vitals_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvitals_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m     target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'urine_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 自动特征列生成器\n",
    "class FeatureGenerator:\n",
    "    @staticmethod\n",
    "    def auto_detect(df, \n",
    "                   target_column, \n",
    "                   exclude_patterns=[r'_id$', r'_date$', r'timestamp'],\n",
    "                   explicit_exclude=[]):\n",
    "        \"\"\"\n",
    "        智能特征列检测\n",
    "        参数：\n",
    "            df: 数据框\n",
    "            target_column: 目标列名\n",
    "            exclude_patterns: 要排除的列名正则模式\n",
    "            explicit_exclude: 显式排除的列名列表\n",
    "        \"\"\"\n",
    "        # 初始化排除列\n",
    "        exclude_columns = set(explicit_exclude)\n",
    "        \n",
    "        # 正则匹配排除\n",
    "        pattern = re.compile('|'.join(exclude_patterns), re.IGNORECASE)\n",
    "        for col in df.columns:\n",
    "            if pattern.search(col):\n",
    "                exclude_columns.add(col)\n",
    "                \n",
    "        # 确保排除目标列\n",
    "        exclude_columns.add(target_column)\n",
    "        \n",
    "        # 生成特征列\n",
    "        features = [col for col in df.columns if col not in exclude_columns]\n",
    "        \n",
    "        # 验证特征有效性\n",
    "        valid_dtypes = ['int64', 'float64', 'bool']\n",
    "        features = [col for col in features if df[col].dtype in valid_dtypes]\n",
    "        \n",
    "        return features \n",
    "\n",
    "# --------------------------\n",
    "# 核心模型架构\n",
    "# --------------------------\n",
    "\n",
    "class FeatureSpecificModel(nn.Module):\n",
    "    \"\"\"特征专用分类模型\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# --------------------------\n",
    "# 训练流水线\n",
    "# --------------------------\n",
    "\n",
    "def train_model(data_loader, epochs=30):\n",
    "    \"\"\"独立训练流程\"\"\"\n",
    "    model = FeatureSpecificModel(\n",
    "        input_dim=data_loader.dataset[0][0].shape[0]\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(data_loader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --------------------------\n",
    "# 智能预测系统\n",
    "# --------------------------\n",
    "\n",
    "class HybridPredictor:\n",
    "    \"\"\"混合预测系统\"\"\"\n",
    "    def __init__(self, modelA, modelB):\n",
    "        self.modelA = modelA.to(device).eval()\n",
    "        self.modelB = modelB.to(device).eval()\n",
    "        \n",
    "    def predict(self, X_dict):\n",
    "        \"\"\"\n",
    "        输入格式：{'feature_set1': X1_tensor, 'feature_set2': X2_tensor}\n",
    "        至少包含一个特征集\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            results = {}\n",
    "            \n",
    "            # 处理第一个特征集\n",
    "            if 'feature_set1' in X_dict:\n",
    "                X1 = X_dict['feature_set1'].to(device)\n",
    "                results['prob1'] = torch.sigmoid(self.modelA(X1))\n",
    "                \n",
    "            # 处理第二个特征集\n",
    "            if 'feature_set2' in X_dict:\n",
    "                X2 = X_dict['feature_set2'].to(device)\n",
    "                results['prob2'] = torch.sigmoid(self.modelB(X2))\n",
    "                \n",
    "            # 动态融合策略\n",
    "            if len(results) == 2:\n",
    "                final_prob = 0.7*results['prob1'] + 0.3*results['prob2']\n",
    "            elif 'prob1' in results:\n",
    "                final_prob = results['prob1']\n",
    "            else:\n",
    "                final_prob = results['prob2']\n",
    "            \n",
    "            return (final_prob > 0.5).float().cpu().numpy()\n",
    "\n",
    "# --------------------------\n",
    "# 数据预处理\n",
    "# --------------------------\n",
    "\n",
    "def prepare_dataset(df, feature_columns, target_column):\n",
    "    \"\"\"创建特征特定的数据集\"\"\"\n",
    "    X = df[feature_columns].values.astype(np.float32)\n",
    "    y = df[target_column].values.astype(np.float32)\n",
    "    dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    return DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# --------------------------\n",
    "# 使用示例\n",
    "# --------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设两个数据文件结构\n",
    "    # urine_data.csv: urine_feature1, urine_feature2..., match_flag\n",
    "    # vitals_data.csv: vital_feature1, vital_feature2..., match_flag\n",
    "    \n",
    "    # 加载数据（示例路径）\n",
    "    urine_df = pd.read_csv(\"./data/cleaned_urine_output.csv\")\n",
    "    vitals_df = pd.read_csv(\"./data/cleaned_vitals_first_day.csv\")\n",
    "    \n",
    "    # 定义特征列（根据实际数据修改）\n",
    "# 自动生成特征列\n",
    "    urine_features = FeatureGenerator.auto_detect(\n",
    "        urine_df,\n",
    "        target_column='match_flag',\n",
    "    )\n",
    "    \n",
    "    vitals_features = FeatureGenerator.auto_detect(\n",
    "        vitals_df,\n",
    "        target_column='match_flag',\n",
    "    )\n",
    "    \n",
    "    # 准备训练数据\n",
    "    print(\"训练尿液特征模型...\")\n",
    "    urine_loader = prepare_dataset(urine_df, urine_features, target_col)\n",
    "    modelA = train_model(urine_loader)\n",
    "    \n",
    "    print(\"\\n训练生命体征模型...\")\n",
    "    vitals_loader = prepare_dataset(vitals_df, vitals_features, target_col)\n",
    "    modelB = train_model(vitals_loader)\n",
    "    \n",
    "    # 初始化预测系统\n",
    "    predictor = HybridPredictor(modelA, modelB)\n",
    "    \n",
    "    # 模拟测试数据（三种情况）\n",
    "    test_case1 = {  # 只有尿液特征\n",
    "        'feature_set1': torch.randn(5, len(urine_features))  \n",
    "    }\n",
    "    test_case2 = {  # 只有生命体征\n",
    "        'feature_set2': torch.randn(3, len(vitals_features))\n",
    "    }\n",
    "    test_case3 = {  # 两者都有\n",
    "        'feature_set1': torch.randn(2, len(urine_features)),\n",
    "        'feature_set2': torch.randn(2, len(vitals_features))\n",
    "    }\n",
    "    \n",
    "    # 执行预测\n",
    "    print(\"\\n测试案例1预测结果:\", predictor.predict(test_case1))\n",
    "    print(\"测试案例2预测结果:\", predictor.predict(test_case2))\n",
    "    print(\"测试案例3预测结果:\", predictor.predict(test_case3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c991ab64-0677-476f-8a5e-e5f5d8e64a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 0.6860, Val Loss: 0.6902\n",
      "Accuracy: 0.5424, Precision: 0.5235, Recall: 0.9432, F1: 0.6733, AUC: 0.6304\n",
      "Epoch 2:\n",
      "Train Loss: 0.6736, Val Loss: 0.6630\n",
      "Accuracy: 0.6039, Precision: 0.6383, Recall: 0.4797, F1: 0.5477, AUC: 0.6512\n",
      "Epoch 3:\n",
      "Train Loss: 0.6626, Val Loss: 0.6506\n",
      "Accuracy: 0.6272, Precision: 0.6267, Recall: 0.6292, F1: 0.6279, AUC: 0.6675\n",
      "Epoch 4:\n",
      "Train Loss: 0.6572, Val Loss: 0.6689\n",
      "Accuracy: 0.5934, Precision: 0.5584, Recall: 0.8932, F1: 0.6872, AUC: 0.6731\n",
      "Epoch 5:\n",
      "Train Loss: 0.6535, Val Loss: 0.6450\n",
      "Accuracy: 0.6310, Precision: 0.6425, Recall: 0.5905, F1: 0.6154, AUC: 0.6804\n",
      "Epoch 6:\n",
      "Train Loss: 0.6499, Val Loss: 0.6469\n",
      "Accuracy: 0.6281, Precision: 0.6647, Recall: 0.5168, F1: 0.5815, AUC: 0.6867\n",
      "Epoch 7:\n",
      "Train Loss: 0.6452, Val Loss: 0.6542\n",
      "Accuracy: 0.6193, Precision: 0.6090, Recall: 0.6665, F1: 0.6365, AUC: 0.6660\n",
      "Epoch 8:\n",
      "Train Loss: 0.6461, Val Loss: 0.6422\n",
      "Accuracy: 0.6312, Precision: 0.6419, Recall: 0.5933, F1: 0.6167, AUC: 0.6819\n",
      "Epoch 9:\n",
      "Train Loss: 0.6372, Val Loss: 0.6488\n",
      "Accuracy: 0.6179, Precision: 0.6928, Recall: 0.4237, F1: 0.5258, AUC: 0.6888\n",
      "Epoch 10:\n",
      "Train Loss: 0.6378, Val Loss: 0.6333\n",
      "Accuracy: 0.6417, Precision: 0.6160, Recall: 0.7522, F1: 0.6773, AUC: 0.7010\n",
      "Epoch 11:\n",
      "Train Loss: 0.6363, Val Loss: 0.6336\n",
      "Accuracy: 0.6420, Precision: 0.6331, Recall: 0.6755, F1: 0.6536, AUC: 0.6937\n",
      "Epoch 12:\n",
      "Train Loss: 0.6325, Val Loss: 0.6373\n",
      "Accuracy: 0.6312, Precision: 0.6874, Recall: 0.4812, F1: 0.5661, AUC: 0.7040\n",
      "Epoch 13:\n",
      "Train Loss: 0.6290, Val Loss: 0.6344\n",
      "Accuracy: 0.6425, Precision: 0.6265, Recall: 0.7057, F1: 0.6637, AUC: 0.6973\n",
      "Epoch 14:\n",
      "Train Loss: 0.6233, Val Loss: 0.6293\n",
      "Accuracy: 0.6472, Precision: 0.6597, Recall: 0.6078, F1: 0.6327, AUC: 0.7077\n",
      "Epoch 15:\n",
      "Train Loss: 0.6170, Val Loss: 0.6297\n",
      "Accuracy: 0.6380, Precision: 0.6133, Recall: 0.7472, F1: 0.6736, AUC: 0.7004\n",
      "Epoch 16:\n",
      "Train Loss: 0.6156, Val Loss: 0.6173\n",
      "Accuracy: 0.6507, Precision: 0.6529, Recall: 0.6433, F1: 0.6481, AUC: 0.7147\n",
      "Epoch 17:\n",
      "Train Loss: 0.6101, Val Loss: 0.6104\n",
      "Accuracy: 0.6501, Precision: 0.6851, Recall: 0.5555, F1: 0.6135, AUC: 0.7226\n",
      "Epoch 18:\n",
      "Train Loss: 0.6086, Val Loss: 0.6037\n",
      "Accuracy: 0.6594, Precision: 0.6675, Recall: 0.6352, F1: 0.6510, AUC: 0.7283\n",
      "Epoch 19:\n",
      "Train Loss: 0.6004, Val Loss: 0.6024\n",
      "Accuracy: 0.6625, Precision: 0.6504, Recall: 0.7028, F1: 0.6756, AUC: 0.7354\n",
      "Epoch 20:\n",
      "Train Loss: 0.5940, Val Loss: 0.6048\n",
      "Accuracy: 0.6589, Precision: 0.6728, Recall: 0.6188, F1: 0.6447, AUC: 0.7286\n",
      "Epoch 21:\n",
      "Train Loss: 0.5864, Val Loss: 0.5888\n",
      "Accuracy: 0.6643, Precision: 0.6810, Recall: 0.6180, F1: 0.6480, AUC: 0.7426\n",
      "Epoch 22:\n",
      "Train Loss: 0.5825, Val Loss: 0.5880\n",
      "Accuracy: 0.6653, Precision: 0.6804, Recall: 0.6232, F1: 0.6505, AUC: 0.7434\n",
      "Epoch 23:\n",
      "Train Loss: 0.5762, Val Loss: 0.5875\n",
      "Accuracy: 0.6608, Precision: 0.6427, Recall: 0.7243, F1: 0.6811, AUC: 0.7416\n",
      "Epoch 24:\n",
      "Train Loss: 0.5681, Val Loss: 0.5811\n",
      "Accuracy: 0.6711, Precision: 0.6960, Recall: 0.6075, F1: 0.6487, AUC: 0.7547\n",
      "Epoch 25:\n",
      "Train Loss: 0.5650, Val Loss: 0.5780\n",
      "Accuracy: 0.6722, Precision: 0.6932, Recall: 0.6177, F1: 0.6533, AUC: 0.7559\n",
      "Epoch 26:\n",
      "Train Loss: 0.5591, Val Loss: 0.5905\n",
      "Accuracy: 0.6636, Precision: 0.6905, Recall: 0.5928, F1: 0.6380, AUC: 0.7478\n",
      "Epoch 27:\n",
      "Train Loss: 0.5502, Val Loss: 0.5756\n",
      "Accuracy: 0.6760, Precision: 0.6596, Recall: 0.7273, F1: 0.6918, AUC: 0.7599\n",
      "Epoch 28:\n",
      "Train Loss: 0.5438, Val Loss: 0.5624\n",
      "Accuracy: 0.6841, Precision: 0.6823, Recall: 0.6890, F1: 0.6856, AUC: 0.7699\n",
      "Epoch 29:\n",
      "Train Loss: 0.5366, Val Loss: 0.5739\n",
      "Accuracy: 0.6754, Precision: 0.6500, Recall: 0.7600, F1: 0.7007, AUC: 0.7618\n",
      "Epoch 30:\n",
      "Train Loss: 0.5330, Val Loss: 0.6091\n",
      "Accuracy: 0.6795, Precision: 0.6651, Recall: 0.7232, F1: 0.6929, AUC: 0.7581\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df, target_column):\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "    return X, y\n",
    "\n",
    "# 加载数据\n",
    "def load_data(file_path, target_column, batch_size=64):\n",
    "    df = pd.read_csv(file_path)\n",
    "    X, y = preprocess_data(df, target_column)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# FT-Transformer 模型\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(FTTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norms = nn.ModuleList([nn.LayerNorm(embed_dim) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, embed_dim)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        x = x.transpose(0, 1)  # (sequence_length=1, batch_size, embed_dim)\n",
    "        \n",
    "        for layer, layer_norm in zip(self.transformer_layers, self.layer_norms):\n",
    "            residual = x\n",
    "            x = layer(x)  # Transformer layer\n",
    "            x = self.dropout(x)\n",
    "            x = layer_norm(x + residual)  # 残差连接 + Layer Normalization\n",
    "        \n",
    "        x = x.squeeze(0)  # (batch_size, embed_dim)\n",
    "        return x  # 返回 (batch_size, embed_dim)\n",
    "\n",
    "# 联合 FT-Transformer 模型\n",
    "class DualFTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(DualFTTransformer, self).__init__()\n",
    "        self.transformer1 = FTTransformer(input_dim1, embed_dim, num_heads, num_layers, dropout)\n",
    "        self.transformer2 = FTTransformer(input_dim2, embed_dim, num_heads, num_layers, dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 128),  # 拼接两个特征\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)  # 输出单个值\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.transformer1(x1)  # (batch_size, embed_dim)\n",
    "        x2 = self.transformer2(x2)  # (batch_size, embed_dim)\n",
    "        combined = torch.cat((x1, x2), dim=1)  # 拼接两个特征 (batch_size, embed_dim * 2)\n",
    "        return self.fc(combined).squeeze(1)  # 输出 (batch_size, )\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader1, train_loader2, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (X1_batch, y1_batch), (X2_batch, _) in zip(train_loader1, train_loader2):\n",
    "        X1_batch, X2_batch, y1_batch = X1_batch.to(device), X2_batch.to(device), y1_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X1_batch, X2_batch)\n",
    "        loss = criterion(outputs, y1_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader1)\n",
    "\n",
    "# 评估模型\n",
    "def evaluate_model(model, test_loader1, test_loader2, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (X1_batch, y1_batch), (X2_batch, _) in zip(test_loader1, test_loader2):\n",
    "            X1_batch, X2_batch, y1_batch = X1_batch.to(device), X2_batch.to(device), y1_batch.to(device)\n",
    "            outputs = model(X1_batch, X2_batch)\n",
    "            loss = criterion(outputs, y1_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            y_true.extend(y1_batch.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_pred_proba.extend(probs.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    return total_loss / len(test_loader1), accuracy, precision, recall, f1, auc\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    file_path1 = 'cleaned_aki_data1.csv'\n",
    "    file_path2 = 'cleaned_aki_data2.csv'\n",
    "    target_column = 'match_flag'\n",
    "\n",
    "    # 加载两个文件的数据\n",
    "    train_loader1, test_loader1 = load_data(file_path1, target_column)\n",
    "    train_loader2, test_loader2 = load_data(file_path2, target_column)\n",
    "\n",
    "    # 模型参数\n",
    "    input_dim1 = next(iter(train_loader1))[0].shape[1]\n",
    "    input_dim2 = next(iter(train_loader2))[0].shape[1]\n",
    "    embed_dim = 32  # 增加宽度\n",
    "    num_heads = 8    # 增加多头注意力机制\n",
    "    num_layers = 2   # 增加深度\n",
    "    dropout = 0.1\n",
    "    epochs = 30\n",
    "\n",
    "    # 模型初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DualFTTransformer(input_dim1, input_dim2, embed_dim, num_heads, num_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)  # 学习率调度器\n",
    "\n",
    "    # 训练和评估\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_loader1, train_loader2, criterion, optimizer, device)\n",
    "        val_loss, accuracy, precision, recall, f1, auc = evaluate_model(model, test_loader1, test_loader2, criterion, device)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc73d8b6-b187-4e5a-b3b4-53b62131a8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 0.6934, Val Loss: 0.6931\n",
      "Accuracy: 0.5000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, AUC: 0.4988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 159\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 159\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 148\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# 训练和评估\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 148\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     val_loss, accuracy, precision, recall, f1, auc \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, criterion, device)\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# 更新学习率\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 84\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     82\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     83\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 84\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df, target_column):\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "    return X, y\n",
    "\n",
    "# 加载数据\n",
    "def load_data(file_path, target_column, batch_size=64):\n",
    "    df = pd.read_csv(file_path)\n",
    "    X, y = preprocess_data(df, target_column)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # 将标签转换为 torch.long 类型\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# SwinLSTM 模型\n",
    "class SwinLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, hidden_dim, dropout=0.1):\n",
    "        super(SwinLSTM, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Swin Transformer 模块\n",
    "        self.swin_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # LSTM 模块\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 2)  # 输出维度改为 2（二分类问题）\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 嵌入层\n",
    "        x = self.embedding(x)  # (batch_size, embed_dim)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Swin Transformer\n",
    "        x = x.transpose(0, 1)  # (sequence_length=1, batch_size, embed_dim)\n",
    "        x = self.swin_transformer(x)  # (sequence_length=1, batch_size, embed_dim)\n",
    "        x = x.transpose(0, 1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # (batch_size, 1, hidden_dim)\n",
    "        lstm_out = lstm_out.squeeze(1)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # 全连接层\n",
    "        return self.fc(lstm_out)  # 输出 (batch_size, 2)\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 评估模型\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 获取预测类别和概率\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]  # 取正类的概率\n",
    "            preds = torch.argmax(outputs, dim=1)  # 取预测类别\n",
    "\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_pred_proba.extend(probs.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba) if len(set(y_true)) > 1 else 0.0\n",
    "\n",
    "    return total_loss / len(test_loader), accuracy, precision, recall, f1, auc\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    file_path = 'cleaned_aki_data4.csv'  # 仅使用 data1.csv\n",
    "    target_column = 'match_flag'\n",
    "\n",
    "    # 加载数据\n",
    "    train_loader, test_loader = load_data(file_path, target_column)\n",
    "\n",
    "    # 模型参数\n",
    "    input_dim = next(iter(train_loader))[0].shape[1]\n",
    "    embed_dim = 128\n",
    "    num_heads = 8\n",
    "    num_layers = 8\n",
    "    hidden_dim = 128\n",
    "    dropout = 0.3\n",
    "    epochs = 15\n",
    "\n",
    "    # 模型初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SwinLSTM(input_dim, embed_dim, num_heads, num_layers, hidden_dim, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # 使用交叉熵损失\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)  # 学习率调度器\n",
    "\n",
    "    # 训练和评估\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, accuracy, precision, recall, f1, auc = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52942582-08d6-424d-8ff9-f709162bd272",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------第1轮训练-------\n",
      "训练损失: 282.0862\n",
      "测试损失: 112.6526, 准确率: 0.6692\n",
      "------第2轮训练-------\n",
      "训练损失: 259.1579\n",
      "测试损失: 110.2535, 准确率: 0.6773\n",
      "------第3轮训练-------\n",
      "训练损失: 254.4266\n",
      "测试损失: 107.8549, 准确率: 0.6902\n",
      "------第4轮训练-------\n",
      "训练损失: 251.1169\n",
      "测试损失: 106.6084, 准确率: 0.6943\n",
      "------第5轮训练-------\n",
      "训练损失: 248.6360\n",
      "测试损失: 106.8097, 准确率: 0.6919\n",
      "------第6轮训练-------\n",
      "训练损失: 247.1097\n",
      "测试损失: 105.7158, 准确率: 0.6999\n",
      "------第7轮训练-------\n",
      "训练损失: 246.0206\n",
      "测试损失: 107.0438, 准确率: 0.6905\n",
      "------第8轮训练-------\n",
      "训练损失: 244.9165\n",
      "测试损失: 105.7287, 准确率: 0.7000\n",
      "------第9轮训练-------\n",
      "训练损失: 243.4746\n",
      "测试损失: 104.7938, 准确率: 0.7015\n",
      "------第10轮训练-------\n",
      "训练损失: 242.3866\n",
      "测试损失: 104.8042, 准确率: 0.7027\n",
      "------第11轮训练-------\n",
      "训练损失: 241.2682\n",
      "测试损失: 104.4832, 准确率: 0.7013\n",
      "------第12轮训练-------\n",
      "训练损失: 239.9584\n",
      "测试损失: 103.7318, 准确率: 0.7040\n",
      "------第13轮训练-------\n",
      "训练损失: 239.0094\n",
      "测试损失: 103.2659, 准确率: 0.7055\n",
      "------第14轮训练-------\n",
      "训练损失: 238.6424\n",
      "测试损失: 104.6873, 准确率: 0.6999\n",
      "------第15轮训练-------\n",
      "训练损失: 237.4868\n",
      "测试损失: 103.2907, 准确率: 0.7027\n",
      "------第16轮训练-------\n",
      "训练损失: 236.5372\n",
      "测试损失: 103.1025, 准确率: 0.7004\n",
      "------第17轮训练-------\n",
      "训练损失: 235.6993\n",
      "测试损失: 102.8714, 准确率: 0.7057\n",
      "------第18轮训练-------\n",
      "训练损失: 234.7587\n",
      "测试损失: 104.3871, 准确率: 0.7023\n",
      "------第19轮训练-------\n",
      "训练损失: 233.1635\n",
      "测试损失: 102.5834, 准确率: 0.7034\n",
      "------第20轮训练-------\n",
      "训练损失: 232.1823\n",
      "测试损失: 101.8015, 准确率: 0.7037\n",
      "------第21轮训练-------\n",
      "训练损失: 230.7881\n",
      "测试损失: 101.8226, 准确率: 0.7048\n",
      "------第22轮训练-------\n",
      "训练损失: 229.6167\n",
      "测试损失: 101.4417, 准确率: 0.7055\n",
      "------第23轮训练-------\n",
      "训练损失: 229.1551\n",
      "测试损失: 101.4696, 准确率: 0.7017\n",
      "------第24轮训练-------\n",
      "训练损失: 227.7004\n",
      "测试损失: 101.4326, 准确率: 0.7053\n",
      "------第25轮训练-------\n",
      "训练损失: 226.7526\n",
      "测试损失: 100.2618, 准确率: 0.7103\n",
      "------第26轮训练-------\n",
      "训练损失: 223.8944\n",
      "测试损失: 101.6833, 准确率: 0.7044\n",
      "------第27轮训练-------\n",
      "训练损失: 223.4690\n",
      "测试损失: 99.9993, 准确率: 0.7080\n",
      "------第28轮训练-------\n",
      "训练损失: 222.6399\n",
      "测试损失: 100.3685, 准确率: 0.7127\n",
      "------第29轮训练-------\n",
      "训练损失: 221.8088\n",
      "测试损失: 99.2249, 准确率: 0.7145\n",
      "------第30轮训练-------\n",
      "训练损失: 220.6473\n",
      "测试损失: 101.8064, 准确率: 0.7119\n",
      "------第31轮训练-------\n",
      "训练损失: 219.7808\n",
      "测试损失: 99.2062, 准确率: 0.7143\n",
      "------第32轮训练-------\n",
      "训练损失: 217.7647\n",
      "测试损失: 99.8236, 准确率: 0.7093\n",
      "------第33轮训练-------\n",
      "训练损失: 217.3182\n",
      "测试损失: 98.4165, 准确率: 0.7159\n",
      "------第34轮训练-------\n",
      "训练损失: 216.0496\n",
      "测试损失: 98.2663, 准确率: 0.7094\n",
      "------第35轮训练-------\n",
      "训练损失: 214.2833\n",
      "测试损失: 98.9693, 准确率: 0.7108\n",
      "------第36轮训练-------\n",
      "训练损失: 213.3054\n",
      "测试损失: 96.9510, 准确率: 0.7192\n",
      "------第37轮训练-------\n",
      "训练损失: 212.1808\n",
      "测试损失: 97.5745, 准确率: 0.7206\n",
      "------第38轮训练-------\n",
      "训练损失: 210.8164\n",
      "测试损失: 97.1193, 准确率: 0.7231\n",
      "------第39轮训练-------\n",
      "训练损失: 209.6611\n",
      "测试损失: 97.9396, 准确率: 0.7182\n",
      "------第40轮训练-------\n",
      "训练损失: 208.2939\n",
      "测试损失: 96.7284, 准确率: 0.7179\n",
      "------第41轮训练-------\n",
      "训练损失: 207.5937\n",
      "测试损失: 95.7728, 准确率: 0.7238\n",
      "------第42轮训练-------\n",
      "训练损失: 205.9881\n",
      "测试损失: 96.8730, 准确率: 0.7212\n",
      "------第43轮训练-------\n",
      "训练损失: 205.4406\n",
      "测试损失: 96.1607, 准确率: 0.7206\n",
      "------第44轮训练-------\n",
      "训练损失: 204.5290\n",
      "测试损失: 95.8567, 准确率: 0.7243\n",
      "------第45轮训练-------\n",
      "训练损失: 203.5897\n",
      "测试损失: 94.3036, 准确率: 0.7238\n",
      "------第46轮训练-------\n",
      "训练损失: 201.7627\n",
      "测试损失: 96.9823, 准确率: 0.7210\n",
      "------第47轮训练-------\n",
      "训练损失: 200.9711\n",
      "测试损失: 96.8364, 准确率: 0.7184\n",
      "------第48轮训练-------\n",
      "训练损失: 200.9152\n",
      "测试损失: 96.7041, 准确率: 0.7151\n",
      "------第49轮训练-------\n",
      "训练损失: 198.9733\n",
      "测试损失: 94.2373, 准确率: 0.7234\n",
      "------第50轮训练-------\n",
      "训练损失: 198.1136\n",
      "测试损失: 95.4886, 准确率: 0.7260\n",
      "------第51轮训练-------\n",
      "训练损失: 196.4248\n",
      "测试损失: 94.2577, 准确率: 0.7239\n",
      "------第52轮训练-------\n",
      "训练损失: 195.9273\n",
      "测试损失: 95.9843, 准确率: 0.7260\n",
      "------第53轮训练-------\n",
      "训练损失: 195.4028\n",
      "测试损失: 94.1411, 准确率: 0.7239\n",
      "------第54轮训练-------\n",
      "训练损失: 194.4343\n",
      "测试损失: 94.4631, 准确率: 0.7159\n",
      "------第55轮训练-------\n",
      "训练损失: 194.1378\n",
      "测试损失: 92.6192, 准确率: 0.7293\n",
      "------第56轮训练-------\n",
      "训练损失: 192.3882\n",
      "测试损失: 93.2629, 准确率: 0.7308\n",
      "------第57轮训练-------\n",
      "训练损失: 191.6083\n",
      "测试损失: 95.8487, 准确率: 0.7275\n",
      "------第58轮训练-------\n",
      "训练损失: 191.2191\n",
      "测试损失: 95.3993, 准确率: 0.7228\n",
      "------第59轮训练-------\n",
      "训练损失: 190.0261\n",
      "测试损失: 92.8231, 准确率: 0.7337\n",
      "------第60轮训练-------\n",
      "训练损失: 189.0453\n",
      "测试损失: 91.9118, 准确率: 0.7315\n",
      "------第61轮训练-------\n",
      "训练损失: 188.0813\n",
      "测试损失: 92.2987, 准确率: 0.7330\n",
      "------第62轮训练-------\n",
      "训练损失: 188.0707\n",
      "测试损失: 92.4561, 准确率: 0.7317\n",
      "------第63轮训练-------\n",
      "训练损失: 187.5821\n",
      "测试损失: 93.5574, 准确率: 0.7347\n",
      "------第64轮训练-------\n",
      "训练损失: 186.6405\n",
      "测试损失: 91.6878, 准确率: 0.7316\n",
      "------第65轮训练-------\n",
      "训练损失: 185.3277\n",
      "测试损失: 90.4402, 准确率: 0.7381\n",
      "------第66轮训练-------\n",
      "训练损失: 184.4285\n",
      "测试损失: 93.8701, 准确率: 0.7272\n",
      "------第67轮训练-------\n",
      "训练损失: 185.2650\n",
      "测试损失: 90.5630, 准确率: 0.7332\n",
      "------第68轮训练-------\n",
      "训练损失: 182.7709\n",
      "测试损失: 90.6300, 准确率: 0.7321\n",
      "------第69轮训练-------\n",
      "训练损失: 183.1592\n",
      "测试损失: 90.0813, 准确率: 0.7334\n",
      "------第70轮训练-------\n",
      "训练损失: 182.1619\n",
      "测试损失: 90.7196, 准确率: 0.7292\n",
      "------第71轮训练-------\n",
      "训练损失: 182.4487\n",
      "测试损失: 90.1465, 准确率: 0.7415\n",
      "------第72轮训练-------\n",
      "训练损失: 180.6445\n",
      "测试损失: 91.8655, 准确率: 0.7323\n",
      "------第73轮训练-------\n",
      "训练损失: 179.6300\n",
      "测试损失: 91.3024, 准确率: 0.7332\n",
      "------第74轮训练-------\n",
      "训练损失: 179.9927\n",
      "测试损失: 91.1930, 准确率: 0.7419\n",
      "------第75轮训练-------\n",
      "训练损失: 179.8584\n",
      "测试损失: 90.2162, 准确率: 0.7362\n",
      "------第76轮训练-------\n",
      "训练损失: 177.9512\n",
      "测试损失: 90.1911, 准确率: 0.7382\n",
      "------第77轮训练-------\n",
      "训练损失: 178.4011\n",
      "测试损失: 92.0675, 准确率: 0.7336\n",
      "------第78轮训练-------\n",
      "训练损失: 177.0199\n",
      "测试损失: 91.6925, 准确率: 0.7340\n",
      "------第79轮训练-------\n",
      "训练损失: 176.5939\n",
      "测试损失: 90.7663, 准确率: 0.7348\n",
      "------第80轮训练-------\n",
      "训练损失: 175.2115\n",
      "测试损失: 91.8315, 准确率: 0.7403\n",
      "------第81轮训练-------\n",
      "训练损失: 175.5185\n",
      "测试损失: 90.4999, 准确率: 0.7348\n",
      "------第82轮训练-------\n",
      "训练损失: 175.5920\n",
      "测试损失: 97.9219, 准确率: 0.7395\n",
      "------第83轮训练-------\n",
      "训练损失: 174.5684\n",
      "测试损失: 90.5726, 准确率: 0.7378\n",
      "------第84轮训练-------\n",
      "训练损失: 173.9472\n",
      "测试损失: 89.4988, 准确率: 0.7372\n",
      "------第85轮训练-------\n",
      "训练损失: 173.0116\n",
      "测试损失: 91.0372, 准确率: 0.7332\n",
      "------第86轮训练-------\n",
      "训练损失: 172.7083\n",
      "测试损失: 87.5487, 准确率: 0.7422\n",
      "------第87轮训练-------\n",
      "训练损失: 171.9178\n",
      "测试损失: 88.4663, 准确率: 0.7417\n",
      "------第88轮训练-------\n",
      "训练损失: 171.9424\n",
      "测试损失: 88.4134, 准确率: 0.7416\n",
      "------第89轮训练-------\n",
      "训练损失: 171.0544\n",
      "测试损失: 88.1988, 准确率: 0.7441\n",
      "------第90轮训练-------\n",
      "训练损失: 171.7327\n",
      "测试损失: 89.5884, 准确率: 0.7408\n",
      "------第91轮训练-------\n",
      "训练损失: 169.4166\n",
      "测试损失: 89.8422, 准确率: 0.7377\n",
      "------第92轮训练-------\n",
      "训练损失: 169.7139\n",
      "测试损失: 90.4740, 准确率: 0.7408\n",
      "------第93轮训练-------\n",
      "训练损失: 170.1894\n",
      "测试损失: 88.0640, 准确率: 0.7438\n",
      "------第94轮训练-------\n",
      "训练损失: 169.0404\n",
      "测试损失: 89.3573, 准确率: 0.7471\n",
      "------第95轮训练-------\n",
      "训练损失: 168.0557\n",
      "测试损失: 88.7715, 准确率: 0.7445\n",
      "------第96轮训练-------\n",
      "训练损失: 168.2958\n",
      "测试损失: 86.7246, 准确率: 0.7458\n",
      "------第97轮训练-------\n",
      "训练损失: 168.7303\n",
      "测试损失: 89.4761, 准确率: 0.7403\n",
      "------第98轮训练-------\n",
      "训练损失: 167.3579\n",
      "测试损失: 89.9916, 准确率: 0.7387\n",
      "------第99轮训练-------\n",
      "训练损失: 167.4668\n",
      "测试损失: 87.0896, 准确率: 0.7438\n",
      "------第100轮训练-------\n",
      "训练损失: 166.6992\n",
      "测试损失: 87.7480, 准确率: 0.7457\n",
      "模型已保存到 cnn_model.pth\n",
      "总训练时间: 338.32 秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 检测设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'cleaned_aki_data2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "X = df.iloc[:, :-1].values  # 特征\n",
    "Y = df['match_flag'].values  # 标签\n",
    "\n",
    "# 数据分割\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # 添加通道维度\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)  # 转换为浮点型\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)  # 转换为浮点型\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# 定义 CNN 模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(1),\n",
    "            nn.Conv1d(16, 32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(1),\n",
    "            nn.Conv1d(32, 64, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.model2 = nn.Sequential(\n",
    "            nn.Linear(in_features=2560, out_features=128, bias=True),\n",
    "            nn.ReLU(),  # 使用 ReLU 代替 Sigmoid\n",
    "            nn.Linear(in_features=128, out_features=1, bias=True),  # 单个输出值\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.model1(input)\n",
    "        x = self.model2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 实例化模型并移动到设备\n",
    "cnn = CNN().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# 超参数设置\n",
    "epoch = 100\n",
    "\n",
    "# TensorBoard 日志记录\n",
    "writer = SummaryWriter('loss_train')\n",
    "start_time = time.time()\n",
    "\n",
    "# 开始训练\n",
    "for i in range(epoch):\n",
    "    print(f'------第{i + 1}轮训练-------')\n",
    "\n",
    "    cnn.train()\n",
    "    total_train_loss = 0\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = cnn(batch_data).squeeze(1)  # 去掉通道维度\n",
    "        loss = loss_fn(outputs, batch_labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    print(f'训练损失: {total_train_loss:.4f}')\n",
    "    writer.add_scalar('Train Loss', total_train_loss, i)\n",
    "\n",
    "    # 测试阶段\n",
    "    cnn.eval()\n",
    "    total_test_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "            outputs = cnn(batch_data).squeeze(1)  # 去掉通道维度\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            predictions = torch.sigmoid(outputs) > 0.5  # 转为概率并判断大于0.5\n",
    "            total_correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(test_dataset)\n",
    "    print(f'测试损失: {total_test_loss:.4f}, 准确率: {accuracy:.4f}')\n",
    "    writer.add_scalar('Test Loss', total_test_loss, i)\n",
    "    writer.add_scalar('Accuracy', accuracy, i)\n",
    "\n",
    "# 保存模型\n",
    "model_path = 'cnn_model.pth'\n",
    "torch.save(cnn.state_dict(), model_path)\n",
    "print(f'模型已保存到 {model_path}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'总训练时间: {end_time - start_time:.2f} 秒')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b87026-8fa5-4e0e-a06a-b83a7d5e7e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829361ec-91ae-4a92-a7d9-2eeb1b8b0f94",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ca5507-ba30-4d10-b434-6b893d3e45a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:33:06,148] A new study created in memory with name: no-name-7eb98df6-3e49-412d-b60e-c494e7622aa9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after preprocessing: (246014, 10, 4)\n",
      "Trial 0 | Epoch 1: Loss=0.6088, Accuracy=0.6697, Precision=0.6350, Recall=0.7980, F1=0.7072, ROC_AUC=0.7126\n",
      "Trial 0 | Epoch 2: Loss=0.6012, Accuracy=0.6761, Precision=0.6414, Recall=0.7988, F1=0.7115, ROC_AUC=0.7219\n",
      "Trial 0 | Epoch 3: Loss=0.5985, Accuracy=0.6792, Precision=0.6347, Recall=0.8443, F1=0.7246, ROC_AUC=0.7256\n",
      "Trial 0 | Epoch 4: Loss=0.5896, Accuracy=0.6841, Precision=0.6439, Recall=0.8240, F1=0.7229, ROC_AUC=0.7295\n",
      "Trial 0 | Epoch 5: Loss=0.5808, Accuracy=0.6885, Precision=0.6392, Recall=0.8658, F1=0.7354, ROC_AUC=0.7343\n",
      "Trial 0 | Epoch 6: Loss=0.5663, Accuracy=0.6976, Precision=0.6424, Recall=0.8918, F1=0.7468, ROC_AUC=0.7433\n",
      "Trial 0 | Epoch 7: Loss=0.5526, Accuracy=0.7058, Precision=0.6486, Recall=0.8986, F1=0.7534, ROC_AUC=0.7500\n",
      "Trial 0 | Epoch 8: Loss=0.5491, Accuracy=0.7118, Precision=0.6571, Recall=0.8857, F1=0.7545, ROC_AUC=0.7552\n",
      "Trial 0 | Epoch 9: Loss=0.5317, Accuracy=0.7194, Precision=0.6598, Recall=0.9058, F1=0.7635, ROC_AUC=0.7616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:35:43,276] Trial 0 finished with value: 0.727511686200122 and parameters: {'hidden_dim': 168, 'num_layers': 3, 'dropout': 0.40367633657648205, 'lr': 0.0015246343590330066}. Best is trial 0 with value: 0.727511686200122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 | Epoch 10: Loss=0.5188, Accuracy=0.7275, Precision=0.6616, Recall=0.9313, F1=0.7736, ROC_AUC=0.7668\n",
      "Trial 1 | Epoch 1: Loss=0.6101, Accuracy=0.6674, Precision=0.6306, Recall=0.8085, F1=0.7085, ROC_AUC=0.7121\n",
      "Trial 1 | Epoch 2: Loss=0.6030, Accuracy=0.6749, Precision=0.6398, Recall=0.8004, F1=0.7111, ROC_AUC=0.7202\n",
      "Trial 1 | Epoch 3: Loss=0.5985, Accuracy=0.6776, Precision=0.6443, Recall=0.7932, F1=0.7110, ROC_AUC=0.7233\n",
      "Trial 1 | Epoch 4: Loss=0.5964, Accuracy=0.6809, Precision=0.6393, Recall=0.8301, F1=0.7223, ROC_AUC=0.7254\n",
      "Trial 1 | Epoch 5: Loss=0.5925, Accuracy=0.6812, Precision=0.6465, Recall=0.7995, F1=0.7149, ROC_AUC=0.7275\n",
      "Trial 1 | Epoch 6: Loss=0.5882, Accuracy=0.6847, Precision=0.6398, Recall=0.8453, F1=0.7283, ROC_AUC=0.7306\n",
      "Trial 1 | Epoch 7: Loss=0.5801, Accuracy=0.6879, Precision=0.6373, Recall=0.8719, F1=0.7364, ROC_AUC=0.7362\n",
      "Trial 1 | Epoch 8: Loss=0.5737, Accuracy=0.6917, Precision=0.6410, Recall=0.8714, F1=0.7387, ROC_AUC=0.7391\n",
      "Trial 1 | Epoch 9: Loss=0.5644, Accuracy=0.6980, Precision=0.6446, Recall=0.8827, F1=0.7451, ROC_AUC=0.7440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:38:09,450] Trial 1 finished with value: 0.7007113339204661 and parameters: {'hidden_dim': 100, 'num_layers': 3, 'dropout': 0.4380936474936191, 'lr': 0.0010963816604935579}. Best is trial 0 with value: 0.727511686200122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 | Epoch 10: Loss=0.5592, Accuracy=0.7007, Precision=0.6484, Recall=0.8768, F1=0.7455, ROC_AUC=0.7455\n",
      "Trial 2 | Epoch 1: Loss=0.6102, Accuracy=0.6689, Precision=0.6375, Recall=0.7830, F1=0.7028, ROC_AUC=0.7138\n",
      "Trial 2 | Epoch 2: Loss=0.6052, Accuracy=0.6739, Precision=0.6447, Recall=0.7747, F1=0.7038, ROC_AUC=0.7192\n",
      "Trial 2 | Epoch 3: Loss=0.5973, Accuracy=0.6794, Precision=0.6433, Recall=0.8053, F1=0.7152, ROC_AUC=0.7269\n",
      "Trial 2 | Epoch 4: Loss=0.5918, Accuracy=0.6831, Precision=0.6444, Recall=0.8171, F1=0.7205, ROC_AUC=0.7304\n",
      "Trial 2 | Epoch 5: Loss=0.5860, Accuracy=0.6880, Precision=0.6479, Recall=0.8235, F1=0.7252, ROC_AUC=0.7350\n",
      "Trial 2 | Epoch 6: Loss=0.5779, Accuracy=0.6912, Precision=0.6444, Recall=0.8533, F1=0.7342, ROC_AUC=0.7389\n",
      "Trial 2 | Epoch 7: Loss=0.5663, Accuracy=0.6974, Precision=0.6536, Recall=0.8398, F1=0.7351, ROC_AUC=0.7462\n",
      "Trial 2 | Epoch 8: Loss=0.5568, Accuracy=0.7028, Precision=0.6522, Recall=0.8692, F1=0.7452, ROC_AUC=0.7513\n",
      "Trial 2 | Epoch 9: Loss=0.5465, Accuracy=0.7079, Precision=0.6477, Recall=0.9115, F1=0.7573, ROC_AUC=0.7555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:40:22,458] Trial 2 finished with value: 0.7174446175733351 and parameters: {'hidden_dim': 214, 'num_layers': 2, 'dropout': 0.49507466815662404, 'lr': 0.0008414068077127449}. Best is trial 0 with value: 0.727511686200122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 | Epoch 10: Loss=0.5315, Accuracy=0.7174, Precision=0.6600, Recall=0.8969, F1=0.7604, ROC_AUC=0.7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3102541752445873 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 | Epoch 1: Loss=0.6085, Accuracy=0.6703, Precision=0.6385, Recall=0.7850, F1=0.7042, ROC_AUC=0.7138\n",
      "Trial 3 | Epoch 2: Loss=0.6030, Accuracy=0.6750, Precision=0.6368, Recall=0.8144, F1=0.7148, ROC_AUC=0.7198\n",
      "Trial 3 | Epoch 3: Loss=0.5986, Accuracy=0.6786, Precision=0.6413, Recall=0.8107, F1=0.7161, ROC_AUC=0.7233\n",
      "Trial 3 | Epoch 4: Loss=0.5938, Accuracy=0.6802, Precision=0.6453, Recall=0.8002, F1=0.7144, ROC_AUC=0.7265\n",
      "Trial 3 | Epoch 5: Loss=0.5882, Accuracy=0.6832, Precision=0.6482, Recall=0.8013, F1=0.7166, ROC_AUC=0.7317\n",
      "Trial 3 | Epoch 6: Loss=0.5806, Accuracy=0.6869, Precision=0.6517, Recall=0.8028, F1=0.7194, ROC_AUC=0.7366\n",
      "Trial 3 | Epoch 7: Loss=0.5725, Accuracy=0.6918, Precision=0.6468, Recall=0.8450, F1=0.7328, ROC_AUC=0.7406\n",
      "Trial 3 | Epoch 8: Loss=0.5641, Accuracy=0.6982, Precision=0.6520, Recall=0.8502, F1=0.7380, ROC_AUC=0.7454\n",
      "Trial 3 | Epoch 9: Loss=0.5537, Accuracy=0.7041, Precision=0.6536, Recall=0.8684, F1=0.7459, ROC_AUC=0.7507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:41:45,871] Trial 3 finished with value: 0.7095454237517783 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.3102541752445873, 'lr': 0.000974997267505236}. Best is trial 0 with value: 0.727511686200122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 | Epoch 10: Loss=0.5491, Accuracy=0.7095, Precision=0.6533, Recall=0.8929, F1=0.7545, ROC_AUC=0.7531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3419225175738291 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 | Epoch 1: Loss=0.6055, Accuracy=0.6717, Precision=0.6429, Recall=0.7727, F1=0.7018, ROC_AUC=0.7181\n",
      "Trial 4 | Epoch 2: Loss=0.5962, Accuracy=0.6784, Precision=0.6435, Recall=0.7999, F1=0.7132, ROC_AUC=0.7262\n",
      "Trial 4 | Epoch 3: Loss=0.5876, Accuracy=0.6859, Precision=0.6490, Recall=0.8096, F1=0.7205, ROC_AUC=0.7320\n",
      "Trial 4 | Epoch 4: Loss=0.5772, Accuracy=0.6891, Precision=0.6533, Recall=0.8060, F1=0.7216, ROC_AUC=0.7403\n",
      "Trial 4 | Epoch 5: Loss=0.5663, Accuracy=0.6997, Precision=0.6570, Recall=0.8357, F1=0.7356, ROC_AUC=0.7483\n",
      "Trial 4 | Epoch 6: Loss=0.5444, Accuracy=0.7141, Precision=0.6588, Recall=0.8883, F1=0.7565, ROC_AUC=0.7562\n",
      "Trial 4 | Epoch 7: Loss=0.5292, Accuracy=0.7203, Precision=0.6647, Recall=0.8888, F1=0.7606, ROC_AUC=0.7650\n",
      "Trial 4 | Epoch 8: Loss=0.5184, Accuracy=0.7292, Precision=0.6770, Recall=0.8768, F1=0.7641, ROC_AUC=0.7744\n",
      "Trial 4 | Epoch 9: Loss=0.5063, Accuracy=0.7393, Precision=0.6773, Recall=0.9141, F1=0.7781, ROC_AUC=0.7811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:43:10,855] Trial 4 finished with value: 0.7397601788496714 and parameters: {'hidden_dim': 225, 'num_layers': 1, 'dropout': 0.3419225175738291, 'lr': 0.0013195040152683486}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 | Epoch 10: Loss=0.5044, Accuracy=0.7398, Precision=0.6838, Recall=0.8920, F1=0.7741, ROC_AUC=0.7831\n",
      "Trial 5 | Epoch 1: Loss=0.6068, Accuracy=0.6717, Precision=0.6289, Recall=0.8375, F1=0.7184, ROC_AUC=0.7170\n",
      "Trial 5 | Epoch 2: Loss=0.5989, Accuracy=0.6809, Precision=0.6431, Recall=0.8132, F1=0.7182, ROC_AUC=0.7211\n",
      "Trial 5 | Epoch 3: Loss=0.5881, Accuracy=0.6834, Precision=0.6470, Recall=0.8075, F1=0.7184, ROC_AUC=0.7321\n",
      "Trial 5 | Epoch 4: Loss=0.5769, Accuracy=0.6890, Precision=0.6460, Recall=0.8361, F1=0.7289, ROC_AUC=0.7369\n",
      "Trial 5 | Epoch 5: Loss=0.5596, Accuracy=0.7002, Precision=0.6443, Recall=0.8939, F1=0.7489, ROC_AUC=0.7462\n",
      "Trial 5 | Epoch 6: Loss=0.5452, Accuracy=0.7096, Precision=0.6511, Recall=0.9029, F1=0.7566, ROC_AUC=0.7533\n",
      "Trial 5 | Epoch 7: Loss=0.5336, Accuracy=0.7168, Precision=0.6562, Recall=0.9104, F1=0.7627, ROC_AUC=0.7584\n",
      "Trial 5 | Epoch 8: Loss=0.5264, Accuracy=0.7232, Precision=0.6603, Recall=0.9195, F1=0.7686, ROC_AUC=0.7617\n",
      "Trial 5 | Epoch 9: Loss=0.5217, Accuracy=0.7261, Precision=0.6580, Recall=0.9415, F1=0.7746, ROC_AUC=0.7637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:45:08,847] Trial 5 finished with value: 0.730844793713163 and parameters: {'hidden_dim': 135, 'num_layers': 2, 'dropout': 0.4993040309103929, 'lr': 0.002958362697453893}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 | Epoch 10: Loss=0.5163, Accuracy=0.7308, Precision=0.6620, Recall=0.9434, F1=0.7780, ROC_AUC=0.7670\n",
      "Trial 6 | Epoch 1: Loss=0.6108, Accuracy=0.6697, Precision=0.6476, Recall=0.7443, F1=0.6926, ROC_AUC=0.7156\n",
      "Trial 6 | Epoch 2: Loss=0.5984, Accuracy=0.6781, Precision=0.6466, Recall=0.7854, F1=0.7093, ROC_AUC=0.7241\n",
      "Trial 6 | Epoch 3: Loss=0.5920, Accuracy=0.6794, Precision=0.6465, Recall=0.7916, F1=0.7117, ROC_AUC=0.7279\n",
      "Trial 6 | Epoch 4: Loss=0.5766, Accuracy=0.6903, Precision=0.6429, Recall=0.8562, F1=0.7343, ROC_AUC=0.7372\n",
      "Trial 6 | Epoch 5: Loss=0.5671, Accuracy=0.6935, Precision=0.6406, Recall=0.8818, F1=0.7421, ROC_AUC=0.7421\n",
      "Trial 6 | Epoch 6: Loss=0.5541, Accuracy=0.7002, Precision=0.6505, Recall=0.8654, F1=0.7427, ROC_AUC=0.7484\n",
      "Trial 6 | Epoch 7: Loss=0.5473, Accuracy=0.7052, Precision=0.6443, Recall=0.9163, F1=0.7566, ROC_AUC=0.7497\n",
      "Trial 6 | Epoch 8: Loss=0.5364, Accuracy=0.7113, Precision=0.6574, Recall=0.8823, F1=0.7534, ROC_AUC=0.7556\n",
      "Trial 6 | Epoch 9: Loss=0.5314, Accuracy=0.7155, Precision=0.6534, Recall=0.9179, F1=0.7634, ROC_AUC=0.7589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:46:46,859] Trial 6 finished with value: 0.7209809633493666 and parameters: {'hidden_dim': 74, 'num_layers': 2, 'dropout': 0.23741820045155962, 'lr': 0.0031495157699304847}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 | Epoch 10: Loss=0.5266, Accuracy=0.7210, Precision=0.6571, Recall=0.9241, F1=0.7681, ROC_AUC=0.7611\n",
      "Trial 7 | Epoch 1: Loss=0.6054, Accuracy=0.6720, Precision=0.6376, Recall=0.7969, F1=0.7084, ROC_AUC=0.7167\n",
      "Trial 7 | Epoch 2: Loss=0.5961, Accuracy=0.6798, Precision=0.6398, Recall=0.8229, F1=0.7199, ROC_AUC=0.7264\n",
      "Trial 7 | Epoch 3: Loss=0.5854, Accuracy=0.6854, Precision=0.6424, Recall=0.8362, F1=0.7266, ROC_AUC=0.7328\n",
      "Trial 7 | Epoch 4: Loss=0.5713, Accuracy=0.6925, Precision=0.6486, Recall=0.8401, F1=0.7320, ROC_AUC=0.7401\n",
      "Trial 7 | Epoch 5: Loss=0.5579, Accuracy=0.7016, Precision=0.6470, Recall=0.8871, F1=0.7483, ROC_AUC=0.7460\n",
      "Trial 7 | Epoch 6: Loss=0.5476, Accuracy=0.7079, Precision=0.6525, Recall=0.8893, F1=0.7527, ROC_AUC=0.7512\n",
      "Trial 7 | Epoch 7: Loss=0.5360, Accuracy=0.7167, Precision=0.6561, Recall=0.9105, F1=0.7627, ROC_AUC=0.7565\n",
      "Trial 7 | Epoch 8: Loss=0.5275, Accuracy=0.7222, Precision=0.6591, Recall=0.9204, F1=0.7682, ROC_AUC=0.7614\n",
      "Trial 7 | Epoch 9: Loss=0.5218, Accuracy=0.7258, Precision=0.6605, Recall=0.9292, F1=0.7721, ROC_AUC=0.7642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:48:45,666] Trial 7 finished with value: 0.7261161167942551 and parameters: {'hidden_dim': 126, 'num_layers': 2, 'dropout': 0.3919543088860086, 'lr': 0.0038956949983801827}. Best is trial 4 with value: 0.7397601788496714.\n",
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.21435885043254418 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 | Epoch 10: Loss=0.5245, Accuracy=0.7261, Precision=0.6636, Recall=0.9173, F1=0.7701, ROC_AUC=0.7642\n",
      "Trial 8 | Epoch 1: Loss=0.6283, Accuracy=0.6469, Precision=0.6294, Recall=0.7146, F1=0.6693, ROC_AUC=0.6912\n",
      "Trial 8 | Epoch 2: Loss=0.6200, Accuracy=0.6602, Precision=0.6337, Recall=0.7596, F1=0.6909, ROC_AUC=0.7038\n",
      "Trial 8 | Epoch 3: Loss=0.6133, Accuracy=0.6664, Precision=0.6458, Recall=0.7368, F1=0.6883, ROC_AUC=0.7113\n",
      "Trial 8 | Epoch 4: Loss=0.6100, Accuracy=0.6693, Precision=0.6401, Recall=0.7734, F1=0.7005, ROC_AUC=0.7146\n",
      "Trial 8 | Epoch 5: Loss=0.6086, Accuracy=0.6713, Precision=0.6417, Recall=0.7756, F1=0.7023, ROC_AUC=0.7160\n",
      "Trial 8 | Epoch 6: Loss=0.6059, Accuracy=0.6728, Precision=0.6455, Recall=0.7662, F1=0.7007, ROC_AUC=0.7184\n",
      "Trial 8 | Epoch 7: Loss=0.6046, Accuracy=0.6750, Precision=0.6437, Recall=0.7839, F1=0.7069, ROC_AUC=0.7193\n",
      "Trial 8 | Epoch 8: Loss=0.6032, Accuracy=0.6760, Precision=0.6425, Recall=0.7935, F1=0.7101, ROC_AUC=0.7208\n",
      "Trial 8 | Epoch 9: Loss=0.6023, Accuracy=0.6758, Precision=0.6446, Recall=0.7837, F1=0.7074, ROC_AUC=0.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:50:08,607] Trial 8 finished with value: 0.6777725086376262 and parameters: {'hidden_dim': 165, 'num_layers': 1, 'dropout': 0.21435885043254418, 'lr': 0.00012186145450938273}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 | Epoch 10: Loss=0.6008, Accuracy=0.6778, Precision=0.6426, Recall=0.8012, F1=0.7132, ROC_AUC=0.7227\n",
      "Trial 9 | Epoch 1: Loss=0.6168, Accuracy=0.6661, Precision=0.6300, Recall=0.8048, F1=0.7068, ROC_AUC=0.7101\n",
      "Trial 9 | Epoch 2: Loss=0.6008, Accuracy=0.6776, Precision=0.6350, Recall=0.8350, F1=0.7214, ROC_AUC=0.7220\n",
      "Trial 9 | Epoch 3: Loss=0.5998, Accuracy=0.6785, Precision=0.6327, Recall=0.8510, F1=0.7258, ROC_AUC=0.7233\n",
      "Trial 9 | Epoch 4: Loss=0.5995, Accuracy=0.6756, Precision=0.6497, Recall=0.7619, F1=0.7013, ROC_AUC=0.7249\n",
      "Trial 9 | Epoch 5: Loss=0.5910, Accuracy=0.6808, Precision=0.6308, Recall=0.8720, F1=0.7321, ROC_AUC=0.7273\n",
      "Trial 9 | Epoch 6: Loss=0.5919, Accuracy=0.6819, Precision=0.6345, Recall=0.8581, F1=0.7296, ROC_AUC=0.7253\n",
      "Trial 9 | Epoch 7: Loss=0.5887, Accuracy=0.6847, Precision=0.6437, Recall=0.8276, F1=0.7241, ROC_AUC=0.7291\n",
      "Trial 9 | Epoch 8: Loss=0.5851, Accuracy=0.6871, Precision=0.6415, Recall=0.8482, F1=0.7305, ROC_AUC=0.7316\n",
      "Trial 9 | Epoch 9: Loss=0.5885, Accuracy=0.6851, Precision=0.6378, Recall=0.8568, F1=0.7312, ROC_AUC=0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:54:02,005] Trial 9 finished with value: 0.6829618589526455 and parameters: {'hidden_dim': 251, 'num_layers': 3, 'dropout': 0.4514407380938953, 'lr': 0.008399856960483792}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 | Epoch 10: Loss=0.5869, Accuracy=0.6830, Precision=0.6327, Recall=0.8723, F1=0.7334, ROC_AUC=0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.13493739066810584 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 | Epoch 1: Loss=0.6147, Accuracy=0.6646, Precision=0.6415, Recall=0.7465, F1=0.6900, ROC_AUC=0.7095\n",
      "Trial 10 | Epoch 2: Loss=0.6074, Accuracy=0.6712, Precision=0.6466, Recall=0.7551, F1=0.6967, ROC_AUC=0.7169\n",
      "Trial 10 | Epoch 3: Loss=0.6054, Accuracy=0.6763, Precision=0.6384, Recall=0.8132, F1=0.7153, ROC_AUC=0.7208\n",
      "Trial 10 | Epoch 4: Loss=0.6013, Accuracy=0.6770, Precision=0.6394, Recall=0.8116, F1=0.7153, ROC_AUC=0.7219\n",
      "Trial 10 | Epoch 5: Loss=0.5985, Accuracy=0.6777, Precision=0.6421, Recall=0.8032, F1=0.7137, ROC_AUC=0.7230\n",
      "Trial 10 | Epoch 6: Loss=0.5943, Accuracy=0.6812, Precision=0.6434, Recall=0.8130, F1=0.7183, ROC_AUC=0.7272\n",
      "Trial 10 | Epoch 7: Loss=0.5933, Accuracy=0.6818, Precision=0.6409, Recall=0.8270, F1=0.7222, ROC_AUC=0.7282\n",
      "Trial 10 | Epoch 8: Loss=0.5878, Accuracy=0.6850, Precision=0.6491, Recall=0.8052, F1=0.7188, ROC_AUC=0.7327\n",
      "Trial 10 | Epoch 9: Loss=0.5817, Accuracy=0.6889, Precision=0.6477, Recall=0.8285, F1=0.7270, ROC_AUC=0.7363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:55:30,230] Trial 10 finished with value: 0.6922566221800691 and parameters: {'hidden_dim': 219, 'num_layers': 1, 'dropout': 0.13493739066810584, 'lr': 0.000292274934830643}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 | Epoch 10: Loss=0.5759, Accuracy=0.6923, Precision=0.6500, Recall=0.8329, F1=0.7302, ROC_AUC=0.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30970773908645177 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 | Epoch 1: Loss=0.6040, Accuracy=0.6747, Precision=0.6467, Recall=0.7698, F1=0.7029, ROC_AUC=0.7198\n",
      "Trial 11 | Epoch 2: Loss=0.5941, Accuracy=0.6825, Precision=0.6463, Recall=0.8064, F1=0.7175, ROC_AUC=0.7268\n",
      "Trial 11 | Epoch 3: Loss=0.5809, Accuracy=0.6882, Precision=0.6464, Recall=0.8311, F1=0.7272, ROC_AUC=0.7361\n",
      "Trial 11 | Epoch 4: Loss=0.5684, Accuracy=0.6954, Precision=0.6471, Recall=0.8596, F1=0.7384, ROC_AUC=0.7415\n",
      "Trial 11 | Epoch 5: Loss=0.5515, Accuracy=0.7056, Precision=0.6499, Recall=0.8915, F1=0.7518, ROC_AUC=0.7501\n",
      "Trial 11 | Epoch 6: Loss=0.5422, Accuracy=0.7098, Precision=0.6529, Recall=0.8958, F1=0.7553, ROC_AUC=0.7567\n",
      "Trial 11 | Epoch 7: Loss=0.5301, Accuracy=0.7194, Precision=0.6665, Recall=0.8780, F1=0.7578, ROC_AUC=0.7637\n",
      "Trial 11 | Epoch 8: Loss=0.5171, Accuracy=0.7269, Precision=0.6694, Recall=0.8968, F1=0.7666, ROC_AUC=0.7710\n",
      "Trial 11 | Epoch 9: Loss=0.5180, Accuracy=0.7286, Precision=0.6754, Recall=0.8804, F1=0.7644, ROC_AUC=0.7718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:56:53,872] Trial 11 finished with value: 0.7369012939502744 and parameters: {'hidden_dim': 131, 'num_layers': 1, 'dropout': 0.30970773908645177, 'lr': 0.0025489323529980703}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 | Epoch 10: Loss=0.5089, Accuracy=0.7369, Precision=0.6737, Recall=0.9189, F1=0.7774, ROC_AUC=0.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3312409781815586 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 | Epoch 1: Loss=0.6105, Accuracy=0.6691, Precision=0.6419, Recall=0.7647, F1=0.6979, ROC_AUC=0.7140\n",
      "Trial 12 | Epoch 2: Loss=0.6044, Accuracy=0.6743, Precision=0.6391, Recall=0.8010, F1=0.7109, ROC_AUC=0.7189\n",
      "Trial 12 | Epoch 3: Loss=0.6013, Accuracy=0.6755, Precision=0.6448, Recall=0.7815, F1=0.7066, ROC_AUC=0.7220\n",
      "Trial 12 | Epoch 4: Loss=0.6006, Accuracy=0.6753, Precision=0.6430, Recall=0.7883, F1=0.7083, ROC_AUC=0.7223\n",
      "Trial 12 | Epoch 5: Loss=0.5931, Accuracy=0.6801, Precision=0.6476, Recall=0.7903, F1=0.7119, ROC_AUC=0.7276\n",
      "Trial 12 | Epoch 6: Loss=0.5899, Accuracy=0.6823, Precision=0.6455, Recall=0.8088, F1=0.7180, ROC_AUC=0.7300\n",
      "Trial 12 | Epoch 7: Loss=0.5823, Accuracy=0.6879, Precision=0.6510, Recall=0.8097, F1=0.7218, ROC_AUC=0.7352\n",
      "Trial 12 | Epoch 8: Loss=0.5743, Accuracy=0.6907, Precision=0.6535, Recall=0.8118, F1=0.7241, ROC_AUC=0.7402\n",
      "Trial 12 | Epoch 9: Loss=0.5669, Accuracy=0.6957, Precision=0.6544, Recall=0.8294, F1=0.7316, ROC_AUC=0.7429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:58:16,629] Trial 12 finished with value: 0.7027708149854346 and parameters: {'hidden_dim': 193, 'num_layers': 1, 'dropout': 0.3312409781815586, 'lr': 0.0004509624401760412}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 | Epoch 10: Loss=0.5602, Accuracy=0.7028, Precision=0.6483, Recall=0.8866, F1=0.7489, ROC_AUC=0.7483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.24442120610390372 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 | Epoch 1: Loss=0.6024, Accuracy=0.6758, Precision=0.6324, Recall=0.8397, F1=0.7215, ROC_AUC=0.7204\n",
      "Trial 13 | Epoch 2: Loss=0.5930, Accuracy=0.6783, Precision=0.6508, Recall=0.7694, F1=0.7051, ROC_AUC=0.7273\n",
      "Trial 13 | Epoch 3: Loss=0.5860, Accuracy=0.6866, Precision=0.6416, Recall=0.8455, F1=0.7296, ROC_AUC=0.7299\n",
      "Trial 13 | Epoch 4: Loss=0.5734, Accuracy=0.6928, Precision=0.6448, Recall=0.8583, F1=0.7364, ROC_AUC=0.7349\n",
      "Trial 13 | Epoch 5: Loss=0.5678, Accuracy=0.6940, Precision=0.6478, Recall=0.8504, F1=0.7354, ROC_AUC=0.7390\n",
      "Trial 13 | Epoch 6: Loss=0.5629, Accuracy=0.6967, Precision=0.6499, Recall=0.8531, F1=0.7377, ROC_AUC=0.7422\n",
      "Trial 13 | Epoch 7: Loss=0.5575, Accuracy=0.7002, Precision=0.6484, Recall=0.8746, F1=0.7447, ROC_AUC=0.7458\n",
      "Trial 13 | Epoch 8: Loss=0.5570, Accuracy=0.7020, Precision=0.6471, Recall=0.8885, F1=0.7488, ROC_AUC=0.7431\n",
      "Trial 13 | Epoch 9: Loss=0.5519, Accuracy=0.7051, Precision=0.6513, Recall=0.8828, F1=0.7496, ROC_AUC=0.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 21:59:39,823] Trial 13 finished with value: 0.705006435878328 and parameters: {'hidden_dim': 136, 'num_layers': 1, 'dropout': 0.24442120610390372, 'lr': 0.008766676680979505}. Best is trial 4 with value: 0.7397601788496714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 | Epoch 10: Loss=0.5523, Accuracy=0.7050, Precision=0.6485, Recall=0.8952, F1=0.7522, ROC_AUC=0.7464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.34682404683596796 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 | Epoch 1: Loss=0.6085, Accuracy=0.6691, Precision=0.6428, Recall=0.7612, F1=0.6970, ROC_AUC=0.7143\n",
      "Trial 14 | Epoch 2: Loss=0.5962, Accuracy=0.6802, Precision=0.6439, Recall=0.8065, F1=0.7161, ROC_AUC=0.7256\n",
      "Trial 14 | Epoch 3: Loss=0.5870, Accuracy=0.6859, Precision=0.6463, Recall=0.8210, F1=0.7233, ROC_AUC=0.7325\n",
      "Trial 14 | Epoch 4: Loss=0.5771, Accuracy=0.6913, Precision=0.6503, Recall=0.8279, F1=0.7284, ROC_AUC=0.7386\n",
      "Trial 14 | Epoch 5: Loss=0.5594, Accuracy=0.7014, Precision=0.6512, Recall=0.8672, F1=0.7438, ROC_AUC=0.7478\n",
      "Trial 14 | Epoch 6: Loss=0.5412, Accuracy=0.7145, Precision=0.6591, Recall=0.8884, F1=0.7568, ROC_AUC=0.7595\n",
      "Trial 14 | Epoch 7: Loss=0.5262, Accuracy=0.7227, Precision=0.6689, Recall=0.8818, F1=0.7608, ROC_AUC=0.7684\n",
      "Trial 14 | Epoch 8: Loss=0.5151, Accuracy=0.7302, Precision=0.6735, Recall=0.8935, F1=0.7681, ROC_AUC=0.7735\n",
      "Trial 14 | Epoch 9: Loss=0.5092, Accuracy=0.7339, Precision=0.6797, Recall=0.8850, F1=0.7689, ROC_AUC=0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:01:02,501] Trial 14 finished with value: 0.7415486755639862 and parameters: {'hidden_dim': 192, 'num_layers': 1, 'dropout': 0.34682404683596796, 'lr': 0.0015712489049957061}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 | Epoch 10: Loss=0.5011, Accuracy=0.7415, Precision=0.6827, Recall=0.9027, F1=0.7774, ROC_AUC=0.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3601920229161753 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 | Epoch 1: Loss=0.6111, Accuracy=0.6687, Precision=0.6451, Recall=0.7502, F1=0.6937, ROC_AUC=0.7143\n",
      "Trial 15 | Epoch 2: Loss=0.6059, Accuracy=0.6751, Precision=0.6392, Recall=0.8042, F1=0.7123, ROC_AUC=0.7192\n",
      "Trial 15 | Epoch 3: Loss=0.6029, Accuracy=0.6767, Precision=0.6380, Recall=0.8171, F1=0.7165, ROC_AUC=0.7216\n",
      "Trial 15 | Epoch 4: Loss=0.5953, Accuracy=0.6791, Precision=0.6453, Recall=0.7953, F1=0.7125, ROC_AUC=0.7271\n",
      "Trial 15 | Epoch 5: Loss=0.5896, Accuracy=0.6827, Precision=0.6476, Recall=0.8015, F1=0.7164, ROC_AUC=0.7308\n",
      "Trial 15 | Epoch 6: Loss=0.5840, Accuracy=0.6863, Precision=0.6431, Recall=0.8371, F1=0.7274, ROC_AUC=0.7340\n",
      "Trial 15 | Epoch 7: Loss=0.5739, Accuracy=0.6934, Precision=0.6471, Recall=0.8507, F1=0.7351, ROC_AUC=0.7414\n",
      "Trial 15 | Epoch 8: Loss=0.5632, Accuracy=0.6974, Precision=0.6559, Recall=0.8302, F1=0.7329, ROC_AUC=0.7473\n",
      "Trial 15 | Epoch 9: Loss=0.5523, Accuracy=0.7058, Precision=0.6566, Recall=0.8629, F1=0.7457, ROC_AUC=0.7533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:02:38,666] Trial 15 finished with value: 0.709992547930357 and parameters: {'hidden_dim': 243, 'num_layers': 1, 'dropout': 0.3601920229161753, 'lr': 0.0004669752843095286}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 | Epoch 10: Loss=0.5430, Accuracy=0.7100, Precision=0.6666, Recall=0.8401, F1=0.7434, ROC_AUC=0.7589\n",
      "Trial 16 | Epoch 1: Loss=0.6059, Accuracy=0.6723, Precision=0.6388, Recall=0.7931, F1=0.7076, ROC_AUC=0.7161\n",
      "Trial 16 | Epoch 2: Loss=0.5985, Accuracy=0.6782, Precision=0.6372, Recall=0.8275, F1=0.7200, ROC_AUC=0.7244\n",
      "Trial 16 | Epoch 3: Loss=0.5912, Accuracy=0.6803, Precision=0.6452, Recall=0.8013, F1=0.7148, ROC_AUC=0.7286\n",
      "Trial 16 | Epoch 4: Loss=0.5810, Accuracy=0.6872, Precision=0.6454, Recall=0.8309, F1=0.7265, ROC_AUC=0.7353\n",
      "Trial 16 | Epoch 5: Loss=0.5690, Accuracy=0.6952, Precision=0.6534, Recall=0.8316, F1=0.7318, ROC_AUC=0.7433\n",
      "Trial 16 | Epoch 6: Loss=0.5485, Accuracy=0.7064, Precision=0.6522, Recall=0.8843, F1=0.7507, ROC_AUC=0.7524\n",
      "Trial 16 | Epoch 7: Loss=0.5349, Accuracy=0.7159, Precision=0.6551, Recall=0.9115, F1=0.7624, ROC_AUC=0.7601\n",
      "Trial 16 | Epoch 8: Loss=0.5192, Accuracy=0.7254, Precision=0.6634, Recall=0.9152, F1=0.7692, ROC_AUC=0.7676\n",
      "Trial 16 | Epoch 9: Loss=0.5162, Accuracy=0.7309, Precision=0.6693, Recall=0.9129, F1=0.7724, ROC_AUC=0.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:05:03,191] Trial 16 finished with value: 0.7406679764243614 and parameters: {'hidden_dim': 194, 'num_layers': 2, 'dropout': 0.27703656649961217, 'lr': 0.0015789818908330615}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 | Epoch 10: Loss=0.5015, Accuracy=0.7407, Precision=0.6719, Recall=0.9407, F1=0.7839, ROC_AUC=0.7780\n",
      "Trial 17 | Epoch 1: Loss=0.6118, Accuracy=0.6716, Precision=0.6476, Recall=0.7528, F1=0.6963, ROC_AUC=0.7182\n",
      "Trial 17 | Epoch 2: Loss=0.5942, Accuracy=0.6802, Precision=0.6446, Recall=0.8034, F1=0.7153, ROC_AUC=0.7265\n",
      "Trial 17 | Epoch 3: Loss=0.5794, Accuracy=0.6885, Precision=0.6405, Recall=0.8595, F1=0.7340, ROC_AUC=0.7350\n",
      "Trial 17 | Epoch 4: Loss=0.5657, Accuracy=0.6971, Precision=0.6429, Recall=0.8868, F1=0.7454, ROC_AUC=0.7414\n",
      "Trial 17 | Epoch 5: Loss=0.5474, Accuracy=0.7088, Precision=0.6533, Recall=0.8898, F1=0.7534, ROC_AUC=0.7508\n",
      "Trial 17 | Epoch 6: Loss=0.5413, Accuracy=0.7125, Precision=0.6526, Recall=0.9085, F1=0.7596, ROC_AUC=0.7539\n",
      "Trial 17 | Epoch 7: Loss=0.5360, Accuracy=0.7182, Precision=0.6579, Recall=0.9090, F1=0.7633, ROC_AUC=0.7577\n",
      "Trial 17 | Epoch 8: Loss=0.5297, Accuracy=0.7229, Precision=0.6617, Recall=0.9122, F1=0.7670, ROC_AUC=0.7586\n",
      "Trial 17 | Epoch 9: Loss=0.5291, Accuracy=0.7227, Precision=0.6638, Recall=0.9026, F1=0.7650, ROC_AUC=0.7622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:07:13,732] Trial 17 finished with value: 0.72972020865795 and parameters: {'hidden_dim': 191, 'num_layers': 2, 'dropout': 0.1980095115597431, 'lr': 0.005426774530039858}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 | Epoch 10: Loss=0.5208, Accuracy=0.7297, Precision=0.6605, Recall=0.9452, F1=0.7776, ROC_AUC=0.7655\n",
      "Trial 18 | Epoch 1: Loss=0.6097, Accuracy=0.6699, Precision=0.6441, Recall=0.7594, F1=0.6970, ROC_AUC=0.7152\n",
      "Trial 18 | Epoch 2: Loss=0.6066, Accuracy=0.6720, Precision=0.6308, Recall=0.8293, F1=0.7166, ROC_AUC=0.7174\n",
      "Trial 18 | Epoch 3: Loss=0.5986, Accuracy=0.6787, Precision=0.6453, Recall=0.7937, F1=0.7118, ROC_AUC=0.7237\n",
      "Trial 18 | Epoch 4: Loss=0.5961, Accuracy=0.6800, Precision=0.6423, Recall=0.8124, F1=0.7174, ROC_AUC=0.7259\n",
      "Trial 18 | Epoch 5: Loss=0.5955, Accuracy=0.6790, Precision=0.6477, Recall=0.7852, F1=0.7098, ROC_AUC=0.7272\n",
      "Trial 18 | Epoch 6: Loss=0.5873, Accuracy=0.6844, Precision=0.6447, Recall=0.8215, F1=0.7225, ROC_AUC=0.7315\n",
      "Trial 18 | Epoch 7: Loss=0.5839, Accuracy=0.6865, Precision=0.6522, Recall=0.7989, F1=0.7182, ROC_AUC=0.7360\n",
      "Trial 18 | Epoch 8: Loss=0.5754, Accuracy=0.6903, Precision=0.6449, Recall=0.8472, F1=0.7323, ROC_AUC=0.7391\n",
      "Trial 18 | Epoch 9: Loss=0.5651, Accuracy=0.6971, Precision=0.6466, Recall=0.8693, F1=0.7416, ROC_AUC=0.7459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:10:20,461] Trial 18 finished with value: 0.7030282501185556 and parameters: {'hidden_dim': 191, 'num_layers': 3, 'dropout': 0.26992675020129725, 'lr': 0.0005672613652569076}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 | Epoch 10: Loss=0.5532, Accuracy=0.7030, Precision=0.6569, Recall=0.8499, F1=0.7411, ROC_AUC=0.7523\n",
      "Trial 19 | Epoch 1: Loss=0.6058, Accuracy=0.6716, Precision=0.6417, Recall=0.7772, F1=0.7030, ROC_AUC=0.7169\n",
      "Trial 19 | Epoch 2: Loss=0.6011, Accuracy=0.6766, Precision=0.6328, Recall=0.8411, F1=0.7223, ROC_AUC=0.7227\n",
      "Trial 19 | Epoch 3: Loss=0.5907, Accuracy=0.6818, Precision=0.6378, Recall=0.8414, F1=0.7256, ROC_AUC=0.7299\n",
      "Trial 19 | Epoch 4: Loss=0.5792, Accuracy=0.6902, Precision=0.6492, Recall=0.8274, F1=0.7276, ROC_AUC=0.7385\n",
      "Trial 19 | Epoch 5: Loss=0.5573, Accuracy=0.7016, Precision=0.6467, Recall=0.8887, F1=0.7487, ROC_AUC=0.7490\n",
      "Trial 19 | Epoch 6: Loss=0.5418, Accuracy=0.7111, Precision=0.6584, Recall=0.8772, F1=0.7523, ROC_AUC=0.7570\n",
      "Trial 19 | Epoch 7: Loss=0.5291, Accuracy=0.7212, Precision=0.6634, Recall=0.8980, F1=0.7631, ROC_AUC=0.7640\n",
      "Trial 19 | Epoch 8: Loss=0.5153, Accuracy=0.7310, Precision=0.6663, Recall=0.9255, F1=0.7748, ROC_AUC=0.7703\n",
      "Trial 19 | Epoch 9: Loss=0.5072, Accuracy=0.7378, Precision=0.6663, Recall=0.9527, F1=0.7842, ROC_AUC=0.7739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:12:37,137] Trial 19 finished with value: 0.741426732606192 and parameters: {'hidden_dim': 176, 'num_layers': 2, 'dropout': 0.16468876743256564, 'lr': 0.002048347834930178}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 | Epoch 10: Loss=0.5005, Accuracy=0.7414, Precision=0.6732, Recall=0.9386, F1=0.7840, ROC_AUC=0.7777\n",
      "Trial 20 | Epoch 1: Loss=0.6040, Accuracy=0.6751, Precision=0.6393, Recall=0.8036, F1=0.7121, ROC_AUC=0.7197\n",
      "Trial 20 | Epoch 2: Loss=0.5990, Accuracy=0.6775, Precision=0.6483, Recall=0.7758, F1=0.7064, ROC_AUC=0.7261\n",
      "Trial 20 | Epoch 3: Loss=0.5905, Accuracy=0.6815, Precision=0.6452, Recall=0.8065, F1=0.7169, ROC_AUC=0.7295\n",
      "Trial 20 | Epoch 4: Loss=0.5846, Accuracy=0.6862, Precision=0.6361, Recall=0.8704, F1=0.7350, ROC_AUC=0.7347\n",
      "Trial 20 | Epoch 5: Loss=0.5604, Accuracy=0.7012, Precision=0.6501, Recall=0.8715, F1=0.7447, ROC_AUC=0.7475\n",
      "Trial 20 | Epoch 6: Loss=0.5417, Accuracy=0.7116, Precision=0.6580, Recall=0.8814, F1=0.7535, ROC_AUC=0.7578\n",
      "Trial 20 | Epoch 7: Loss=0.5265, Accuracy=0.7227, Precision=0.6581, Recall=0.9271, F1=0.7697, ROC_AUC=0.7643\n",
      "Trial 20 | Epoch 8: Loss=0.5184, Accuracy=0.7265, Precision=0.6700, Recall=0.8925, F1=0.7654, ROC_AUC=0.7713\n",
      "Trial 20 | Epoch 9: Loss=0.5097, Accuracy=0.7394, Precision=0.6701, Recall=0.9432, F1=0.7835, ROC_AUC=0.7772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:14:45,184] Trial 20 finished with value: 0.7410880021678748 and parameters: {'hidden_dim': 155, 'num_layers': 2, 'dropout': 0.11658698775851392, 'lr': 0.0019914156492755824}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 | Epoch 10: Loss=0.5023, Accuracy=0.7411, Precision=0.6731, Recall=0.9373, F1=0.7836, ROC_AUC=0.7789\n",
      "Trial 21 | Epoch 1: Loss=0.6073, Accuracy=0.6708, Precision=0.6384, Recall=0.7880, F1=0.7053, ROC_AUC=0.7151\n",
      "Trial 21 | Epoch 2: Loss=0.5983, Accuracy=0.6779, Precision=0.6386, Recall=0.8196, F1=0.7179, ROC_AUC=0.7238\n",
      "Trial 21 | Epoch 3: Loss=0.5883, Accuracy=0.6840, Precision=0.6428, Recall=0.8282, F1=0.7238, ROC_AUC=0.7296\n",
      "Trial 21 | Epoch 4: Loss=0.5737, Accuracy=0.6942, Precision=0.6522, Recall=0.8323, F1=0.7313, ROC_AUC=0.7403\n",
      "Trial 21 | Epoch 5: Loss=0.5572, Accuracy=0.7012, Precision=0.6541, Recall=0.8540, F1=0.7408, ROC_AUC=0.7483\n",
      "Trial 21 | Epoch 6: Loss=0.5404, Accuracy=0.7142, Precision=0.6596, Recall=0.8852, F1=0.7560, ROC_AUC=0.7548\n",
      "Trial 21 | Epoch 7: Loss=0.5245, Accuracy=0.7223, Precision=0.6631, Recall=0.9039, F1=0.7650, ROC_AUC=0.7645\n",
      "Trial 21 | Epoch 8: Loss=0.5147, Accuracy=0.7308, Precision=0.6659, Recall=0.9265, F1=0.7748, ROC_AUC=0.7677\n",
      "Trial 21 | Epoch 9: Loss=0.5055, Accuracy=0.7376, Precision=0.6740, Recall=0.9204, F1=0.7782, ROC_AUC=0.7760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:16:46,584] Trial 21 finished with value: 0.7404647381613711 and parameters: {'hidden_dim': 153, 'num_layers': 2, 'dropout': 0.1044470360769878, 'lr': 0.0021166671488475216}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 | Epoch 10: Loss=0.5026, Accuracy=0.7405, Precision=0.6768, Recall=0.9205, F1=0.7801, ROC_AUC=0.7772\n",
      "Trial 22 | Epoch 1: Loss=0.6045, Accuracy=0.6734, Precision=0.6337, Recall=0.8218, F1=0.7156, ROC_AUC=0.7182\n",
      "Trial 22 | Epoch 2: Loss=0.5939, Accuracy=0.6790, Precision=0.6495, Recall=0.7775, F1=0.7078, ROC_AUC=0.7270\n",
      "Trial 22 | Epoch 3: Loss=0.5804, Accuracy=0.6892, Precision=0.6490, Recall=0.8240, F1=0.7261, ROC_AUC=0.7352\n",
      "Trial 22 | Epoch 4: Loss=0.5609, Accuracy=0.6998, Precision=0.6506, Recall=0.8633, F1=0.7420, ROC_AUC=0.7451\n",
      "Trial 22 | Epoch 5: Loss=0.5489, Accuracy=0.7090, Precision=0.6566, Recall=0.8760, F1=0.7506, ROC_AUC=0.7520\n",
      "Trial 22 | Epoch 6: Loss=0.5355, Accuracy=0.7173, Precision=0.6538, Recall=0.9240, F1=0.7658, ROC_AUC=0.7588\n",
      "Trial 22 | Epoch 7: Loss=0.5280, Accuracy=0.7217, Precision=0.6612, Recall=0.9096, F1=0.7657, ROC_AUC=0.7626\n",
      "Trial 22 | Epoch 8: Loss=0.5234, Accuracy=0.7263, Precision=0.6645, Recall=0.9138, F1=0.7695, ROC_AUC=0.7641\n",
      "Trial 22 | Epoch 9: Loss=0.5183, Accuracy=0.7307, Precision=0.6633, Recall=0.9372, F1=0.7768, ROC_AUC=0.7676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:18:53,929] Trial 22 finished with value: 0.7344759840119233 and parameters: {'hidden_dim': 172, 'num_layers': 2, 'dropout': 0.16781991836015192, 'lr': 0.004700082900217895}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 | Epoch 10: Loss=0.5138, Accuracy=0.7345, Precision=0.6668, Recall=0.9373, F1=0.7792, ROC_AUC=0.7698\n",
      "Trial 23 | Epoch 1: Loss=0.6098, Accuracy=0.6690, Precision=0.6331, Recall=0.8039, F1=0.7083, ROC_AUC=0.7142\n",
      "Trial 23 | Epoch 2: Loss=0.5978, Accuracy=0.6781, Precision=0.6422, Recall=0.8045, F1=0.7143, ROC_AUC=0.7244\n",
      "Trial 23 | Epoch 3: Loss=0.5894, Accuracy=0.6834, Precision=0.6413, Recall=0.8323, F1=0.7244, ROC_AUC=0.7303\n",
      "Trial 23 | Epoch 4: Loss=0.5743, Accuracy=0.6925, Precision=0.6476, Recall=0.8447, F1=0.7331, ROC_AUC=0.7395\n",
      "Trial 23 | Epoch 5: Loss=0.5603, Accuracy=0.6996, Precision=0.6471, Recall=0.8780, F1=0.7451, ROC_AUC=0.7475\n",
      "Trial 23 | Epoch 6: Loss=0.5441, Accuracy=0.7086, Precision=0.6559, Recall=0.8777, F1=0.7508, ROC_AUC=0.7557\n",
      "Trial 23 | Epoch 7: Loss=0.5256, Accuracy=0.7221, Precision=0.6575, Recall=0.9273, F1=0.7694, ROC_AUC=0.7647\n",
      "Trial 23 | Epoch 8: Loss=0.5195, Accuracy=0.7252, Precision=0.6644, Recall=0.9100, F1=0.7680, ROC_AUC=0.7688\n",
      "Trial 23 | Epoch 9: Loss=0.5084, Accuracy=0.7346, Precision=0.6661, Recall=0.9405, F1=0.7799, ROC_AUC=0.7747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:20:55,175] Trial 23 finished with value: 0.7399634171126618 and parameters: {'hidden_dim': 150, 'num_layers': 2, 'dropout': 0.16522251857927503, 'lr': 0.0020410922310099555}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 | Epoch 10: Loss=0.5015, Accuracy=0.7400, Precision=0.6706, Recall=0.9431, F1=0.7839, ROC_AUC=0.7777\n",
      "Trial 24 | Epoch 1: Loss=0.6106, Accuracy=0.6664, Precision=0.6389, Recall=0.7653, F1=0.6964, ROC_AUC=0.7134\n",
      "Trial 24 | Epoch 2: Loss=0.6030, Accuracy=0.6754, Precision=0.6348, Recall=0.8258, F1=0.7178, ROC_AUC=0.7204\n",
      "Trial 24 | Epoch 3: Loss=0.5990, Accuracy=0.6778, Precision=0.6493, Recall=0.7734, F1=0.7059, ROC_AUC=0.7243\n",
      "Trial 24 | Epoch 4: Loss=0.5979, Accuracy=0.6789, Precision=0.6426, Recall=0.8062, F1=0.7152, ROC_AUC=0.7235\n",
      "Trial 24 | Epoch 5: Loss=0.5952, Accuracy=0.6793, Precision=0.6473, Recall=0.7881, F1=0.7108, ROC_AUC=0.7275\n",
      "Trial 24 | Epoch 6: Loss=0.5857, Accuracy=0.6852, Precision=0.6412, Recall=0.8408, F1=0.7276, ROC_AUC=0.7330\n",
      "Trial 24 | Epoch 7: Loss=0.5762, Accuracy=0.6912, Precision=0.6471, Recall=0.8410, F1=0.7314, ROC_AUC=0.7386\n",
      "Trial 24 | Epoch 8: Loss=0.5649, Accuracy=0.6963, Precision=0.6461, Recall=0.8682, F1=0.7408, ROC_AUC=0.7438\n",
      "Trial 24 | Epoch 9: Loss=0.5544, Accuracy=0.7028, Precision=0.6485, Recall=0.8858, F1=0.7488, ROC_AUC=0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:23:48,255] Trial 24 finished with value: 0.7103990244563376 and parameters: {'hidden_dim': 178, 'num_layers': 3, 'dropout': 0.13515965298954757, 'lr': 0.0006378596119016921}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 | Epoch 10: Loss=0.5420, Accuracy=0.7104, Precision=0.6555, Recall=0.8871, F1=0.7539, ROC_AUC=0.7552\n",
      "Trial 25 | Epoch 1: Loss=0.6091, Accuracy=0.6728, Precision=0.6488, Recall=0.7535, F1=0.6973, ROC_AUC=0.7162\n",
      "Trial 25 | Epoch 2: Loss=0.6041, Accuracy=0.6751, Precision=0.6355, Recall=0.8210, F1=0.7165, ROC_AUC=0.7195\n",
      "Trial 25 | Epoch 3: Loss=0.5943, Accuracy=0.6803, Precision=0.6477, Recall=0.7909, F1=0.7121, ROC_AUC=0.7275\n",
      "Trial 25 | Epoch 4: Loss=0.5883, Accuracy=0.6835, Precision=0.6347, Recall=0.8647, F1=0.7321, ROC_AUC=0.7320\n",
      "Trial 25 | Epoch 5: Loss=0.5718, Accuracy=0.6920, Precision=0.6499, Recall=0.8320, F1=0.7298, ROC_AUC=0.7413\n",
      "Trial 25 | Epoch 6: Loss=0.5542, Accuracy=0.7045, Precision=0.6498, Recall=0.8873, F1=0.7502, ROC_AUC=0.7488\n",
      "Trial 25 | Epoch 7: Loss=0.5412, Accuracy=0.7102, Precision=0.6525, Recall=0.8994, F1=0.7563, ROC_AUC=0.7552\n",
      "Trial 25 | Epoch 8: Loss=0.5326, Accuracy=0.7164, Precision=0.6624, Recall=0.8826, F1=0.7568, ROC_AUC=0.7617\n",
      "Trial 25 | Epoch 9: Loss=0.5201, Accuracy=0.7253, Precision=0.6638, Recall=0.9128, F1=0.7687, ROC_AUC=0.7672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:25:43,333] Trial 25 finished with value: 0.7310751304112187 and parameters: {'hidden_dim': 113, 'num_layers': 2, 'dropout': 0.11594176511358822, 'lr': 0.0018225910723245933}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 | Epoch 10: Loss=0.5125, Accuracy=0.7311, Precision=0.6650, Recall=0.9312, F1=0.7759, ROC_AUC=0.7718\n",
      "Trial 26 | Epoch 1: Loss=0.6150, Accuracy=0.6646, Precision=0.6289, Recall=0.8028, F1=0.7053, ROC_AUC=0.7099\n",
      "Trial 26 | Epoch 2: Loss=0.6086, Accuracy=0.6722, Precision=0.6476, Recall=0.7555, F1=0.6974, ROC_AUC=0.7166\n",
      "Trial 26 | Epoch 3: Loss=0.6053, Accuracy=0.6730, Precision=0.6363, Recall=0.8080, F1=0.7119, ROC_AUC=0.7185\n",
      "Trial 26 | Epoch 4: Loss=0.6012, Accuracy=0.6760, Precision=0.6444, Recall=0.7853, F1=0.7079, ROC_AUC=0.7220\n",
      "Trial 26 | Epoch 5: Loss=0.5975, Accuracy=0.6796, Precision=0.6415, Recall=0.8141, F1=0.7176, ROC_AUC=0.7251\n",
      "Trial 26 | Epoch 6: Loss=0.5939, Accuracy=0.6807, Precision=0.6447, Recall=0.8050, F1=0.7160, ROC_AUC=0.7283\n",
      "Trial 26 | Epoch 7: Loss=0.5929, Accuracy=0.6833, Precision=0.6419, Recall=0.8293, F1=0.7237, ROC_AUC=0.7283\n",
      "Trial 26 | Epoch 8: Loss=0.5880, Accuracy=0.6836, Precision=0.6408, Recall=0.8358, F1=0.7254, ROC_AUC=0.7308\n",
      "Trial 26 | Epoch 9: Loss=0.5834, Accuracy=0.6864, Precision=0.6428, Recall=0.8393, F1=0.7280, ROC_AUC=0.7353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:27:55,067] Trial 26 finished with value: 0.6880021678748053 and parameters: {'hidden_dim': 203, 'num_layers': 2, 'dropout': 0.18224995067499633, 'lr': 0.00029037115097777606}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 | Epoch 10: Loss=0.5766, Accuracy=0.6880, Precision=0.6490, Recall=0.8191, F1=0.7242, ROC_AUC=0.7394\n",
      "Trial 27 | Epoch 1: Loss=0.6072, Accuracy=0.6737, Precision=0.6297, Recall=0.8430, F1=0.7209, ROC_AUC=0.7181\n",
      "Trial 27 | Epoch 2: Loss=0.5955, Accuracy=0.6771, Precision=0.6451, Recall=0.7875, F1=0.7092, ROC_AUC=0.7257\n",
      "Trial 27 | Epoch 3: Loss=0.5888, Accuracy=0.6875, Precision=0.6432, Recall=0.8423, F1=0.7294, ROC_AUC=0.7336\n",
      "Trial 27 | Epoch 4: Loss=0.5704, Accuracy=0.6939, Precision=0.6421, Recall=0.8759, F1=0.7410, ROC_AUC=0.7403\n",
      "Trial 27 | Epoch 5: Loss=0.5605, Accuracy=0.7014, Precision=0.6450, Recall=0.8960, F1=0.7501, ROC_AUC=0.7460\n",
      "Trial 27 | Epoch 6: Loss=0.5491, Accuracy=0.7075, Precision=0.6498, Recall=0.9002, F1=0.7548, ROC_AUC=0.7513\n",
      "Trial 27 | Epoch 7: Loss=0.5452, Accuracy=0.7109, Precision=0.6489, Recall=0.9192, F1=0.7608, ROC_AUC=0.7550\n",
      "Trial 27 | Epoch 8: Loss=0.5381, Accuracy=0.7141, Precision=0.6507, Recall=0.9245, F1=0.7638, ROC_AUC=0.7569\n",
      "Trial 27 | Epoch 9: Loss=0.5350, Accuracy=0.7173, Precision=0.6567, Recall=0.9107, F1=0.7631, ROC_AUC=0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:30:30,868] Trial 27 finished with value: 0.7237856513786329 and parameters: {'hidden_dim': 151, 'num_layers': 3, 'dropout': 0.1450177805860468, 'lr': 0.005519627152737912}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 | Epoch 10: Loss=0.5289, Accuracy=0.7238, Precision=0.6620, Recall=0.9143, F1=0.7680, ROC_AUC=0.7618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.101357797250636 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 | Epoch 1: Loss=0.6077, Accuracy=0.6714, Precision=0.6357, Recall=0.8031, F1=0.7097, ROC_AUC=0.7157\n",
      "Trial 28 | Epoch 2: Loss=0.6019, Accuracy=0.6765, Precision=0.6436, Recall=0.7910, F1=0.7097, ROC_AUC=0.7227\n",
      "Trial 28 | Epoch 3: Loss=0.5967, Accuracy=0.6797, Precision=0.6436, Recall=0.8051, F1=0.7154, ROC_AUC=0.7253\n",
      "Trial 28 | Epoch 4: Loss=0.5886, Accuracy=0.6849, Precision=0.6413, Recall=0.8392, F1=0.7270, ROC_AUC=0.7309\n",
      "Trial 28 | Epoch 5: Loss=0.5787, Accuracy=0.6881, Precision=0.6538, Recall=0.7994, F1=0.7193, ROC_AUC=0.7389\n",
      "Trial 28 | Epoch 6: Loss=0.5677, Accuracy=0.6963, Precision=0.6522, Recall=0.8411, F1=0.7347, ROC_AUC=0.7429\n",
      "Trial 28 | Epoch 7: Loss=0.5544, Accuracy=0.7043, Precision=0.6543, Recall=0.8663, F1=0.7455, ROC_AUC=0.7511\n",
      "Trial 28 | Epoch 8: Loss=0.5413, Accuracy=0.7104, Precision=0.6604, Recall=0.8659, F1=0.7493, ROC_AUC=0.7569\n",
      "Trial 28 | Epoch 9: Loss=0.5307, Accuracy=0.7170, Precision=0.6636, Recall=0.8801, F1=0.7567, ROC_AUC=0.7625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:31:54,432] Trial 28 finished with value: 0.7239753404240905 and parameters: {'hidden_dim': 181, 'num_layers': 1, 'dropout': 0.101357797250636, 'lr': 0.0007903026544812819}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 | Epoch 10: Loss=0.5232, Accuracy=0.7240, Precision=0.6682, Recall=0.8900, F1=0.7633, ROC_AUC=0.7699\n",
      "Trial 29 | Epoch 1: Loss=0.6128, Accuracy=0.6666, Precision=0.6204, Recall=0.8587, F1=0.7203, ROC_AUC=0.7137\n",
      "Trial 29 | Epoch 2: Loss=0.6041, Accuracy=0.6720, Precision=0.6332, Recall=0.8173, F1=0.7136, ROC_AUC=0.7177\n",
      "Trial 29 | Epoch 3: Loss=0.5940, Accuracy=0.6798, Precision=0.6462, Recall=0.7948, F1=0.7128, ROC_AUC=0.7274\n",
      "Trial 29 | Epoch 4: Loss=0.5877, Accuracy=0.6828, Precision=0.6455, Recall=0.8107, F1=0.7188, ROC_AUC=0.7312\n",
      "Trial 29 | Epoch 5: Loss=0.5774, Accuracy=0.6903, Precision=0.6406, Recall=0.8668, F1=0.7367, ROC_AUC=0.7376\n",
      "Trial 29 | Epoch 6: Loss=0.5628, Accuracy=0.7000, Precision=0.6463, Recall=0.8837, F1=0.7466, ROC_AUC=0.7447\n",
      "Trial 29 | Epoch 7: Loss=0.5443, Accuracy=0.7108, Precision=0.6518, Recall=0.9051, F1=0.7578, ROC_AUC=0.7529\n",
      "Trial 29 | Epoch 8: Loss=0.5317, Accuracy=0.7201, Precision=0.6568, Recall=0.9217, F1=0.7670, ROC_AUC=0.7603\n",
      "Trial 29 | Epoch 9: Loss=0.5218, Accuracy=0.7257, Precision=0.6592, Recall=0.9345, F1=0.7731, ROC_AUC=0.7642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:35:40,107] Trial 29 finished with value: 0.7368335478626109 and parameters: {'hidden_dim': 232, 'num_layers': 3, 'dropout': 0.37910627686467246, 'lr': 0.0012782549551187935}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29 | Epoch 10: Loss=0.5086, Accuracy=0.7368, Precision=0.6661, Recall=0.9497, F1=0.7830, ROC_AUC=0.7702\n",
      "Trial 30 | Epoch 1: Loss=0.6078, Accuracy=0.6696, Precision=0.6357, Recall=0.7942, F1=0.7062, ROC_AUC=0.7111\n",
      "Trial 30 | Epoch 2: Loss=0.5938, Accuracy=0.6795, Precision=0.6402, Recall=0.8198, F1=0.7189, ROC_AUC=0.7269\n",
      "Trial 30 | Epoch 3: Loss=0.5816, Accuracy=0.6866, Precision=0.6427, Recall=0.8407, F1=0.7285, ROC_AUC=0.7363\n",
      "Trial 30 | Epoch 4: Loss=0.5658, Accuracy=0.6959, Precision=0.6472, Recall=0.8616, F1=0.7391, ROC_AUC=0.7437\n",
      "Trial 30 | Epoch 5: Loss=0.5497, Accuracy=0.7045, Precision=0.6492, Recall=0.8900, F1=0.7507, ROC_AUC=0.7514\n",
      "Trial 30 | Epoch 6: Loss=0.5351, Accuracy=0.7152, Precision=0.6525, Recall=0.9208, F1=0.7638, ROC_AUC=0.7590\n",
      "Trial 30 | Epoch 7: Loss=0.5284, Accuracy=0.7254, Precision=0.6603, Recall=0.9288, F1=0.7718, ROC_AUC=0.7645\n",
      "Trial 30 | Epoch 8: Loss=0.5174, Accuracy=0.7288, Precision=0.6636, Recall=0.9281, F1=0.7739, ROC_AUC=0.7673\n",
      "Trial 30 | Epoch 9: Loss=0.5098, Accuracy=0.7354, Precision=0.6693, Recall=0.9304, F1=0.7786, ROC_AUC=0.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:37:54,503] Trial 30 finished with value: 0.7384052570964027 and parameters: {'hidden_dim': 208, 'num_layers': 2, 'dropout': 0.22582142037651493, 'lr': 0.0037393834191426878}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 | Epoch 10: Loss=0.5071, Accuracy=0.7384, Precision=0.6711, Recall=0.9349, F1=0.7814, ROC_AUC=0.7763\n",
      "Trial 31 | Epoch 1: Loss=0.6129, Accuracy=0.6708, Precision=0.6333, Recall=0.8116, F1=0.7114, ROC_AUC=0.7134\n",
      "Trial 31 | Epoch 2: Loss=0.6004, Accuracy=0.6751, Precision=0.6459, Recall=0.7750, F1=0.7046, ROC_AUC=0.7223\n",
      "Trial 31 | Epoch 3: Loss=0.5922, Accuracy=0.6807, Precision=0.6448, Recall=0.8046, F1=0.7159, ROC_AUC=0.7282\n",
      "Trial 31 | Epoch 4: Loss=0.5830, Accuracy=0.6854, Precision=0.6410, Recall=0.8430, F1=0.7282, ROC_AUC=0.7330\n",
      "Trial 31 | Epoch 5: Loss=0.5669, Accuracy=0.6954, Precision=0.6469, Recall=0.8604, F1=0.7385, ROC_AUC=0.7422\n",
      "Trial 31 | Epoch 6: Loss=0.5508, Accuracy=0.7058, Precision=0.6464, Recall=0.9089, F1=0.7555, ROC_AUC=0.7503\n",
      "Trial 31 | Epoch 7: Loss=0.5344, Accuracy=0.7152, Precision=0.6540, Recall=0.9139, F1=0.7624, ROC_AUC=0.7594\n",
      "Trial 31 | Epoch 8: Loss=0.5200, Accuracy=0.7267, Precision=0.6620, Recall=0.9262, F1=0.7721, ROC_AUC=0.7678\n",
      "Trial 31 | Epoch 9: Loss=0.5114, Accuracy=0.7328, Precision=0.6673, Recall=0.9288, F1=0.7766, ROC_AUC=0.7729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:39:55,966] Trial 31 finished with value: 0.7382968633561412 and parameters: {'hidden_dim': 168, 'num_layers': 2, 'dropout': 0.26973757251751707, 'lr': 0.0015951677450086823}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31 | Epoch 10: Loss=0.5051, Accuracy=0.7383, Precision=0.6728, Recall=0.9278, F1=0.7800, ROC_AUC=0.7762\n",
      "Trial 32 | Epoch 1: Loss=0.6071, Accuracy=0.6704, Precision=0.6480, Recall=0.7462, F1=0.6937, ROC_AUC=0.7197\n",
      "Trial 32 | Epoch 2: Loss=0.5973, Accuracy=0.6784, Precision=0.6385, Recall=0.8221, F1=0.7188, ROC_AUC=0.7254\n",
      "Trial 32 | Epoch 3: Loss=0.5923, Accuracy=0.6832, Precision=0.6390, Recall=0.8422, F1=0.7267, ROC_AUC=0.7283\n",
      "Trial 32 | Epoch 4: Loss=0.5831, Accuracy=0.6867, Precision=0.6522, Recall=0.7999, F1=0.7185, ROC_AUC=0.7355\n",
      "Trial 32 | Epoch 5: Loss=0.5668, Accuracy=0.6950, Precision=0.6465, Recall=0.8607, F1=0.7384, ROC_AUC=0.7414\n",
      "Trial 32 | Epoch 6: Loss=0.5488, Accuracy=0.7069, Precision=0.6494, Recall=0.8996, F1=0.7543, ROC_AUC=0.7525\n",
      "Trial 32 | Epoch 7: Loss=0.5377, Accuracy=0.7149, Precision=0.6582, Recall=0.8940, F1=0.7582, ROC_AUC=0.7573\n",
      "Trial 32 | Epoch 8: Loss=0.5237, Accuracy=0.7249, Precision=0.6600, Recall=0.9279, F1=0.7713, ROC_AUC=0.7680\n",
      "Trial 32 | Epoch 9: Loss=0.5131, Accuracy=0.7335, Precision=0.6642, Recall=0.9444, F1=0.7799, ROC_AUC=0.7723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:42:03,839] Trial 32 finished with value: 0.7389743242327755 and parameters: {'hidden_dim': 184, 'num_layers': 2, 'dropout': 0.26142763456837403, 'lr': 0.0015569321738001572}. Best is trial 14 with value: 0.7415486755639862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 | Epoch 10: Loss=0.5055, Accuracy=0.7390, Precision=0.6684, Recall=0.9485, F1=0.7842, ROC_AUC=0.7754\n",
      "Trial 33 | Epoch 1: Loss=0.6084, Accuracy=0.6692, Precision=0.6409, Recall=0.7697, F1=0.6994, ROC_AUC=0.7147\n",
      "Trial 33 | Epoch 2: Loss=0.5992, Accuracy=0.6756, Precision=0.6500, Recall=0.7610, F1=0.7011, ROC_AUC=0.7249\n",
      "Trial 33 | Epoch 3: Loss=0.5884, Accuracy=0.6843, Precision=0.6464, Recall=0.8136, F1=0.7204, ROC_AUC=0.7314\n",
      "Trial 33 | Epoch 4: Loss=0.5745, Accuracy=0.6928, Precision=0.6490, Recall=0.8395, F1=0.7321, ROC_AUC=0.7401\n",
      "Trial 33 | Epoch 5: Loss=0.5582, Accuracy=0.7028, Precision=0.6466, Recall=0.8945, F1=0.7506, ROC_AUC=0.7488\n",
      "Trial 33 | Epoch 6: Loss=0.5406, Accuracy=0.7131, Precision=0.6565, Recall=0.8942, F1=0.7571, ROC_AUC=0.7567\n",
      "Trial 33 | Epoch 7: Loss=0.5281, Accuracy=0.7230, Precision=0.6615, Recall=0.9133, F1=0.7673, ROC_AUC=0.7633\n",
      "Trial 33 | Epoch 8: Loss=0.5187, Accuracy=0.7286, Precision=0.6625, Recall=0.9320, F1=0.7745, ROC_AUC=0.7668\n",
      "Trial 33 | Epoch 9: Loss=0.5094, Accuracy=0.7368, Precision=0.6659, Recall=0.9506, F1=0.7832, ROC_AUC=0.7723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:44:16,851] Trial 33 finished with value: 0.7437978456744123 and parameters: {'hidden_dim': 200, 'num_layers': 2, 'dropout': 0.44182738182648656, 'lr': 0.002374063894519534}. Best is trial 33 with value: 0.7437978456744123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 | Epoch 10: Loss=0.4988, Accuracy=0.7438, Precision=0.6731, Recall=0.9479, F1=0.7872, ROC_AUC=0.7765\n",
      "Trial 34 | Epoch 1: Loss=0.6093, Accuracy=0.6675, Precision=0.6470, Recall=0.7371, F1=0.6891, ROC_AUC=0.7148\n",
      "Trial 34 | Epoch 2: Loss=0.5945, Accuracy=0.6801, Precision=0.6407, Recall=0.8200, F1=0.7194, ROC_AUC=0.7265\n",
      "Trial 34 | Epoch 3: Loss=0.5890, Accuracy=0.6841, Precision=0.6359, Recall=0.8617, F1=0.7317, ROC_AUC=0.7314\n",
      "Trial 34 | Epoch 4: Loss=0.5738, Accuracy=0.6926, Precision=0.6531, Recall=0.8216, F1=0.7277, ROC_AUC=0.7411\n",
      "Trial 34 | Epoch 5: Loss=0.5569, Accuracy=0.7019, Precision=0.6486, Recall=0.8812, F1=0.7472, ROC_AUC=0.7500\n",
      "Trial 34 | Epoch 6: Loss=0.5394, Accuracy=0.7144, Precision=0.6529, Recall=0.9156, F1=0.7622, ROC_AUC=0.7576\n",
      "Trial 34 | Epoch 7: Loss=0.5286, Accuracy=0.7201, Precision=0.6643, Recall=0.8900, F1=0.7607, ROC_AUC=0.7632\n",
      "Trial 34 | Epoch 8: Loss=0.5170, Accuracy=0.7302, Precision=0.6624, Recall=0.9387, F1=0.7767, ROC_AUC=0.7690\n",
      "Trial 34 | Epoch 9: Loss=0.5049, Accuracy=0.7395, Precision=0.6677, Recall=0.9537, F1=0.7855, ROC_AUC=0.7747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:46:32,899] Trial 34 finished with value: 0.7421854887880225 and parameters: {'hidden_dim': 206, 'num_layers': 2, 'dropout': 0.4307260122023418, 'lr': 0.002415862017354899}. Best is trial 33 with value: 0.7437978456744123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34 | Epoch 10: Loss=0.5005, Accuracy=0.7422, Precision=0.6723, Recall=0.9448, F1=0.7856, ROC_AUC=0.7771\n",
      "Trial 35 | Epoch 1: Loss=0.6094, Accuracy=0.6706, Precision=0.6437, Recall=0.7645, F1=0.6989, ROC_AUC=0.7146\n",
      "Trial 35 | Epoch 2: Loss=0.6027, Accuracy=0.6758, Precision=0.6323, Recall=0.8406, F1=0.7217, ROC_AUC=0.7204\n",
      "Trial 35 | Epoch 3: Loss=0.5970, Accuracy=0.6784, Precision=0.6423, Recall=0.8048, F1=0.7145, ROC_AUC=0.7258\n",
      "Trial 35 | Epoch 4: Loss=0.5905, Accuracy=0.6824, Precision=0.6444, Recall=0.8142, F1=0.7194, ROC_AUC=0.7294\n",
      "Trial 35 | Epoch 5: Loss=0.5881, Accuracy=0.6821, Precision=0.6303, Recall=0.8806, F1=0.7347, ROC_AUC=0.7326\n",
      "Trial 35 | Epoch 6: Loss=0.5738, Accuracy=0.6914, Precision=0.6503, Recall=0.8282, F1=0.7285, ROC_AUC=0.7393\n",
      "Trial 35 | Epoch 7: Loss=0.5610, Accuracy=0.6986, Precision=0.6481, Recall=0.8690, F1=0.7425, ROC_AUC=0.7448\n",
      "Trial 35 | Epoch 8: Loss=0.5506, Accuracy=0.7053, Precision=0.6527, Recall=0.8775, F1=0.7486, ROC_AUC=0.7514\n",
      "Trial 35 | Epoch 9: Loss=0.5403, Accuracy=0.7112, Precision=0.6498, Recall=0.9161, F1=0.7603, ROC_AUC=0.7541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:49:52,487] Trial 35 finished with value: 0.7186504979337444 and parameters: {'hidden_dim': 203, 'num_layers': 3, 'dropout': 0.42943486759375066, 'lr': 0.0010331624573411923}. Best is trial 33 with value: 0.7437978456744123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35 | Epoch 10: Loss=0.5312, Accuracy=0.7187, Precision=0.6591, Recall=0.9059, F1=0.7630, ROC_AUC=0.7608\n",
      "Trial 36 | Epoch 1: Loss=0.6068, Accuracy=0.6716, Precision=0.6339, Recall=0.8120, F1=0.7120, ROC_AUC=0.7125\n",
      "Trial 36 | Epoch 2: Loss=0.5995, Accuracy=0.6773, Precision=0.6455, Recall=0.7866, F1=0.7091, ROC_AUC=0.7225\n",
      "Trial 36 | Epoch 3: Loss=0.5912, Accuracy=0.6821, Precision=0.6459, Recall=0.8063, F1=0.7172, ROC_AUC=0.7292\n",
      "Trial 36 | Epoch 4: Loss=0.5758, Accuracy=0.6912, Precision=0.6377, Recall=0.8857, F1=0.7415, ROC_AUC=0.7390\n",
      "Trial 36 | Epoch 5: Loss=0.5553, Accuracy=0.7027, Precision=0.6462, Recall=0.8960, F1=0.7508, ROC_AUC=0.7482\n",
      "Trial 36 | Epoch 6: Loss=0.5383, Accuracy=0.7142, Precision=0.6564, Recall=0.8990, F1=0.7588, ROC_AUC=0.7584\n",
      "Trial 36 | Epoch 7: Loss=0.5260, Accuracy=0.7224, Precision=0.6572, Recall=0.9300, F1=0.7701, ROC_AUC=0.7632\n",
      "Trial 36 | Epoch 8: Loss=0.5148, Accuracy=0.7302, Precision=0.6647, Recall=0.9288, F1=0.7749, ROC_AUC=0.7677\n",
      "Trial 36 | Epoch 9: Loss=0.5090, Accuracy=0.7352, Precision=0.6653, Recall=0.9465, F1=0.7814, ROC_AUC=0.7712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:52:23,574] Trial 36 finished with value: 0.7437436488042816 and parameters: {'hidden_dim': 228, 'num_layers': 2, 'dropout': 0.4707074665905118, 'lr': 0.0027992455123900944}. Best is trial 33 with value: 0.7437978456744123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36 | Epoch 10: Loss=0.4976, Accuracy=0.7437, Precision=0.6702, Recall=0.9596, F1=0.7892, ROC_AUC=0.7769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.47212177749477396 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37 | Epoch 1: Loss=0.6028, Accuracy=0.6743, Precision=0.6419, Recall=0.7882, F1=0.7076, ROC_AUC=0.7194\n",
      "Trial 37 | Epoch 2: Loss=0.5936, Accuracy=0.6819, Precision=0.6438, Recall=0.8146, F1=0.7192, ROC_AUC=0.7272\n",
      "Trial 37 | Epoch 3: Loss=0.5823, Accuracy=0.6882, Precision=0.6491, Recall=0.8190, F1=0.7242, ROC_AUC=0.7365\n",
      "Trial 37 | Epoch 4: Loss=0.5687, Accuracy=0.6986, Precision=0.6523, Recall=0.8504, F1=0.7383, ROC_AUC=0.7452\n",
      "Trial 37 | Epoch 5: Loss=0.5487, Accuracy=0.7075, Precision=0.6636, Recall=0.8419, F1=0.7422, ROC_AUC=0.7559\n",
      "Trial 37 | Epoch 6: Loss=0.5310, Accuracy=0.7230, Precision=0.6652, Recall=0.8979, F1=0.7642, ROC_AUC=0.7653\n",
      "Trial 37 | Epoch 7: Loss=0.5148, Accuracy=0.7303, Precision=0.6731, Recall=0.8954, F1=0.7685, ROC_AUC=0.7724\n",
      "Trial 37 | Epoch 8: Loss=0.5070, Accuracy=0.7366, Precision=0.6783, Recall=0.9000, F1=0.7736, ROC_AUC=0.7784\n",
      "Trial 37 | Epoch 9: Loss=0.5005, Accuracy=0.7430, Precision=0.6783, Recall=0.9246, F1=0.7825, ROC_AUC=0.7808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:53:50,700] Trial 37 finished with value: 0.7483774812004607 and parameters: {'hidden_dim': 236, 'num_layers': 1, 'dropout': 0.47212177749477396, 'lr': 0.0026502378845647662}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37 | Epoch 10: Loss=0.4973, Accuracy=0.7484, Precision=0.6807, Recall=0.9357, F1=0.7881, ROC_AUC=0.7843\n",
      "Trial 38 | Epoch 1: Loss=0.6075, Accuracy=0.6671, Precision=0.6460, Recall=0.7394, F1=0.6896, ROC_AUC=0.7158\n",
      "Trial 38 | Epoch 2: Loss=0.5976, Accuracy=0.6798, Precision=0.6397, Recall=0.8232, F1=0.7200, ROC_AUC=0.7258\n",
      "Trial 38 | Epoch 3: Loss=0.5895, Accuracy=0.6828, Precision=0.6473, Recall=0.8035, F1=0.7170, ROC_AUC=0.7304\n",
      "Trial 38 | Epoch 4: Loss=0.5748, Accuracy=0.6924, Precision=0.6423, Recall=0.8683, F1=0.7384, ROC_AUC=0.7389\n",
      "Trial 38 | Epoch 5: Loss=0.5579, Accuracy=0.7016, Precision=0.6464, Recall=0.8903, F1=0.7490, ROC_AUC=0.7481\n",
      "Trial 38 | Epoch 6: Loss=0.5410, Accuracy=0.7125, Precision=0.6547, Recall=0.8994, F1=0.7578, ROC_AUC=0.7573\n",
      "Trial 38 | Epoch 7: Loss=0.5259, Accuracy=0.7213, Precision=0.6555, Recall=0.9331, F1=0.7700, ROC_AUC=0.7639\n",
      "Trial 38 | Epoch 8: Loss=0.5149, Accuracy=0.7310, Precision=0.6622, Recall=0.9430, F1=0.7781, ROC_AUC=0.7692\n",
      "Trial 38 | Epoch 9: Loss=0.5084, Accuracy=0.7365, Precision=0.6654, Recall=0.9512, F1=0.7831, ROC_AUC=0.7711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:56:23,431] Trial 38 finished with value: 0.7413454373009959 and parameters: {'hidden_dim': 231, 'num_layers': 2, 'dropout': 0.46974860599370744, 'lr': 0.002524053493190392}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38 | Epoch 10: Loss=0.5012, Accuracy=0.7413, Precision=0.6693, Recall=0.9539, F1=0.7867, ROC_AUC=0.7761\n",
      "Trial 39 | Epoch 1: Loss=0.6039, Accuracy=0.6725, Precision=0.6383, Recall=0.7961, F1=0.7085, ROC_AUC=0.7171\n",
      "Trial 39 | Epoch 2: Loss=0.5970, Accuracy=0.6787, Precision=0.6405, Recall=0.8148, F1=0.7172, ROC_AUC=0.7244\n",
      "Trial 39 | Epoch 3: Loss=0.5850, Accuracy=0.6862, Precision=0.6451, Recall=0.8280, F1=0.7252, ROC_AUC=0.7299\n",
      "Trial 39 | Epoch 4: Loss=0.5757, Accuracy=0.6921, Precision=0.6388, Recall=0.8838, F1=0.7416, ROC_AUC=0.7386\n",
      "Trial 39 | Epoch 5: Loss=0.5685, Accuracy=0.6948, Precision=0.6505, Recall=0.8420, F1=0.7340, ROC_AUC=0.7416\n",
      "Trial 39 | Epoch 6: Loss=0.5605, Accuracy=0.6995, Precision=0.6393, Recall=0.9155, F1=0.7529, ROC_AUC=0.7460\n",
      "Trial 39 | Epoch 7: Loss=0.5548, Accuracy=0.7039, Precision=0.6418, Recall=0.9226, F1=0.7570, ROC_AUC=0.7492\n",
      "Trial 39 | Epoch 8: Loss=0.5489, Accuracy=0.7072, Precision=0.6445, Recall=0.9240, F1=0.7594, ROC_AUC=0.7504\n",
      "Trial 39 | Epoch 9: Loss=0.5474, Accuracy=0.7099, Precision=0.6473, Recall=0.9224, F1=0.7607, ROC_AUC=0.7512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 22:59:06,153] Trial 39 finished with value: 0.7098028588848994 and parameters: {'hidden_dim': 254, 'num_layers': 2, 'dropout': 0.4131285911312168, 'lr': 0.006703523154585255}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 39 | Epoch 10: Loss=0.5435, Accuracy=0.7098, Precision=0.6509, Recall=0.9051, F1=0.7572, ROC_AUC=0.7530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.482675596399165 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 | Epoch 1: Loss=0.6032, Accuracy=0.6753, Precision=0.6409, Recall=0.7971, F1=0.7105, ROC_AUC=0.7205\n",
      "Trial 40 | Epoch 2: Loss=0.5960, Accuracy=0.6775, Precision=0.6391, Recall=0.8154, F1=0.7166, ROC_AUC=0.7244\n",
      "Trial 40 | Epoch 3: Loss=0.5776, Accuracy=0.6884, Precision=0.6495, Recall=0.8184, F1=0.7242, ROC_AUC=0.7375\n",
      "Trial 40 | Epoch 4: Loss=0.5559, Accuracy=0.7051, Precision=0.6518, Recall=0.8804, F1=0.7491, ROC_AUC=0.7509\n",
      "Trial 40 | Epoch 5: Loss=0.5365, Accuracy=0.7136, Precision=0.6678, Recall=0.8499, F1=0.7480, ROC_AUC=0.7608\n",
      "Trial 40 | Epoch 6: Loss=0.5216, Accuracy=0.7294, Precision=0.6679, Recall=0.9126, F1=0.7713, ROC_AUC=0.7702\n",
      "Trial 40 | Epoch 7: Loss=0.5108, Accuracy=0.7383, Precision=0.6729, Recall=0.9273, F1=0.7799, ROC_AUC=0.7757\n",
      "Trial 40 | Epoch 8: Loss=0.5031, Accuracy=0.7440, Precision=0.6798, Recall=0.9227, F1=0.7828, ROC_AUC=0.7812\n",
      "Trial 40 | Epoch 9: Loss=0.5013, Accuracy=0.7446, Precision=0.6838, Recall=0.9100, F1=0.7808, ROC_AUC=0.7838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:00:35,035] Trial 40 finished with value: 0.7467922227491363 and parameters: {'hidden_dim': 238, 'num_layers': 1, 'dropout': 0.482675596399165, 'lr': 0.0034637545939868483}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 | Epoch 10: Loss=0.4954, Accuracy=0.7468, Precision=0.6868, Recall=0.9075, F1=0.7819, ROC_AUC=0.7854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4746672474488072 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41 | Epoch 1: Loss=0.6095, Accuracy=0.6728, Precision=0.6286, Recall=0.8445, F1=0.7208, ROC_AUC=0.7184\n",
      "Trial 41 | Epoch 2: Loss=0.5961, Accuracy=0.6799, Precision=0.6425, Recall=0.8111, F1=0.7170, ROC_AUC=0.7256\n",
      "Trial 41 | Epoch 3: Loss=0.5791, Accuracy=0.6902, Precision=0.6454, Recall=0.8443, F1=0.7316, ROC_AUC=0.7353\n",
      "Trial 41 | Epoch 4: Loss=0.5595, Accuracy=0.7005, Precision=0.6559, Recall=0.8437, F1=0.7380, ROC_AUC=0.7477\n",
      "Trial 41 | Epoch 5: Loss=0.5436, Accuracy=0.7102, Precision=0.6628, Recall=0.8558, F1=0.7471, ROC_AUC=0.7560\n",
      "Trial 41 | Epoch 6: Loss=0.5237, Accuracy=0.7251, Precision=0.6674, Recall=0.8975, F1=0.7655, ROC_AUC=0.7664\n",
      "Trial 41 | Epoch 7: Loss=0.5238, Accuracy=0.7308, Precision=0.6719, Recall=0.9019, F1=0.7701, ROC_AUC=0.7706\n",
      "Trial 41 | Epoch 8: Loss=0.5048, Accuracy=0.7380, Precision=0.6770, Recall=0.9102, F1=0.7765, ROC_AUC=0.7782\n",
      "Trial 41 | Epoch 9: Loss=0.5079, Accuracy=0.7372, Precision=0.6809, Recall=0.8931, F1=0.7727, ROC_AUC=0.7782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:02:02,715] Trial 41 finished with value: 0.7432558769731048 and parameters: {'hidden_dim': 239, 'num_layers': 1, 'dropout': 0.4746672474488072, 'lr': 0.0035881423752221874}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41 | Epoch 10: Loss=0.5065, Accuracy=0.7433, Precision=0.6792, Recall=0.9219, F1=0.7822, ROC_AUC=0.7797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4703892730050988 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42 | Epoch 1: Loss=0.6040, Accuracy=0.6744, Precision=0.6452, Recall=0.7746, F1=0.7040, ROC_AUC=0.7192\n",
      "Trial 42 | Epoch 2: Loss=0.5923, Accuracy=0.6817, Precision=0.6434, Recall=0.8152, F1=0.7192, ROC_AUC=0.7280\n",
      "Trial 42 | Epoch 3: Loss=0.5802, Accuracy=0.6861, Precision=0.6460, Recall=0.8232, F1=0.7239, ROC_AUC=0.7354\n",
      "Trial 42 | Epoch 4: Loss=0.5587, Accuracy=0.7018, Precision=0.6559, Recall=0.8488, F1=0.7400, ROC_AUC=0.7486\n",
      "Trial 42 | Epoch 5: Loss=0.5395, Accuracy=0.7135, Precision=0.6573, Recall=0.8920, F1=0.7569, ROC_AUC=0.7571\n",
      "Trial 42 | Epoch 6: Loss=0.5262, Accuracy=0.7221, Precision=0.6637, Recall=0.9004, F1=0.7641, ROC_AUC=0.7645\n",
      "Trial 42 | Epoch 7: Loss=0.5170, Accuracy=0.7273, Precision=0.6730, Recall=0.8845, F1=0.7644, ROC_AUC=0.7693\n",
      "Trial 42 | Epoch 8: Loss=0.5108, Accuracy=0.7349, Precision=0.6721, Recall=0.9173, F1=0.7758, ROC_AUC=0.7740\n",
      "Trial 42 | Epoch 9: Loss=0.5010, Accuracy=0.7436, Precision=0.6760, Recall=0.9357, F1=0.7849, ROC_AUC=0.7776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:03:28,542] Trial 42 finished with value: 0.7438926901971411 and parameters: {'hidden_dim': 240, 'num_layers': 1, 'dropout': 0.4703892730050988, 'lr': 0.003628483193799635}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42 | Epoch 10: Loss=0.4987, Accuracy=0.7439, Precision=0.6812, Recall=0.9168, F1=0.7816, ROC_AUC=0.7805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4824207790030034 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43 | Epoch 1: Loss=0.6045, Accuracy=0.6735, Precision=0.6445, Recall=0.7741, F1=0.7033, ROC_AUC=0.7196\n",
      "Trial 43 | Epoch 2: Loss=0.5943, Accuracy=0.6811, Precision=0.6385, Recall=0.8348, F1=0.7235, ROC_AUC=0.7291\n",
      "Trial 43 | Epoch 3: Loss=0.5763, Accuracy=0.6920, Precision=0.6470, Recall=0.8450, F1=0.7329, ROC_AUC=0.7390\n",
      "Trial 43 | Epoch 4: Loss=0.5596, Accuracy=0.7019, Precision=0.6494, Recall=0.8775, F1=0.7464, ROC_AUC=0.7482\n",
      "Trial 43 | Epoch 5: Loss=0.5382, Accuracy=0.7151, Precision=0.6571, Recall=0.8997, F1=0.7595, ROC_AUC=0.7576\n",
      "Trial 43 | Epoch 6: Loss=0.5251, Accuracy=0.7245, Precision=0.6609, Recall=0.9218, F1=0.7699, ROC_AUC=0.7661\n",
      "Trial 43 | Epoch 7: Loss=0.5160, Accuracy=0.7278, Precision=0.6729, Recall=0.8866, F1=0.7651, ROC_AUC=0.7716\n",
      "Trial 43 | Epoch 8: Loss=0.5096, Accuracy=0.7366, Precision=0.6725, Recall=0.9224, F1=0.7779, ROC_AUC=0.7760\n",
      "Trial 43 | Epoch 9: Loss=0.5036, Accuracy=0.7424, Precision=0.6777, Recall=0.9244, F1=0.7820, ROC_AUC=0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:04:55,723] Trial 43 finished with value: 0.7428358512295915 and parameters: {'hidden_dim': 222, 'num_layers': 1, 'dropout': 0.4824207790030034, 'lr': 0.0030247446848509175}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43 | Epoch 10: Loss=0.5018, Accuracy=0.7428, Precision=0.6797, Recall=0.9184, F1=0.7812, ROC_AUC=0.7820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.45797432016030937 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44 | Epoch 1: Loss=0.6067, Accuracy=0.6722, Precision=0.6491, Recall=0.7498, F1=0.6958, ROC_AUC=0.7184\n",
      "Trial 44 | Epoch 2: Loss=0.5929, Accuracy=0.6811, Precision=0.6418, Recall=0.8195, F1=0.7199, ROC_AUC=0.7264\n",
      "Trial 44 | Epoch 3: Loss=0.5792, Accuracy=0.6902, Precision=0.6453, Recall=0.8445, F1=0.7316, ROC_AUC=0.7352\n",
      "Trial 44 | Epoch 4: Loss=0.5603, Accuracy=0.7008, Precision=0.6489, Recall=0.8751, F1=0.7452, ROC_AUC=0.7446\n",
      "Trial 44 | Epoch 5: Loss=0.5413, Accuracy=0.7157, Precision=0.6600, Recall=0.8896, F1=0.7578, ROC_AUC=0.7568\n",
      "Trial 44 | Epoch 6: Loss=0.5321, Accuracy=0.7178, Precision=0.6699, Recall=0.8589, F1=0.7527, ROC_AUC=0.7620\n",
      "Trial 44 | Epoch 7: Loss=0.5227, Accuracy=0.7259, Precision=0.6710, Recall=0.8868, F1=0.7639, ROC_AUC=0.7657\n",
      "Trial 44 | Epoch 8: Loss=0.5163, Accuracy=0.7339, Precision=0.6728, Recall=0.9104, F1=0.7738, ROC_AUC=0.7714\n",
      "Trial 44 | Epoch 9: Loss=0.5104, Accuracy=0.7407, Precision=0.6737, Recall=0.9335, F1=0.7826, ROC_AUC=0.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:06:23,442] Trial 44 finished with value: 0.7412505927782671 and parameters: {'hidden_dim': 243, 'num_layers': 1, 'dropout': 0.45797432016030937, 'lr': 0.004225449612594557}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44 | Epoch 10: Loss=0.5048, Accuracy=0.7413, Precision=0.6809, Recall=0.9080, F1=0.7782, ROC_AUC=0.7805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.49988642143818945 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45 | Epoch 1: Loss=0.6052, Accuracy=0.6739, Precision=0.6410, Recall=0.7904, F1=0.7079, ROC_AUC=0.7174\n",
      "Trial 45 | Epoch 2: Loss=0.5968, Accuracy=0.6796, Precision=0.6314, Recall=0.8633, F1=0.7293, ROC_AUC=0.7224\n",
      "Trial 45 | Epoch 3: Loss=0.5737, Accuracy=0.6926, Precision=0.6447, Recall=0.8578, F1=0.7361, ROC_AUC=0.7384\n",
      "Trial 45 | Epoch 4: Loss=0.5608, Accuracy=0.7014, Precision=0.6499, Recall=0.8735, F1=0.7453, ROC_AUC=0.7447\n",
      "Trial 45 | Epoch 5: Loss=0.5526, Accuracy=0.7054, Precision=0.6547, Recall=0.8693, F1=0.7469, ROC_AUC=0.7495\n",
      "Trial 45 | Epoch 6: Loss=0.5476, Accuracy=0.7071, Precision=0.6569, Recall=0.8668, F1=0.7474, ROC_AUC=0.7526\n",
      "Trial 45 | Epoch 7: Loss=0.5392, Accuracy=0.7123, Precision=0.6577, Recall=0.8857, F1=0.7548, ROC_AUC=0.7549\n",
      "Trial 45 | Epoch 8: Loss=0.5397, Accuracy=0.7123, Precision=0.6550, Recall=0.8972, F1=0.7572, ROC_AUC=0.7517\n",
      "Trial 45 | Epoch 9: Loss=0.5353, Accuracy=0.7183, Precision=0.6579, Recall=0.9092, F1=0.7634, ROC_AUC=0.7568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:07:48,537] Trial 45 finished with value: 0.7241650294695481 and parameters: {'hidden_dim': 216, 'num_layers': 1, 'dropout': 0.49988642143818945, 'lr': 0.0070752470956092555}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45 | Epoch 10: Loss=0.5270, Accuracy=0.7242, Precision=0.6624, Recall=0.9144, F1=0.7682, ROC_AUC=0.7610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4477281704333894 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46 | Epoch 1: Loss=0.6041, Accuracy=0.6734, Precision=0.6449, Recall=0.7715, F1=0.7026, ROC_AUC=0.7197\n",
      "Trial 46 | Epoch 2: Loss=0.5927, Accuracy=0.6805, Precision=0.6475, Recall=0.7925, F1=0.7127, ROC_AUC=0.7275\n",
      "Trial 46 | Epoch 3: Loss=0.5807, Accuracy=0.6881, Precision=0.6549, Recall=0.7955, F1=0.7184, ROC_AUC=0.7359\n",
      "Trial 46 | Epoch 4: Loss=0.5593, Accuracy=0.7013, Precision=0.6545, Recall=0.8524, F1=0.7405, ROC_AUC=0.7474\n",
      "Trial 46 | Epoch 5: Loss=0.5402, Accuracy=0.7149, Precision=0.6605, Recall=0.8842, F1=0.7562, ROC_AUC=0.7574\n",
      "Trial 46 | Epoch 6: Loss=0.5236, Accuracy=0.7277, Precision=0.6663, Recall=0.9124, F1=0.7702, ROC_AUC=0.7683\n",
      "Trial 46 | Epoch 7: Loss=0.5137, Accuracy=0.7339, Precision=0.6711, Recall=0.9172, F1=0.7751, ROC_AUC=0.7729\n",
      "Trial 46 | Epoch 8: Loss=0.5090, Accuracy=0.7411, Precision=0.6759, Recall=0.9264, F1=0.7816, ROC_AUC=0.7784\n",
      "Trial 46 | Epoch 9: Loss=0.5018, Accuracy=0.7448, Precision=0.6794, Recall=0.9268, F1=0.7841, ROC_AUC=0.7815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:09:16,130] Trial 46 finished with value: 0.7452340627328772 and parameters: {'hidden_dim': 229, 'num_layers': 1, 'dropout': 0.4477281704333894, 'lr': 0.0030839441712230725}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46 | Epoch 10: Loss=0.5014, Accuracy=0.7452, Precision=0.6841, Recall=0.9111, F1=0.7815, ROC_AUC=0.7827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4423933806791868 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47 | Epoch 1: Loss=0.6066, Accuracy=0.6732, Precision=0.6356, Recall=0.8118, F1=0.7129, ROC_AUC=0.7167\n",
      "Trial 47 | Epoch 2: Loss=0.5945, Accuracy=0.6806, Precision=0.6441, Recall=0.8072, F1=0.7165, ROC_AUC=0.7272\n",
      "Trial 47 | Epoch 3: Loss=0.5756, Accuracy=0.6911, Precision=0.6477, Recall=0.8381, F1=0.7307, ROC_AUC=0.7403\n",
      "Trial 47 | Epoch 4: Loss=0.5589, Accuracy=0.7018, Precision=0.6553, Recall=0.8512, F1=0.7405, ROC_AUC=0.7466\n",
      "Trial 47 | Epoch 5: Loss=0.5378, Accuracy=0.7145, Precision=0.6643, Recall=0.8669, F1=0.7522, ROC_AUC=0.7601\n",
      "Trial 47 | Epoch 6: Loss=0.5232, Accuracy=0.7248, Precision=0.6683, Recall=0.8927, F1=0.7643, ROC_AUC=0.7675\n",
      "Trial 47 | Epoch 7: Loss=0.5114, Accuracy=0.7320, Precision=0.6751, Recall=0.8944, F1=0.7694, ROC_AUC=0.7745\n",
      "Trial 47 | Epoch 8: Loss=0.5049, Accuracy=0.7425, Precision=0.6782, Recall=0.9228, F1=0.7818, ROC_AUC=0.7781\n",
      "Trial 47 | Epoch 9: Loss=0.5016, Accuracy=0.7422, Precision=0.6852, Recall=0.8960, F1=0.7766, ROC_AUC=0.7834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:10:44,767] Trial 47 finished with value: 0.7463992954406883 and parameters: {'hidden_dim': 238, 'num_layers': 1, 'dropout': 0.4423933806791868, 'lr': 0.0033250665335463944}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47 | Epoch 10: Loss=0.4981, Accuracy=0.7464, Precision=0.6865, Recall=0.9069, F1=0.7815, ROC_AUC=0.7856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4023119604548739 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 | Epoch 1: Loss=0.6018, Accuracy=0.6737, Precision=0.6514, Recall=0.7471, F1=0.6960, ROC_AUC=0.7222\n",
      "Trial 48 | Epoch 2: Loss=0.5908, Accuracy=0.6813, Precision=0.6498, Recall=0.7862, F1=0.7116, ROC_AUC=0.7281\n",
      "Trial 48 | Epoch 3: Loss=0.5740, Accuracy=0.6908, Precision=0.6480, Recall=0.8354, F1=0.7299, ROC_AUC=0.7375\n",
      "Trial 48 | Epoch 4: Loss=0.5546, Accuracy=0.7051, Precision=0.6535, Recall=0.8731, F1=0.7475, ROC_AUC=0.7491\n",
      "Trial 48 | Epoch 5: Loss=0.5374, Accuracy=0.7173, Precision=0.6577, Recall=0.9063, F1=0.7623, ROC_AUC=0.7573\n",
      "Trial 48 | Epoch 6: Loss=0.5268, Accuracy=0.7242, Precision=0.6619, Recall=0.9167, F1=0.7688, ROC_AUC=0.7637\n",
      "Trial 48 | Epoch 7: Loss=0.5265, Accuracy=0.7244, Precision=0.6679, Recall=0.8924, F1=0.7640, ROC_AUC=0.7658\n",
      "Trial 48 | Epoch 8: Loss=0.5170, Accuracy=0.7303, Precision=0.6718, Recall=0.9006, F1=0.7696, ROC_AUC=0.7710\n",
      "Trial 48 | Epoch 9: Loss=0.5214, Accuracy=0.7321, Precision=0.6672, Recall=0.9262, F1=0.7757, ROC_AUC=0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:12:10,990] Trial 48 finished with value: 0.7394078991938215 and parameters: {'hidden_dim': 249, 'num_layers': 1, 'dropout': 0.4023119604548739, 'lr': 0.005095554204771827}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 | Epoch 10: Loss=0.5079, Accuracy=0.7394, Precision=0.6773, Recall=0.9144, F1=0.7782, ROC_AUC=0.7756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.44914579333177224 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49 | Epoch 1: Loss=0.6048, Accuracy=0.6740, Precision=0.6397, Recall=0.7968, F1=0.7097, ROC_AUC=0.7175\n",
      "Trial 49 | Epoch 2: Loss=0.5929, Accuracy=0.6817, Precision=0.6440, Recall=0.8126, F1=0.7185, ROC_AUC=0.7270\n",
      "Trial 49 | Epoch 3: Loss=0.5782, Accuracy=0.6875, Precision=0.6514, Recall=0.8067, F1=0.7208, ROC_AUC=0.7366\n",
      "Trial 49 | Epoch 4: Loss=0.5558, Accuracy=0.7046, Precision=0.6499, Recall=0.8871, F1=0.7502, ROC_AUC=0.7498\n",
      "Trial 49 | Epoch 5: Loss=0.5335, Accuracy=0.7186, Precision=0.6630, Recall=0.8889, F1=0.7596, ROC_AUC=0.7594\n",
      "Trial 49 | Epoch 6: Loss=0.5228, Accuracy=0.7270, Precision=0.6668, Recall=0.9072, F1=0.7686, ROC_AUC=0.7674\n",
      "Trial 49 | Epoch 7: Loss=0.5097, Accuracy=0.7357, Precision=0.6758, Recall=0.9059, F1=0.7741, ROC_AUC=0.7741\n",
      "Trial 49 | Epoch 8: Loss=0.5076, Accuracy=0.7405, Precision=0.6751, Recall=0.9274, F1=0.7814, ROC_AUC=0.7773\n",
      "Trial 49 | Epoch 9: Loss=0.4984, Accuracy=0.7466, Precision=0.6824, Recall=0.9226, F1=0.7845, ROC_AUC=0.7837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-10 23:13:37,965] Trial 49 finished with value: 0.742077095047761 and parameters: {'hidden_dim': 239, 'num_layers': 1, 'dropout': 0.44914579333177224, 'lr': 0.003227203695540291}. Best is trial 37 with value: 0.7483774812004607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49 | Epoch 10: Loss=0.4995, Accuracy=0.7421, Precision=0.6830, Recall=0.9035, F1=0.7779, ROC_AUC=0.7818\n",
      "Best parameters: {'hidden_dim': 236, 'num_layers': 1, 'dropout': 0.47212177749477396, 'lr': 0.0026502378845647662}\n",
      "Best accuracy: 0.7483774812004607\n",
      "Best model saved as 'best_lstm_model.pth'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# 定义 LSTM 模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers, dropout, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # 取最后一个时间步的输出\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df, target_column, sequence_length):\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # 将数据转换为 LSTM 的输入格式 (batch_size, sequence_length, input_size)\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    if num_features % sequence_length != 0:\n",
    "        raise ValueError(\"特征数不能被序列长度整除，请调整 sequence_length。\")\n",
    "    input_size = num_features // sequence_length\n",
    "    X = X.reshape(num_samples, sequence_length, input_size)\n",
    "    return X, y\n",
    "\n",
    "# 模型训练函数\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 模型评估函数\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]  # 取正类的概率\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0\n",
    "    return val_loss / len(test_loader), accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "# Optuna 目标函数\n",
    "def objective(trial, X_train, y_train, X_test, y_test, train_loader, test_loader, device):\n",
    "    global best_accuracy, best_model_state\n",
    "\n",
    "    # 超参数搜索空间\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 64, 256)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # 模型初始化\n",
    "    model = LSTMModel(input_size=X_train.shape[2], hidden_dim=hidden_dim, num_layers=num_layers, \n",
    "                      dropout=dropout, output_dim=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 模型训练和评估\n",
    "    for epoch in range(10):  # 每次试验训练 10 个 epoch\n",
    "        train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, accuracy, precision, recall, f1, roc_auc = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "        # 打印每轮指标\n",
    "        print(f\"Trial {trial.number} | Epoch {epoch + 1}: Loss={val_loss:.4f}, \"\n",
    "              f\"Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, \"\n",
    "              f\"F1={f1:.4f}, ROC_AUC={roc_auc:.4f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    global train_loader, test_loader, device, best_accuracy, best_model_state\n",
    "\n",
    "    # 初始化全局变量\n",
    "    best_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    # 加载数据\n",
    "    target_column = 'match_flag'\n",
    "    sequence_length = 10  # 请根据特征数调整此参数\n",
    "    df = pd.read_csv('cleaned_aki_patients_labs_sampled_with_flags.csv')\n",
    "    X, y = preprocess_data(df, target_column, sequence_length)\n",
    "\n",
    "    # 检查预处理后的数据形状\n",
    "    print(\"X shape after preprocessing:\", X.shape)  # 应为 (num_samples, sequence_length, input_size)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # 数据加载器\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 贝叶斯优化\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test, train_loader, test_loader, device), n_trials=50)\n",
    "\n",
    "    # 输出最佳参数\n",
    "    print(\"Best parameters:\", study.best_params)\n",
    "    print(\"Best accuracy:\", best_accuracy)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    torch.save(best_model_state, \"best_lstm_model.pth\")\n",
    "    print(\"Best model saved as 'best_lstm_model.pth'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d362d8f-3bf0-4522-bdcc-5399555cc596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 09:41:46,882] A new study created in memory with name: no-name-ebdf5121-3127-443e-b046-7e55add0bb0a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after preprocessing: (60000, 34, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.43228922254176383 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 | Epoch 1: Loss=0.6841, Accuracy=0.5559, Precision=0.5526, Recall=0.5876, F1=0.5696, ROC_AUC=0.5782\n",
      "Trial 0 | Epoch 2: Loss=0.6842, Accuracy=0.5585, Precision=0.5544, Recall=0.5967, F1=0.5747, ROC_AUC=0.5796\n",
      "Trial 0 | Epoch 3: Loss=0.6838, Accuracy=0.5598, Precision=0.5539, Recall=0.6141, F1=0.5825, ROC_AUC=0.5818\n",
      "Trial 0 | Epoch 4: Loss=0.6832, Accuracy=0.5590, Precision=0.5541, Recall=0.6042, F1=0.5781, ROC_AUC=0.5834\n",
      "Trial 0 | Epoch 5: Loss=0.6826, Accuracy=0.5619, Precision=0.5592, Recall=0.5848, F1=0.5717, ROC_AUC=0.5851\n",
      "Trial 0 | Epoch 6: Loss=0.6847, Accuracy=0.5610, Precision=0.5629, Recall=0.5457, F1=0.5542, ROC_AUC=0.5859\n",
      "Trial 0 | Epoch 7: Loss=0.6820, Accuracy=0.5634, Precision=0.5622, Recall=0.5732, F1=0.5677, ROC_AUC=0.5889\n",
      "Trial 0 | Epoch 8: Loss=0.6807, Accuracy=0.5672, Precision=0.5613, Recall=0.6156, F1=0.5872, ROC_AUC=0.5915\n",
      "Trial 0 | Epoch 9: Loss=0.6818, Accuracy=0.5623, Precision=0.5757, Recall=0.4738, F1=0.5198, ROC_AUC=0.5933\n",
      "Trial 0 | Epoch 10: Loss=0.6800, Accuracy=0.5692, Precision=0.5615, Recall=0.6319, F1=0.5946, ROC_AUC=0.5935\n",
      "Trial 0 | Epoch 11: Loss=0.6795, Accuracy=0.5666, Precision=0.5688, Recall=0.5506, F1=0.5595, ROC_AUC=0.5954\n",
      "Trial 0 | Epoch 12: Loss=0.6795, Accuracy=0.5656, Precision=0.5696, Recall=0.5367, F1=0.5527, ROC_AUC=0.5960\n",
      "Trial 0 | Epoch 13: Loss=0.6793, Accuracy=0.5668, Precision=0.5679, Recall=0.5591, F1=0.5635, ROC_AUC=0.5965\n",
      "Trial 0 | Epoch 14: Loss=0.6797, Accuracy=0.5653, Precision=0.5696, Recall=0.5346, F1=0.5515, ROC_AUC=0.5971\n",
      "Trial 0 | Epoch 15: Loss=0.6801, Accuracy=0.5745, Precision=0.5650, Recall=0.6476, F1=0.6035, ROC_AUC=0.5963\n",
      "Trial 0 | Epoch 16: Loss=0.6786, Accuracy=0.5716, Precision=0.5719, Recall=0.5698, F1=0.5708, ROC_AUC=0.5987\n",
      "Trial 0 | Epoch 17: Loss=0.6776, Accuracy=0.5704, Precision=0.5677, Recall=0.5908, F1=0.5790, ROC_AUC=0.5997\n",
      "Trial 0 | Epoch 18: Loss=0.6780, Accuracy=0.5712, Precision=0.5749, Recall=0.5463, F1=0.5602, ROC_AUC=0.6009\n",
      "Trial 0 | Epoch 19: Loss=0.6782, Accuracy=0.5760, Precision=0.5640, Recall=0.6700, F1=0.6124, ROC_AUC=0.5992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 09:46:32,299] Trial 0 finished with value: 0.5747222222222222 and parameters: {'hidden_dim': 310, 'num_layers': 1, 'dropout': 0.43228922254176383, 'lr': 4.687153088813999e-05}. Best is trial 0 with value: 0.5747222222222222.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 | Epoch 20: Loss=0.6763, Accuracy=0.5747, Precision=0.5652, Recall=0.6481, F1=0.6038, ROC_AUC=0.6014\n",
      "Trial 1 | Epoch 1: Loss=0.6923, Accuracy=0.4989, Precision=0.4994, Recall=0.9877, F1=0.6634, ROC_AUC=0.5293\n",
      "Trial 1 | Epoch 2: Loss=0.6886, Accuracy=0.5491, Precision=0.5525, Recall=0.5167, F1=0.5340, ROC_AUC=0.5551\n",
      "Trial 1 | Epoch 3: Loss=0.6795, Accuracy=0.5683, Precision=0.5804, Recall=0.4928, F1=0.5330, ROC_AUC=0.6004\n",
      "Trial 1 | Epoch 4: Loss=0.6768, Accuracy=0.5784, Precision=0.5753, Recall=0.5992, F1=0.5870, ROC_AUC=0.6055\n",
      "Trial 1 | Epoch 5: Loss=0.6731, Accuracy=0.5827, Precision=0.5834, Recall=0.5786, F1=0.5810, ROC_AUC=0.6156\n",
      "Trial 1 | Epoch 6: Loss=0.6718, Accuracy=0.5867, Precision=0.5790, Recall=0.6358, F1=0.6060, ROC_AUC=0.6184\n",
      "Trial 1 | Epoch 7: Loss=0.6764, Accuracy=0.5785, Precision=0.5975, Recall=0.4810, F1=0.5330, ROC_AUC=0.6107\n",
      "Trial 1 | Epoch 8: Loss=0.6711, Accuracy=0.5916, Precision=0.5914, Recall=0.5928, F1=0.5921, ROC_AUC=0.6187\n",
      "Trial 1 | Epoch 9: Loss=0.6744, Accuracy=0.5849, Precision=0.5918, Recall=0.5470, F1=0.5685, ROC_AUC=0.6133\n",
      "Trial 1 | Epoch 10: Loss=0.6658, Accuracy=0.5934, Precision=0.5932, Recall=0.5949, F1=0.5940, ROC_AUC=0.6313\n",
      "Trial 1 | Epoch 11: Loss=0.6707, Accuracy=0.5869, Precision=0.5776, Recall=0.6469, F1=0.6103, ROC_AUC=0.6169\n",
      "Trial 1 | Epoch 12: Loss=0.6507, Accuracy=0.6066, Precision=0.5969, Recall=0.6563, F1=0.6252, ROC_AUC=0.6553\n",
      "Trial 1 | Epoch 13: Loss=0.6571, Accuracy=0.5981, Precision=0.5849, Recall=0.6760, F1=0.6272, ROC_AUC=0.6459\n",
      "Trial 1 | Epoch 14: Loss=0.6468, Accuracy=0.6073, Precision=0.5943, Recall=0.6760, F1=0.6325, ROC_AUC=0.6576\n",
      "Trial 1 | Epoch 15: Loss=0.6316, Accuracy=0.6247, Precision=0.6187, Recall=0.6501, F1=0.6340, ROC_AUC=0.6806\n",
      "Trial 1 | Epoch 16: Loss=0.6252, Accuracy=0.6253, Precision=0.6220, Recall=0.6389, F1=0.6303, ROC_AUC=0.6904\n",
      "Trial 1 | Epoch 17: Loss=0.6222, Accuracy=0.6369, Precision=0.6228, Recall=0.6943, F1=0.6566, ROC_AUC=0.6987\n",
      "Trial 1 | Epoch 18: Loss=0.6062, Accuracy=0.6422, Precision=0.6527, Recall=0.6077, F1=0.6294, ROC_AUC=0.7154\n",
      "Trial 1 | Epoch 19: Loss=0.5950, Accuracy=0.6531, Precision=0.6333, Recall=0.7272, F1=0.6770, ROC_AUC=0.7292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 09:56:40,593] Trial 1 finished with value: 0.6596111111111111 and parameters: {'hidden_dim': 191, 'num_layers': 3, 'dropout': 0.39613443858322284, 'lr': 0.0008705189828171966}. Best is trial 1 with value: 0.6596111111111111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 | Epoch 20: Loss=0.5841, Accuracy=0.6596, Precision=0.6577, Recall=0.6658, F1=0.6617, ROC_AUC=0.7424\n",
      "Trial 2 | Epoch 1: Loss=0.6852, Accuracy=0.5568, Precision=0.5488, Recall=0.6389, F1=0.5904, ROC_AUC=0.5775\n",
      "Trial 2 | Epoch 2: Loss=0.6828, Accuracy=0.5611, Precision=0.5599, Recall=0.5709, F1=0.5653, ROC_AUC=0.5842\n",
      "Trial 2 | Epoch 3: Loss=0.6837, Accuracy=0.5583, Precision=0.5682, Recall=0.4856, F1=0.5236, ROC_AUC=0.5849\n",
      "Trial 2 | Epoch 4: Loss=0.6823, Accuracy=0.5642, Precision=0.5778, Recall=0.4770, F1=0.5226, ROC_AUC=0.5881\n",
      "Trial 2 | Epoch 5: Loss=0.6818, Accuracy=0.5667, Precision=0.5610, Recall=0.6136, F1=0.5861, ROC_AUC=0.5879\n",
      "Trial 2 | Epoch 6: Loss=0.6820, Accuracy=0.5631, Precision=0.5564, Recall=0.6222, F1=0.5875, ROC_AUC=0.5889\n",
      "Trial 2 | Epoch 7: Loss=0.6802, Accuracy=0.5696, Precision=0.5660, Recall=0.5970, F1=0.5811, ROC_AUC=0.5920\n",
      "Trial 2 | Epoch 8: Loss=0.6828, Accuracy=0.5642, Precision=0.5541, Recall=0.6582, F1=0.6017, ROC_AUC=0.5861\n",
      "Trial 2 | Epoch 9: Loss=0.6797, Accuracy=0.5694, Precision=0.5642, Recall=0.6104, F1=0.5864, ROC_AUC=0.5935\n",
      "Trial 2 | Epoch 10: Loss=0.6786, Accuracy=0.5700, Precision=0.5725, Recall=0.5524, F1=0.5623, ROC_AUC=0.5971\n",
      "Trial 2 | Epoch 11: Loss=0.6790, Accuracy=0.5733, Precision=0.5737, Recall=0.5703, F1=0.5720, ROC_AUC=0.5975\n",
      "Trial 2 | Epoch 12: Loss=0.6777, Accuracy=0.5753, Precision=0.5671, Recall=0.6370, F1=0.6000, ROC_AUC=0.6005\n",
      "Trial 2 | Epoch 13: Loss=0.6780, Accuracy=0.5703, Precision=0.5816, Recall=0.5013, F1=0.5385, ROC_AUC=0.6009\n",
      "Trial 2 | Epoch 14: Loss=0.6776, Accuracy=0.5732, Precision=0.5746, Recall=0.5642, F1=0.5693, ROC_AUC=0.6015\n",
      "Trial 2 | Epoch 15: Loss=0.6763, Accuracy=0.5766, Precision=0.5643, Recall=0.6722, F1=0.6135, ROC_AUC=0.6031\n",
      "Trial 2 | Epoch 16: Loss=0.6746, Accuracy=0.5782, Precision=0.5691, Recall=0.6437, F1=0.6041, ROC_AUC=0.6076\n",
      "Trial 2 | Epoch 17: Loss=0.6743, Accuracy=0.5786, Precision=0.5689, Recall=0.6483, F1=0.6060, ROC_AUC=0.6096\n",
      "Trial 2 | Epoch 18: Loss=0.6715, Accuracy=0.5807, Precision=0.5727, Recall=0.6354, F1=0.6024, ROC_AUC=0.6132\n",
      "Trial 2 | Epoch 19: Loss=0.6731, Accuracy=0.5791, Precision=0.5592, Recall=0.7469, F1=0.6396, ROC_AUC=0.6134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 11:00:40,126] Trial 2 finished with value: 0.5827222222222223 and parameters: {'hidden_dim': 476, 'num_layers': 5, 'dropout': 0.3893086381720563, 'lr': 3.461937397726362e-05}. Best is trial 1 with value: 0.6596111111111111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 | Epoch 20: Loss=0.6709, Accuracy=0.5827, Precision=0.5750, Recall=0.6343, F1=0.6032, ROC_AUC=0.6124\n",
      "Trial 3 | Epoch 1: Loss=0.6840, Accuracy=0.5586, Precision=0.5532, Recall=0.6098, F1=0.5801, ROC_AUC=0.5809\n",
      "Trial 3 | Epoch 2: Loss=0.6826, Accuracy=0.5652, Precision=0.5771, Recall=0.4876, F1=0.5286, ROC_AUC=0.5881\n",
      "Trial 3 | Epoch 3: Loss=0.6827, Accuracy=0.5690, Precision=0.5603, Recall=0.6407, F1=0.5978, ROC_AUC=0.5908\n",
      "Trial 3 | Epoch 4: Loss=0.6792, Accuracy=0.5717, Precision=0.5785, Recall=0.5286, F1=0.5524, ROC_AUC=0.5974\n",
      "Trial 3 | Epoch 5: Loss=0.6771, Accuracy=0.5744, Precision=0.5781, Recall=0.5512, F1=0.5643, ROC_AUC=0.6023\n",
      "Trial 3 | Epoch 6: Loss=0.6792, Accuracy=0.5706, Precision=0.5544, Recall=0.7192, F1=0.6261, ROC_AUC=0.5983\n",
      "Trial 3 | Epoch 7: Loss=0.6730, Accuracy=0.5783, Precision=0.5696, Recall=0.6410, F1=0.6032, ROC_AUC=0.6123\n",
      "Trial 3 | Epoch 8: Loss=0.6760, Accuracy=0.5797, Precision=0.5597, Recall=0.7469, F1=0.6399, ROC_AUC=0.6146\n",
      "Trial 3 | Epoch 9: Loss=0.6690, Accuracy=0.5898, Precision=0.5771, Recall=0.6726, F1=0.6212, ROC_AUC=0.6259\n",
      "Trial 3 | Epoch 10: Loss=0.6663, Accuracy=0.5896, Precision=0.5957, Recall=0.5578, F1=0.5761, ROC_AUC=0.6283\n",
      "Trial 3 | Epoch 11: Loss=0.6653, Accuracy=0.5934, Precision=0.5994, Recall=0.5631, F1=0.5807, ROC_AUC=0.6321\n",
      "Trial 3 | Epoch 12: Loss=0.6643, Accuracy=0.5947, Precision=0.5846, Recall=0.6543, F1=0.6175, ROC_AUC=0.6341\n",
      "Trial 3 | Epoch 13: Loss=0.6618, Accuracy=0.5975, Precision=0.5933, Recall=0.6199, F1=0.6063, ROC_AUC=0.6371\n",
      "Trial 3 | Epoch 14: Loss=0.6626, Accuracy=0.5936, Precision=0.5906, Recall=0.6100, F1=0.6001, ROC_AUC=0.6369\n",
      "Trial 3 | Epoch 15: Loss=0.6607, Accuracy=0.5976, Precision=0.6127, Recall=0.5306, F1=0.5687, ROC_AUC=0.6418\n",
      "Trial 3 | Epoch 16: Loss=0.6582, Accuracy=0.5979, Precision=0.5921, Recall=0.6296, F1=0.6102, ROC_AUC=0.6434\n",
      "Trial 3 | Epoch 17: Loss=0.6549, Accuracy=0.6023, Precision=0.5968, Recall=0.6308, F1=0.6133, ROC_AUC=0.6495\n",
      "Trial 3 | Epoch 18: Loss=0.6518, Accuracy=0.6061, Precision=0.6120, Recall=0.5793, F1=0.5952, ROC_AUC=0.6564\n",
      "Trial 3 | Epoch 19: Loss=0.6519, Accuracy=0.6054, Precision=0.6046, Recall=0.6097, F1=0.6071, ROC_AUC=0.6549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 11:35:26,164] Trial 3 finished with value: 0.6130555555555556 and parameters: {'hidden_dim': 508, 'num_layers': 3, 'dropout': 0.3510048797938311, 'lr': 5.019442737680919e-05}. Best is trial 1 with value: 0.6596111111111111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 | Epoch 20: Loss=0.6488, Accuracy=0.6131, Precision=0.6180, Recall=0.5921, F1=0.6048, ROC_AUC=0.6638\n",
      "Trial 4 | Epoch 1: Loss=0.6838, Accuracy=0.5644, Precision=0.5552, Recall=0.6479, F1=0.5980, ROC_AUC=0.5841\n",
      "Trial 4 | Epoch 2: Loss=0.6784, Accuracy=0.5754, Precision=0.5760, Recall=0.5718, F1=0.5739, ROC_AUC=0.6019\n",
      "Trial 4 | Epoch 3: Loss=0.6804, Accuracy=0.5703, Precision=0.5488, Recall=0.7899, F1=0.6477, ROC_AUC=0.6145\n",
      "Trial 4 | Epoch 4: Loss=0.6690, Accuracy=0.5923, Precision=0.5876, Recall=0.6192, F1=0.6030, ROC_AUC=0.6299\n",
      "Trial 4 | Epoch 5: Loss=0.6610, Accuracy=0.5960, Precision=0.6009, Recall=0.5717, F1=0.5859, ROC_AUC=0.6401\n",
      "Trial 4 | Epoch 6: Loss=0.6652, Accuracy=0.5921, Precision=0.5951, Recall=0.5762, F1=0.5855, ROC_AUC=0.6325\n",
      "Trial 4 | Epoch 7: Loss=0.6433, Accuracy=0.6233, Precision=0.6119, Recall=0.6739, F1=0.6414, ROC_AUC=0.6749\n",
      "Trial 4 | Epoch 8: Loss=0.6304, Accuracy=0.6282, Precision=0.6380, Recall=0.5924, F1=0.6144, ROC_AUC=0.6895\n",
      "Trial 4 | Epoch 9: Loss=0.5989, Accuracy=0.6563, Precision=0.6342, Recall=0.7386, F1=0.6824, ROC_AUC=0.7313\n",
      "Trial 4 | Epoch 10: Loss=0.5767, Accuracy=0.6745, Precision=0.6791, Recall=0.6616, F1=0.6702, ROC_AUC=0.7558\n",
      "Trial 4 | Epoch 11: Loss=0.5306, Accuracy=0.6959, Precision=0.6886, Recall=0.7152, F1=0.7017, ROC_AUC=0.7910\n",
      "Trial 4 | Epoch 12: Loss=0.4949, Accuracy=0.7148, Precision=0.7237, Recall=0.6949, F1=0.7090, ROC_AUC=0.8222\n",
      "Trial 4 | Epoch 13: Loss=0.4876, Accuracy=0.7176, Precision=0.7168, Recall=0.7193, F1=0.7181, ROC_AUC=0.8296\n",
      "Trial 4 | Epoch 14: Loss=0.4755, Accuracy=0.7256, Precision=0.7255, Recall=0.7258, F1=0.7256, ROC_AUC=0.8414\n",
      "Trial 4 | Epoch 15: Loss=0.4699, Accuracy=0.7274, Precision=0.7325, Recall=0.7163, F1=0.7243, ROC_AUC=0.8470\n",
      "Trial 4 | Epoch 16: Loss=0.4646, Accuracy=0.7297, Precision=0.7300, Recall=0.7292, F1=0.7296, ROC_AUC=0.8525\n",
      "Trial 4 | Epoch 17: Loss=0.4731, Accuracy=0.7314, Precision=0.7201, Recall=0.7572, F1=0.7382, ROC_AUC=0.8563\n",
      "Trial 4 | Epoch 18: Loss=0.4857, Accuracy=0.7313, Precision=0.7344, Recall=0.7249, F1=0.7296, ROC_AUC=0.8551\n",
      "Trial 4 | Epoch 19: Loss=0.4704, Accuracy=0.7298, Precision=0.7333, Recall=0.7223, F1=0.7278, ROC_AUC=0.8567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 12:25:40,084] Trial 4 finished with value: 0.7318888888888889 and parameters: {'hidden_dim': 498, 'num_layers': 4, 'dropout': 0.12580597539344698, 'lr': 0.0003222969336076591}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 | Epoch 20: Loss=0.4710, Accuracy=0.7319, Precision=0.7411, Recall=0.7128, F1=0.7267, ROC_AUC=0.8603\n",
      "Trial 5 | Epoch 1: Loss=0.6844, Accuracy=0.5622, Precision=0.5559, Recall=0.6182, F1=0.5854, ROC_AUC=0.5823\n",
      "Trial 5 | Epoch 2: Loss=0.6818, Accuracy=0.5612, Precision=0.5550, Recall=0.6179, F1=0.5848, ROC_AUC=0.5863\n",
      "Trial 5 | Epoch 3: Loss=0.6824, Accuracy=0.5645, Precision=0.5780, Recall=0.4782, F1=0.5234, ROC_AUC=0.5919\n",
      "Trial 5 | Epoch 4: Loss=0.6797, Accuracy=0.5719, Precision=0.5654, Recall=0.6213, F1=0.5921, ROC_AUC=0.5949\n",
      "Trial 5 | Epoch 5: Loss=0.6802, Accuracy=0.5694, Precision=0.5641, Recall=0.6114, F1=0.5868, ROC_AUC=0.5922\n",
      "Trial 5 | Epoch 6: Loss=0.6784, Accuracy=0.5726, Precision=0.5669, Recall=0.6157, F1=0.5903, ROC_AUC=0.5978\n",
      "Trial 5 | Epoch 7: Loss=0.6772, Accuracy=0.5711, Precision=0.5685, Recall=0.5900, F1=0.5790, ROC_AUC=0.5996\n",
      "Trial 5 | Epoch 8: Loss=0.6772, Accuracy=0.5747, Precision=0.5659, Recall=0.6413, F1=0.6013, ROC_AUC=0.6000\n",
      "Trial 5 | Epoch 9: Loss=0.6759, Accuracy=0.5768, Precision=0.5696, Recall=0.6286, F1=0.5976, ROC_AUC=0.6054\n",
      "Trial 5 | Epoch 10: Loss=0.6747, Accuracy=0.5800, Precision=0.5630, Recall=0.7147, F1=0.6298, ROC_AUC=0.6099\n",
      "Trial 5 | Epoch 11: Loss=0.6729, Accuracy=0.5847, Precision=0.5728, Recall=0.6661, F1=0.6159, ROC_AUC=0.6139\n",
      "Trial 5 | Epoch 12: Loss=0.6711, Accuracy=0.5857, Precision=0.5772, Recall=0.6403, F1=0.6071, ROC_AUC=0.6178\n",
      "Trial 5 | Epoch 13: Loss=0.6713, Accuracy=0.5846, Precision=0.5799, Recall=0.6137, F1=0.5963, ROC_AUC=0.6170\n",
      "Trial 5 | Epoch 14: Loss=0.6679, Accuracy=0.5876, Precision=0.5953, Recall=0.5468, F1=0.5700, ROC_AUC=0.6243\n",
      "Trial 5 | Epoch 15: Loss=0.6666, Accuracy=0.5872, Precision=0.5848, Recall=0.6017, F1=0.5931, ROC_AUC=0.6256\n",
      "Trial 5 | Epoch 16: Loss=0.6648, Accuracy=0.5927, Precision=0.5880, Recall=0.6190, F1=0.6031, ROC_AUC=0.6291\n",
      "Trial 5 | Epoch 17: Loss=0.6653, Accuracy=0.5942, Precision=0.5984, Recall=0.5728, F1=0.5853, ROC_AUC=0.6348\n",
      "Trial 5 | Epoch 18: Loss=0.6617, Accuracy=0.5948, Precision=0.5963, Recall=0.5874, F1=0.5918, ROC_AUC=0.6370\n",
      "Trial 5 | Epoch 19: Loss=0.6561, Accuracy=0.6002, Precision=0.5960, Recall=0.6221, F1=0.6088, ROC_AUC=0.6469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 12:42:47,923] Trial 5 finished with value: 0.6012777777777778 and parameters: {'hidden_dim': 221, 'num_layers': 4, 'dropout': 0.24021684930426815, 'lr': 7.850183548074455e-05}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 | Epoch 20: Loss=0.6540, Accuracy=0.6013, Precision=0.5993, Recall=0.6113, F1=0.6052, ROC_AUC=0.6491\n",
      "Trial 6 | Epoch 1: Loss=0.6848, Accuracy=0.5584, Precision=0.5620, Recall=0.5299, F1=0.5455, ROC_AUC=0.5791\n",
      "Trial 6 | Epoch 2: Loss=0.6819, Accuracy=0.5598, Precision=0.5549, Recall=0.6046, F1=0.5786, ROC_AUC=0.5864\n",
      "Trial 6 | Epoch 3: Loss=0.6833, Accuracy=0.5584, Precision=0.5880, Recall=0.3900, F1=0.4690, ROC_AUC=0.5898\n",
      "Trial 6 | Epoch 4: Loss=0.6834, Accuracy=0.5680, Precision=0.5583, Recall=0.6517, F1=0.6014, ROC_AUC=0.5909\n",
      "Trial 6 | Epoch 5: Loss=0.6828, Accuracy=0.5714, Precision=0.5695, Recall=0.5856, F1=0.5774, ROC_AUC=0.5942\n",
      "Trial 6 | Epoch 6: Loss=0.6799, Accuracy=0.5687, Precision=0.5697, Recall=0.5614, F1=0.5655, ROC_AUC=0.5953\n",
      "Trial 6 | Epoch 7: Loss=0.6777, Accuracy=0.5733, Precision=0.5715, Recall=0.5856, F1=0.5785, ROC_AUC=0.6006\n",
      "Trial 6 | Epoch 8: Loss=0.6778, Accuracy=0.5742, Precision=0.5614, Recall=0.6789, F1=0.6146, ROC_AUC=0.6023\n",
      "Trial 6 | Epoch 9: Loss=0.6754, Accuracy=0.5735, Precision=0.5639, Recall=0.6486, F1=0.6033, ROC_AUC=0.6062\n",
      "Trial 6 | Epoch 10: Loss=0.6739, Accuracy=0.5819, Precision=0.5735, Recall=0.6393, F1=0.6046, ROC_AUC=0.6082\n",
      "Trial 6 | Epoch 11: Loss=0.6754, Accuracy=0.5796, Precision=0.5786, Recall=0.5860, F1=0.5823, ROC_AUC=0.6100\n",
      "Trial 6 | Epoch 12: Loss=0.6705, Accuracy=0.5798, Precision=0.5736, Recall=0.6218, F1=0.5967, ROC_AUC=0.6172\n",
      "Trial 6 | Epoch 13: Loss=0.6691, Accuracy=0.5859, Precision=0.5850, Recall=0.5912, F1=0.5881, ROC_AUC=0.6229\n",
      "Trial 6 | Epoch 14: Loss=0.6659, Accuracy=0.5927, Precision=0.5811, Recall=0.6641, F1=0.6198, ROC_AUC=0.6274\n",
      "Trial 6 | Epoch 15: Loss=0.6626, Accuracy=0.5970, Precision=0.5825, Recall=0.6846, F1=0.6294, ROC_AUC=0.6348\n",
      "Trial 6 | Epoch 16: Loss=0.6590, Accuracy=0.5959, Precision=0.5838, Recall=0.6680, F1=0.6231, ROC_AUC=0.6404\n",
      "Trial 6 | Epoch 17: Loss=0.6506, Accuracy=0.6017, Precision=0.5988, Recall=0.6161, F1=0.6073, ROC_AUC=0.6532\n",
      "Trial 6 | Epoch 18: Loss=0.6521, Accuracy=0.6046, Precision=0.5946, Recall=0.6569, F1=0.6242, ROC_AUC=0.6528\n",
      "Trial 6 | Epoch 19: Loss=0.6416, Accuracy=0.6198, Precision=0.6077, Recall=0.6762, F1=0.6401, ROC_AUC=0.6693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 13:14:47,354] Trial 6 finished with value: 0.6199444444444444 and parameters: {'hidden_dim': 312, 'num_layers': 5, 'dropout': 0.20959820405164598, 'lr': 7.73932864758439e-05}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 | Epoch 20: Loss=0.6341, Accuracy=0.6199, Precision=0.6098, Recall=0.6661, F1=0.6367, ROC_AUC=0.6801\n",
      "Trial 7 | Epoch 1: Loss=0.6868, Accuracy=0.5519, Precision=0.5464, Recall=0.6109, F1=0.5769, ROC_AUC=0.5715\n",
      "Trial 7 | Epoch 2: Loss=0.6836, Accuracy=0.5593, Precision=0.5685, Recall=0.4926, F1=0.5278, ROC_AUC=0.5851\n",
      "Trial 7 | Epoch 3: Loss=0.6817, Accuracy=0.5617, Precision=0.5612, Recall=0.5656, F1=0.5634, ROC_AUC=0.5880\n",
      "Trial 7 | Epoch 4: Loss=0.6814, Accuracy=0.5673, Precision=0.5644, Recall=0.5896, F1=0.5767, ROC_AUC=0.5908\n",
      "Trial 7 | Epoch 5: Loss=0.6798, Accuracy=0.5705, Precision=0.5680, Recall=0.5890, F1=0.5783, ROC_AUC=0.5946\n",
      "Trial 7 | Epoch 6: Loss=0.6793, Accuracy=0.5721, Precision=0.5648, Recall=0.6277, F1=0.5946, ROC_AUC=0.5959\n",
      "Trial 7 | Epoch 7: Loss=0.6784, Accuracy=0.5705, Precision=0.5689, Recall=0.5821, F1=0.5754, ROC_AUC=0.5983\n",
      "Trial 7 | Epoch 8: Loss=0.6780, Accuracy=0.5709, Precision=0.5613, Recall=0.6489, F1=0.6019, ROC_AUC=0.5996\n",
      "Trial 7 | Epoch 9: Loss=0.6780, Accuracy=0.5725, Precision=0.5628, Recall=0.6496, F1=0.6031, ROC_AUC=0.5994\n",
      "Trial 7 | Epoch 10: Loss=0.6781, Accuracy=0.5731, Precision=0.5582, Recall=0.7013, F1=0.6216, ROC_AUC=0.5998\n",
      "Trial 7 | Epoch 11: Loss=0.6777, Accuracy=0.5702, Precision=0.5777, Recall=0.5221, F1=0.5485, ROC_AUC=0.5995\n",
      "Trial 7 | Epoch 12: Loss=0.6763, Accuracy=0.5776, Precision=0.5649, Recall=0.6748, F1=0.6150, ROC_AUC=0.6026\n",
      "Trial 7 | Epoch 13: Loss=0.6766, Accuracy=0.5755, Precision=0.5679, Recall=0.6313, F1=0.5979, ROC_AUC=0.6025\n",
      "Trial 7 | Epoch 14: Loss=0.6774, Accuracy=0.5776, Precision=0.5623, Recall=0.7003, F1=0.6238, ROC_AUC=0.6036\n",
      "Trial 7 | Epoch 15: Loss=0.6754, Accuracy=0.5761, Precision=0.5745, Recall=0.5869, F1=0.5806, ROC_AUC=0.6059\n",
      "Trial 7 | Epoch 16: Loss=0.6749, Accuracy=0.5779, Precision=0.5690, Recall=0.6422, F1=0.6034, ROC_AUC=0.6058\n",
      "Trial 7 | Epoch 17: Loss=0.6745, Accuracy=0.5789, Precision=0.5727, Recall=0.6221, F1=0.5964, ROC_AUC=0.6074\n",
      "Trial 7 | Epoch 18: Loss=0.6739, Accuracy=0.5773, Precision=0.5725, Recall=0.6107, F1=0.5910, ROC_AUC=0.6076\n",
      "Trial 7 | Epoch 19: Loss=0.6725, Accuracy=0.5804, Precision=0.5730, Recall=0.6309, F1=0.6006, ROC_AUC=0.6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 16:25:07,693] Trial 7 finished with value: 0.5812222222222222 and parameters: {'hidden_dim': 197, 'num_layers': 3, 'dropout': 0.1353917276160536, 'lr': 5.253148203847825e-05}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 | Epoch 20: Loss=0.6734, Accuracy=0.5812, Precision=0.5686, Recall=0.6734, F1=0.6166, ROC_AUC=0.6110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2919268562288362 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 | Epoch 1: Loss=0.6850, Accuracy=0.5571, Precision=0.5511, Recall=0.6150, F1=0.5813, ROC_AUC=0.5793\n",
      "Trial 8 | Epoch 2: Loss=0.6833, Accuracy=0.5640, Precision=0.5621, Recall=0.5794, F1=0.5706, ROC_AUC=0.5839\n",
      "Trial 8 | Epoch 3: Loss=0.6823, Accuracy=0.5623, Precision=0.5490, Recall=0.6972, F1=0.6143, ROC_AUC=0.5883\n",
      "Trial 8 | Epoch 4: Loss=0.6806, Accuracy=0.5706, Precision=0.5622, Recall=0.6379, F1=0.5977, ROC_AUC=0.5937\n",
      "Trial 8 | Epoch 5: Loss=0.6800, Accuracy=0.5681, Precision=0.5537, Recall=0.7028, F1=0.6194, ROC_AUC=0.5939\n",
      "Trial 8 | Epoch 6: Loss=0.6765, Accuracy=0.5742, Precision=0.5807, Recall=0.5341, F1=0.5564, ROC_AUC=0.6066\n",
      "Trial 8 | Epoch 7: Loss=0.6758, Accuracy=0.5751, Precision=0.5879, Recall=0.5022, F1=0.5417, ROC_AUC=0.6083\n",
      "Trial 8 | Epoch 8: Loss=0.6743, Accuracy=0.5776, Precision=0.5753, Recall=0.5928, F1=0.5839, ROC_AUC=0.6110\n",
      "Trial 8 | Epoch 9: Loss=0.6736, Accuracy=0.5782, Precision=0.5847, Recall=0.5399, F1=0.5614, ROC_AUC=0.6128\n",
      "Trial 8 | Epoch 10: Loss=0.6729, Accuracy=0.5824, Precision=0.5846, Recall=0.5694, F1=0.5769, ROC_AUC=0.6136\n",
      "Trial 8 | Epoch 11: Loss=0.6741, Accuracy=0.5819, Precision=0.5794, Recall=0.5974, F1=0.5883, ROC_AUC=0.6131\n",
      "Trial 8 | Epoch 12: Loss=0.6730, Accuracy=0.5796, Precision=0.5802, Recall=0.5757, F1=0.5779, ROC_AUC=0.6151\n",
      "Trial 8 | Epoch 13: Loss=0.6724, Accuracy=0.5842, Precision=0.5818, Recall=0.5993, F1=0.5904, ROC_AUC=0.6149\n",
      "Trial 8 | Epoch 14: Loss=0.6722, Accuracy=0.5794, Precision=0.5860, Recall=0.5410, F1=0.5626, ROC_AUC=0.6160\n",
      "Trial 8 | Epoch 15: Loss=0.6718, Accuracy=0.5844, Precision=0.5825, Recall=0.5960, F1=0.5892, ROC_AUC=0.6168\n",
      "Trial 8 | Epoch 16: Loss=0.6733, Accuracy=0.5826, Precision=0.5821, Recall=0.5851, F1=0.5836, ROC_AUC=0.6140\n",
      "Trial 8 | Epoch 17: Loss=0.6715, Accuracy=0.5832, Precision=0.5832, Recall=0.5827, F1=0.5830, ROC_AUC=0.6187\n",
      "Trial 8 | Epoch 18: Loss=0.6741, Accuracy=0.5818, Precision=0.5956, Recall=0.5094, F1=0.5492, ROC_AUC=0.6164\n",
      "Trial 8 | Epoch 19: Loss=0.6739, Accuracy=0.5806, Precision=0.5850, Recall=0.5542, F1=0.5692, ROC_AUC=0.6123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 16:29:34,882] Trial 8 finished with value: 0.5840555555555556 and parameters: {'hidden_dim': 434, 'num_layers': 1, 'dropout': 0.2919268562288362, 'lr': 8.482229420644452e-05}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 | Epoch 20: Loss=0.6706, Accuracy=0.5841, Precision=0.5837, Recall=0.5861, F1=0.5849, ROC_AUC=0.6191\n",
      "Trial 9 | Epoch 1: Loss=0.6842, Accuracy=0.5613, Precision=0.5510, Recall=0.6623, F1=0.6015, ROC_AUC=0.5851\n",
      "Trial 9 | Epoch 2: Loss=0.6837, Accuracy=0.5615, Precision=0.5473, Recall=0.7117, F1=0.6188, ROC_AUC=0.5881\n",
      "Trial 9 | Epoch 3: Loss=0.6797, Accuracy=0.5720, Precision=0.5682, Recall=0.6002, F1=0.5837, ROC_AUC=0.5970\n",
      "Trial 9 | Epoch 4: Loss=0.6791, Accuracy=0.5737, Precision=0.5549, Recall=0.7448, F1=0.6360, ROC_AUC=0.6064\n",
      "Trial 9 | Epoch 5: Loss=0.6771, Accuracy=0.5741, Precision=0.5927, Recall=0.4736, F1=0.5265, ROC_AUC=0.6080\n",
      "Trial 9 | Epoch 6: Loss=0.6730, Accuracy=0.5831, Precision=0.5890, Recall=0.5498, F1=0.5687, ROC_AUC=0.6149\n",
      "Trial 9 | Epoch 7: Loss=0.6727, Accuracy=0.5833, Precision=0.5836, Recall=0.5816, F1=0.5826, ROC_AUC=0.6138\n",
      "Trial 9 | Epoch 8: Loss=0.6717, Accuracy=0.5807, Precision=0.5861, Recall=0.5496, F1=0.5672, ROC_AUC=0.6159\n",
      "Trial 9 | Epoch 9: Loss=0.6718, Accuracy=0.5814, Precision=0.5802, Recall=0.5891, F1=0.5846, ROC_AUC=0.6151\n",
      "Trial 9 | Epoch 10: Loss=0.6698, Accuracy=0.5893, Precision=0.5788, Recall=0.6559, F1=0.6149, ROC_AUC=0.6208\n",
      "Trial 9 | Epoch 11: Loss=0.6690, Accuracy=0.5845, Precision=0.6044, Recall=0.4891, F1=0.5407, ROC_AUC=0.6225\n",
      "Trial 9 | Epoch 12: Loss=0.6685, Accuracy=0.5861, Precision=0.5937, Recall=0.5457, F1=0.5687, ROC_AUC=0.6233\n",
      "Trial 9 | Epoch 13: Loss=0.6663, Accuracy=0.5910, Precision=0.5951, Recall=0.5697, F1=0.5821, ROC_AUC=0.6288\n",
      "Trial 9 | Epoch 14: Loss=0.6670, Accuracy=0.5917, Precision=0.6107, Recall=0.5057, F1=0.5532, ROC_AUC=0.6300\n",
      "Trial 9 | Epoch 15: Loss=0.6657, Accuracy=0.5951, Precision=0.5827, Recall=0.6700, F1=0.6233, ROC_AUC=0.6318\n",
      "Trial 9 | Epoch 16: Loss=0.6614, Accuracy=0.5978, Precision=0.6019, Recall=0.5774, F1=0.5894, ROC_AUC=0.6383\n",
      "Trial 9 | Epoch 17: Loss=0.6621, Accuracy=0.5993, Precision=0.6099, Recall=0.5508, F1=0.5789, ROC_AUC=0.6385\n",
      "Trial 9 | Epoch 18: Loss=0.6603, Accuracy=0.6008, Precision=0.6026, Recall=0.5924, F1=0.5975, ROC_AUC=0.6391\n",
      "Trial 9 | Epoch 19: Loss=0.6642, Accuracy=0.5967, Precision=0.5915, Recall=0.6250, F1=0.6078, ROC_AUC=0.6347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 16:47:03,751] Trial 9 finished with value: 0.6013333333333334 and parameters: {'hidden_dim': 386, 'num_layers': 2, 'dropout': 0.488488315310723, 'lr': 9.555575411261533e-05}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 | Epoch 20: Loss=0.6592, Accuracy=0.6013, Precision=0.6113, Recall=0.5566, F1=0.5826, ROC_AUC=0.6435\n",
      "Trial 10 | Epoch 1: Loss=0.6842, Accuracy=0.5611, Precision=0.5634, Recall=0.5428, F1=0.5529, ROC_AUC=0.5781\n",
      "Trial 10 | Epoch 2: Loss=0.6824, Accuracy=0.5608, Precision=0.5429, Recall=0.7691, F1=0.6365, ROC_AUC=0.5932\n",
      "Trial 10 | Epoch 3: Loss=0.6786, Accuracy=0.5730, Precision=0.5701, Recall=0.5940, F1=0.5818, ROC_AUC=0.6013\n",
      "Trial 10 | Epoch 4: Loss=0.6774, Accuracy=0.5768, Precision=0.5574, Recall=0.7459, F1=0.6380, ROC_AUC=0.6041\n",
      "Trial 10 | Epoch 5: Loss=0.6743, Accuracy=0.5778, Precision=0.5863, Recall=0.5289, F1=0.5561, ROC_AUC=0.6112\n",
      "Trial 10 | Epoch 6: Loss=0.6736, Accuracy=0.5834, Precision=0.5871, Recall=0.5627, F1=0.5746, ROC_AUC=0.6153\n",
      "Trial 10 | Epoch 7: Loss=0.6696, Accuracy=0.5858, Precision=0.5806, Recall=0.6180, F1=0.5987, ROC_AUC=0.6194\n",
      "Trial 10 | Epoch 8: Loss=0.6715, Accuracy=0.5814, Precision=0.5957, Recall=0.5071, F1=0.5478, ROC_AUC=0.6201\n",
      "Trial 10 | Epoch 9: Loss=0.6634, Accuracy=0.5879, Precision=0.5881, Recall=0.5866, F1=0.5873, ROC_AUC=0.6316\n",
      "Trial 10 | Epoch 10: Loss=0.6633, Accuracy=0.5938, Precision=0.5973, Recall=0.5760, F1=0.5865, ROC_AUC=0.6399\n",
      "Trial 10 | Epoch 11: Loss=0.6582, Accuracy=0.5989, Precision=0.5871, Recall=0.6672, F1=0.6246, ROC_AUC=0.6413\n",
      "Trial 10 | Epoch 12: Loss=0.6521, Accuracy=0.6004, Precision=0.5940, Recall=0.6344, F1=0.6135, ROC_AUC=0.6494\n",
      "Trial 10 | Epoch 13: Loss=0.6617, Accuracy=0.6021, Precision=0.5983, Recall=0.6211, F1=0.6095, ROC_AUC=0.6487\n",
      "Trial 10 | Epoch 14: Loss=0.6452, Accuracy=0.6094, Precision=0.6123, Recall=0.5968, F1=0.6044, ROC_AUC=0.6643\n",
      "Trial 10 | Epoch 15: Loss=0.6418, Accuracy=0.6167, Precision=0.6130, Recall=0.6333, F1=0.6230, ROC_AUC=0.6702\n",
      "Trial 10 | Epoch 16: Loss=0.6372, Accuracy=0.6178, Precision=0.6049, Recall=0.6792, F1=0.6399, ROC_AUC=0.6774\n",
      "Trial 10 | Epoch 17: Loss=0.6253, Accuracy=0.6277, Precision=0.6248, Recall=0.6391, F1=0.6319, ROC_AUC=0.6922\n",
      "Trial 10 | Epoch 18: Loss=0.6182, Accuracy=0.6343, Precision=0.6330, Recall=0.6394, F1=0.6362, ROC_AUC=0.6995\n",
      "Trial 10 | Epoch 19: Loss=0.6125, Accuracy=0.6351, Precision=0.6336, Recall=0.6407, F1=0.6371, ROC_AUC=0.7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 16:53:20,616] Trial 10 finished with value: 0.6371111111111111 and parameters: {'hidden_dim': 67, 'num_layers': 4, 'dropout': 0.10326333827597398, 'lr': 0.00036965583764274383}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 | Epoch 20: Loss=0.6100, Accuracy=0.6371, Precision=0.6454, Recall=0.6087, F1=0.6265, ROC_AUC=0.7114\n",
      "Trial 11 | Epoch 1: Loss=0.6875, Accuracy=0.5423, Precision=0.5320, Recall=0.7034, F1=0.6058, ROC_AUC=0.5664\n",
      "Trial 11 | Epoch 2: Loss=0.6801, Accuracy=0.5697, Precision=0.5584, Recall=0.6660, F1=0.6075, ROC_AUC=0.5930\n",
      "Trial 11 | Epoch 3: Loss=0.6810, Accuracy=0.5626, Precision=0.5404, Recall=0.8361, F1=0.6565, ROC_AUC=0.6026\n",
      "Trial 11 | Epoch 4: Loss=0.6713, Accuracy=0.5898, Precision=0.5800, Recall=0.6510, F1=0.6134, ROC_AUC=0.6186\n",
      "Trial 11 | Epoch 5: Loss=0.6701, Accuracy=0.5843, Precision=0.5673, Recall=0.7100, F1=0.6307, ROC_AUC=0.6217\n",
      "Trial 11 | Epoch 6: Loss=0.6584, Accuracy=0.6034, Precision=0.5873, Recall=0.6962, F1=0.6371, ROC_AUC=0.6443\n",
      "Trial 11 | Epoch 7: Loss=0.6512, Accuracy=0.6083, Precision=0.5980, Recall=0.6613, F1=0.6280, ROC_AUC=0.6557\n",
      "Trial 11 | Epoch 8: Loss=0.6450, Accuracy=0.6147, Precision=0.6020, Recall=0.6772, F1=0.6374, ROC_AUC=0.6660\n",
      "Trial 11 | Epoch 9: Loss=0.6343, Accuracy=0.6196, Precision=0.6291, Recall=0.5830, F1=0.6052, ROC_AUC=0.6822\n",
      "Trial 11 | Epoch 10: Loss=0.6206, Accuracy=0.6384, Precision=0.6460, Recall=0.6127, F1=0.6289, ROC_AUC=0.7033\n",
      "Trial 11 | Epoch 11: Loss=0.6056, Accuracy=0.6477, Precision=0.6514, Recall=0.6354, F1=0.6433, ROC_AUC=0.7199\n",
      "Trial 11 | Epoch 12: Loss=0.5877, Accuracy=0.6647, Precision=0.6603, Recall=0.6783, F1=0.6692, ROC_AUC=0.7405\n",
      "Trial 11 | Epoch 13: Loss=0.5709, Accuracy=0.6746, Precision=0.6784, Recall=0.6640, F1=0.6711, ROC_AUC=0.7586\n",
      "Trial 11 | Epoch 14: Loss=0.5678, Accuracy=0.6724, Precision=0.7073, Recall=0.5881, F1=0.6422, ROC_AUC=0.7621\n",
      "Trial 11 | Epoch 15: Loss=0.5504, Accuracy=0.6866, Precision=0.7122, Recall=0.6263, F1=0.6665, ROC_AUC=0.7807\n",
      "Trial 11 | Epoch 16: Loss=0.5379, Accuracy=0.6893, Precision=0.6998, Recall=0.6629, F1=0.6809, ROC_AUC=0.7886\n",
      "Trial 11 | Epoch 17: Loss=0.5228, Accuracy=0.6951, Precision=0.7003, Recall=0.6821, F1=0.6911, ROC_AUC=0.7972\n",
      "Trial 11 | Epoch 18: Loss=0.5110, Accuracy=0.7047, Precision=0.7275, Recall=0.6546, F1=0.6891, ROC_AUC=0.8095\n",
      "Trial 11 | Epoch 19: Loss=0.5029, Accuracy=0.7128, Precision=0.7185, Recall=0.6997, F1=0.7090, ROC_AUC=0.8175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 17:01:27,789] Trial 11 finished with value: 0.7128333333333333 and parameters: {'hidden_dim': 110, 'num_layers': 4, 'dropout': 0.3232698147295837, 'lr': 0.0009368917991532698}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 | Epoch 20: Loss=0.5074, Accuracy=0.7128, Precision=0.7294, Recall=0.6767, F1=0.7021, ROC_AUC=0.8194\n",
      "Trial 12 | Epoch 1: Loss=0.6832, Accuracy=0.5588, Precision=0.5570, Recall=0.5749, F1=0.5658, ROC_AUC=0.5835\n",
      "Trial 12 | Epoch 2: Loss=0.6821, Accuracy=0.5707, Precision=0.5707, Recall=0.5710, F1=0.5708, ROC_AUC=0.5887\n",
      "Trial 12 | Epoch 3: Loss=0.6805, Accuracy=0.5697, Precision=0.5723, Recall=0.5512, F1=0.5616, ROC_AUC=0.5926\n",
      "Trial 12 | Epoch 4: Loss=0.6798, Accuracy=0.5720, Precision=0.5741, Recall=0.5579, F1=0.5659, ROC_AUC=0.5982\n",
      "Trial 12 | Epoch 5: Loss=0.6774, Accuracy=0.5731, Precision=0.5703, Recall=0.5929, F1=0.5814, ROC_AUC=0.6007\n",
      "Trial 12 | Epoch 6: Loss=0.6781, Accuracy=0.5746, Precision=0.5845, Recall=0.5159, F1=0.5480, ROC_AUC=0.6041\n",
      "Trial 12 | Epoch 7: Loss=0.6755, Accuracy=0.5787, Precision=0.5645, Recall=0.6890, F1=0.6206, ROC_AUC=0.6083\n",
      "Trial 12 | Epoch 8: Loss=0.6773, Accuracy=0.5780, Precision=0.5795, Recall=0.5688, F1=0.5741, ROC_AUC=0.6033\n",
      "Trial 12 | Epoch 9: Loss=0.6711, Accuracy=0.5887, Precision=0.5801, Recall=0.6422, F1=0.6096, ROC_AUC=0.6183\n",
      "Trial 12 | Epoch 10: Loss=0.6690, Accuracy=0.5896, Precision=0.5891, Recall=0.5922, F1=0.5906, ROC_AUC=0.6252\n",
      "Trial 12 | Epoch 11: Loss=0.6652, Accuracy=0.5908, Precision=0.5908, Recall=0.5904, F1=0.5906, ROC_AUC=0.6315\n",
      "Trial 12 | Epoch 12: Loss=0.6630, Accuracy=0.5928, Precision=0.5881, Recall=0.6199, F1=0.6036, ROC_AUC=0.6347\n",
      "Trial 12 | Epoch 13: Loss=0.6636, Accuracy=0.6008, Precision=0.5928, Recall=0.6442, F1=0.6174, ROC_AUC=0.6396\n",
      "Trial 12 | Epoch 14: Loss=0.6571, Accuracy=0.5976, Precision=0.5964, Recall=0.6033, F1=0.5999, ROC_AUC=0.6438\n",
      "Trial 12 | Epoch 15: Loss=0.6493, Accuracy=0.6104, Precision=0.5986, Recall=0.6707, F1=0.6326, ROC_AUC=0.6569\n",
      "Trial 12 | Epoch 16: Loss=0.6504, Accuracy=0.6075, Precision=0.6037, Recall=0.6259, F1=0.6146, ROC_AUC=0.6564\n",
      "Trial 12 | Epoch 17: Loss=0.6374, Accuracy=0.6228, Precision=0.6274, Recall=0.6047, F1=0.6158, ROC_AUC=0.6771\n",
      "Trial 12 | Epoch 18: Loss=0.6308, Accuracy=0.6268, Precision=0.6076, Recall=0.7164, F1=0.6575, ROC_AUC=0.6893\n",
      "Trial 12 | Epoch 19: Loss=0.6247, Accuracy=0.6356, Precision=0.6253, Recall=0.6763, F1=0.6498, ROC_AUC=0.6978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 17:08:48,446] Trial 12 finished with value: 0.6378888888888888 and parameters: {'hidden_dim': 89, 'num_layers': 4, 'dropout': 0.1976905937930369, 'lr': 0.000287735529974525}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 | Epoch 20: Loss=0.6164, Accuracy=0.6379, Precision=0.6405, Recall=0.6284, F1=0.6344, ROC_AUC=0.7054\n",
      "Trial 13 | Epoch 1: Loss=0.6848, Accuracy=0.5609, Precision=0.5564, Recall=0.6017, F1=0.5781, ROC_AUC=0.5790\n",
      "Trial 13 | Epoch 2: Loss=0.6808, Accuracy=0.5676, Precision=0.5622, Recall=0.6114, F1=0.5858, ROC_AUC=0.5955\n",
      "Trial 13 | Epoch 3: Loss=0.6827, Accuracy=0.5651, Precision=0.5707, Recall=0.5257, F1=0.5473, ROC_AUC=0.5894\n",
      "Trial 13 | Epoch 4: Loss=0.6756, Accuracy=0.5822, Precision=0.5834, Recall=0.5753, F1=0.5793, ROC_AUC=0.6113\n",
      "Trial 13 | Epoch 5: Loss=0.6718, Accuracy=0.5809, Precision=0.5910, Recall=0.5257, F1=0.5564, ROC_AUC=0.6202\n",
      "Trial 13 | Epoch 6: Loss=0.6709, Accuracy=0.5845, Precision=0.5942, Recall=0.5332, F1=0.5620, ROC_AUC=0.6240\n",
      "Trial 13 | Epoch 7: Loss=0.6651, Accuracy=0.5892, Precision=0.5861, Recall=0.6074, F1=0.5966, ROC_AUC=0.6320\n",
      "Trial 13 | Epoch 8: Loss=0.6626, Accuracy=0.5952, Precision=0.5811, Recall=0.6824, F1=0.6277, ROC_AUC=0.6386\n",
      "Trial 13 | Epoch 9: Loss=0.6534, Accuracy=0.6113, Precision=0.6070, Recall=0.6312, F1=0.6189, ROC_AUC=0.6565\n",
      "Trial 13 | Epoch 10: Loss=0.6410, Accuracy=0.6179, Precision=0.6116, Recall=0.6459, F1=0.6283, ROC_AUC=0.6713\n",
      "Trial 13 | Epoch 11: Loss=0.6300, Accuracy=0.6352, Precision=0.6421, Recall=0.6108, F1=0.6260, ROC_AUC=0.6912\n",
      "Trial 13 | Epoch 12: Loss=0.6198, Accuracy=0.6394, Precision=0.6325, Recall=0.6653, F1=0.6485, ROC_AUC=0.7049\n",
      "Trial 13 | Epoch 13: Loss=0.6101, Accuracy=0.6496, Precision=0.6554, Recall=0.6307, F1=0.6428, ROC_AUC=0.7179\n",
      "Trial 13 | Epoch 14: Loss=0.5975, Accuracy=0.6608, Precision=0.6918, Recall=0.5800, F1=0.6310, ROC_AUC=0.7385\n",
      "Trial 13 | Epoch 15: Loss=0.5782, Accuracy=0.6631, Precision=0.6665, Recall=0.6530, F1=0.6597, ROC_AUC=0.7482\n",
      "Trial 13 | Epoch 16: Loss=0.5632, Accuracy=0.6806, Precision=0.7072, Recall=0.6163, F1=0.6586, ROC_AUC=0.7682\n",
      "Trial 13 | Epoch 17: Loss=0.5501, Accuracy=0.6878, Precision=0.6934, Recall=0.6733, F1=0.6832, ROC_AUC=0.7776\n",
      "Trial 13 | Epoch 18: Loss=0.5374, Accuracy=0.6939, Precision=0.6941, Recall=0.6937, F1=0.6939, ROC_AUC=0.7881\n",
      "Trial 13 | Epoch 19: Loss=0.5287, Accuracy=0.6971, Precision=0.6931, Recall=0.7076, F1=0.7002, ROC_AUC=0.7970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 17:19:04,818] Trial 13 finished with value: 0.7057222222222223 and parameters: {'hidden_dim': 137, 'num_layers': 4, 'dropout': 0.29642669982561765, 'lr': 0.0009889238561060249}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 | Epoch 20: Loss=0.5112, Accuracy=0.7057, Precision=0.6978, Recall=0.7258, F1=0.7115, ROC_AUC=0.8118\n",
      "Trial 14 | Epoch 1: Loss=0.6894, Accuracy=0.5318, Precision=0.5327, Recall=0.5173, F1=0.5249, ROC_AUC=0.5504\n",
      "Trial 14 | Epoch 2: Loss=0.6902, Accuracy=0.5492, Precision=0.5505, Recall=0.5360, F1=0.5432, ROC_AUC=0.5702\n",
      "Trial 14 | Epoch 3: Loss=0.6847, Accuracy=0.5538, Precision=0.5570, Recall=0.5259, F1=0.5410, ROC_AUC=0.5768\n",
      "Trial 14 | Epoch 4: Loss=0.6841, Accuracy=0.5558, Precision=0.5499, Recall=0.6158, F1=0.5810, ROC_AUC=0.5800\n",
      "Trial 14 | Epoch 5: Loss=0.6821, Accuracy=0.5627, Precision=0.5585, Recall=0.5981, F1=0.5776, ROC_AUC=0.5857\n",
      "Trial 14 | Epoch 6: Loss=0.6822, Accuracy=0.5658, Precision=0.5631, Recall=0.5878, F1=0.5752, ROC_AUC=0.5883\n",
      "Trial 14 | Epoch 7: Loss=0.6817, Accuracy=0.5647, Precision=0.5722, Recall=0.5128, F1=0.5408, ROC_AUC=0.5897\n",
      "Trial 14 | Epoch 8: Loss=0.6807, Accuracy=0.5678, Precision=0.5605, Recall=0.6286, F1=0.5926, ROC_AUC=0.5913\n",
      "Trial 14 | Epoch 9: Loss=0.6809, Accuracy=0.5674, Precision=0.5578, Recall=0.6503, F1=0.6005, ROC_AUC=0.5907\n",
      "Trial 14 | Epoch 10: Loss=0.6812, Accuracy=0.5681, Precision=0.5599, Recall=0.6369, F1=0.5959, ROC_AUC=0.5913\n",
      "Trial 14 | Epoch 11: Loss=0.6803, Accuracy=0.5681, Precision=0.5581, Recall=0.6547, F1=0.6025, ROC_AUC=0.5928\n",
      "Trial 14 | Epoch 12: Loss=0.6802, Accuracy=0.5700, Precision=0.5654, Recall=0.6054, F1=0.5847, ROC_AUC=0.5929\n",
      "Trial 14 | Epoch 13: Loss=0.6804, Accuracy=0.5671, Precision=0.5772, Recall=0.5011, F1=0.5365, ROC_AUC=0.5938\n",
      "Trial 14 | Epoch 14: Loss=0.6797, Accuracy=0.5698, Precision=0.5657, Recall=0.6009, F1=0.5828, ROC_AUC=0.5938\n",
      "Trial 14 | Epoch 15: Loss=0.6797, Accuracy=0.5689, Precision=0.5661, Recall=0.5901, F1=0.5778, ROC_AUC=0.5944\n",
      "Trial 14 | Epoch 16: Loss=0.6790, Accuracy=0.5722, Precision=0.5721, Recall=0.5728, F1=0.5724, ROC_AUC=0.5954\n",
      "Trial 14 | Epoch 17: Loss=0.6801, Accuracy=0.5691, Precision=0.5639, Recall=0.6093, F1=0.5857, ROC_AUC=0.5942\n",
      "Trial 14 | Epoch 18: Loss=0.6794, Accuracy=0.5684, Precision=0.5710, Recall=0.5501, F1=0.5604, ROC_AUC=0.5956\n",
      "Trial 14 | Epoch 19: Loss=0.6794, Accuracy=0.5668, Precision=0.5689, Recall=0.5516, F1=0.5601, ROC_AUC=0.5949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 18:00:29,843] Trial 14 finished with value: 0.5701666666666667 and parameters: {'hidden_dim': 365, 'num_layers': 5, 'dropout': 0.16332510856753238, 'lr': 1.1635313846862305e-05}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 | Epoch 20: Loss=0.6794, Accuracy=0.5702, Precision=0.5715, Recall=0.5611, F1=0.5662, ROC_AUC=0.5957\n",
      "Trial 15 | Epoch 1: Loss=0.6832, Accuracy=0.5661, Precision=0.5570, Recall=0.6461, F1=0.5983, ROC_AUC=0.5867\n",
      "Trial 15 | Epoch 2: Loss=0.6818, Accuracy=0.5726, Precision=0.5862, Recall=0.4933, F1=0.5358, ROC_AUC=0.5974\n",
      "Trial 15 | Epoch 3: Loss=0.6774, Accuracy=0.5747, Precision=0.5819, Recall=0.5307, F1=0.5551, ROC_AUC=0.6030\n",
      "Trial 15 | Epoch 4: Loss=0.6741, Accuracy=0.5814, Precision=0.5856, Recall=0.5570, F1=0.5710, ROC_AUC=0.6122\n",
      "Trial 15 | Epoch 5: Loss=0.6725, Accuracy=0.5848, Precision=0.5763, Recall=0.6408, F1=0.6068, ROC_AUC=0.6153\n",
      "Trial 15 | Epoch 6: Loss=0.6694, Accuracy=0.5855, Precision=0.5866, Recall=0.5792, F1=0.5829, ROC_AUC=0.6227\n",
      "Trial 15 | Epoch 7: Loss=0.6635, Accuracy=0.5933, Precision=0.5842, Recall=0.6469, F1=0.6140, ROC_AUC=0.6352\n",
      "Trial 15 | Epoch 8: Loss=0.6652, Accuracy=0.5962, Precision=0.5955, Recall=0.5999, F1=0.5977, ROC_AUC=0.6311\n",
      "Trial 15 | Epoch 9: Loss=0.6571, Accuracy=0.6072, Precision=0.6117, Recall=0.5873, F1=0.5993, ROC_AUC=0.6485\n",
      "Trial 15 | Epoch 10: Loss=0.6551, Accuracy=0.6068, Precision=0.6119, Recall=0.5841, F1=0.5977, ROC_AUC=0.6542\n",
      "Trial 15 | Epoch 11: Loss=0.6456, Accuracy=0.6245, Precision=0.6323, Recall=0.5949, F1=0.6130, ROC_AUC=0.6726\n",
      "Trial 15 | Epoch 12: Loss=0.6380, Accuracy=0.6311, Precision=0.6208, Recall=0.6736, F1=0.6461, ROC_AUC=0.6825\n",
      "Trial 15 | Epoch 13: Loss=0.6278, Accuracy=0.6349, Precision=0.6367, Recall=0.6283, F1=0.6325, ROC_AUC=0.6946\n",
      "Trial 15 | Epoch 14: Loss=0.6242, Accuracy=0.6373, Precision=0.6411, Recall=0.6237, F1=0.6323, ROC_AUC=0.6994\n",
      "Trial 15 | Epoch 15: Loss=0.6128, Accuracy=0.6493, Precision=0.6464, Recall=0.6594, F1=0.6528, ROC_AUC=0.7161\n",
      "Trial 15 | Epoch 16: Loss=0.6016, Accuracy=0.6603, Precision=0.6585, Recall=0.6662, F1=0.6623, ROC_AUC=0.7299\n",
      "Trial 15 | Epoch 17: Loss=0.5947, Accuracy=0.6639, Precision=0.6794, Recall=0.6207, F1=0.6487, ROC_AUC=0.7394\n",
      "Trial 15 | Epoch 18: Loss=0.5883, Accuracy=0.6709, Precision=0.6586, Recall=0.7099, F1=0.6833, ROC_AUC=0.7480\n",
      "Trial 15 | Epoch 19: Loss=0.5806, Accuracy=0.6707, Precision=0.6624, Recall=0.6960, F1=0.6788, ROC_AUC=0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 18:09:36,596] Trial 15 finished with value: 0.68 and parameters: {'hidden_dim': 250, 'num_layers': 2, 'dropout': 0.24531113953765643, 'lr': 0.0003226279679688342}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 | Epoch 20: Loss=0.5735, Accuracy=0.6800, Precision=0.6788, Recall=0.6833, F1=0.6811, ROC_AUC=0.7633\n",
      "Trial 16 | Epoch 1: Loss=0.6857, Accuracy=0.5617, Precision=0.5619, Recall=0.5601, F1=0.5610, ROC_AUC=0.5805\n",
      "Trial 16 | Epoch 2: Loss=0.6777, Accuracy=0.5717, Precision=0.5653, Recall=0.6208, F1=0.5917, ROC_AUC=0.6026\n",
      "Trial 16 | Epoch 3: Loss=0.6746, Accuracy=0.5786, Precision=0.5712, Recall=0.6308, F1=0.5995, ROC_AUC=0.6091\n",
      "Trial 16 | Epoch 4: Loss=0.6722, Accuracy=0.5862, Precision=0.5897, Recall=0.5663, F1=0.5778, ROC_AUC=0.6222\n",
      "Trial 16 | Epoch 5: Loss=0.6698, Accuracy=0.5882, Precision=0.5925, Recall=0.5647, F1=0.5783, ROC_AUC=0.6235\n",
      "Trial 16 | Epoch 6: Loss=0.6643, Accuracy=0.5948, Precision=0.5860, Recall=0.6456, F1=0.6144, ROC_AUC=0.6356\n",
      "Trial 16 | Epoch 7: Loss=0.6588, Accuracy=0.5999, Precision=0.5801, Recall=0.7234, F1=0.6439, ROC_AUC=0.6480\n",
      "Trial 16 | Epoch 8: Loss=0.6515, Accuracy=0.6061, Precision=0.6035, Recall=0.6186, F1=0.6110, ROC_AUC=0.6567\n",
      "Trial 16 | Epoch 9: Loss=0.6447, Accuracy=0.6211, Precision=0.6149, Recall=0.6482, F1=0.6311, ROC_AUC=0.6699\n",
      "Trial 16 | Epoch 10: Loss=0.6408, Accuracy=0.6194, Precision=0.6101, Recall=0.6616, F1=0.6348, ROC_AUC=0.6768\n",
      "Trial 16 | Epoch 11: Loss=0.6274, Accuracy=0.6397, Precision=0.6294, Recall=0.6798, F1=0.6536, ROC_AUC=0.6960\n",
      "Trial 16 | Epoch 12: Loss=0.6160, Accuracy=0.6419, Precision=0.6498, Recall=0.6153, F1=0.6321, ROC_AUC=0.7099\n",
      "Trial 16 | Epoch 13: Loss=0.6024, Accuracy=0.6536, Precision=0.6415, Recall=0.6964, F1=0.6678, ROC_AUC=0.7258\n",
      "Trial 16 | Epoch 14: Loss=0.5952, Accuracy=0.6539, Precision=0.6737, Recall=0.5970, F1=0.6330, ROC_AUC=0.7351\n",
      "Trial 16 | Epoch 15: Loss=0.5676, Accuracy=0.6737, Precision=0.6775, Recall=0.6629, F1=0.6701, ROC_AUC=0.7607\n",
      "Trial 16 | Epoch 16: Loss=0.5576, Accuracy=0.6815, Precision=0.6774, Recall=0.6930, F1=0.6851, ROC_AUC=0.7727\n",
      "Trial 16 | Epoch 17: Loss=0.5418, Accuracy=0.6888, Precision=0.6934, Recall=0.6770, F1=0.6851, ROC_AUC=0.7832\n",
      "Trial 16 | Epoch 18: Loss=0.5352, Accuracy=0.6945, Precision=0.7034, Recall=0.6726, F1=0.6876, ROC_AUC=0.7927\n",
      "Trial 16 | Epoch 19: Loss=0.5273, Accuracy=0.7001, Precision=0.6986, Recall=0.7039, F1=0.7012, ROC_AUC=0.8004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 18:19:28,547] Trial 16 finished with value: 0.7028333333333333 and parameters: {'hidden_dim': 126, 'num_layers': 4, 'dropout': 0.3312634085574883, 'lr': 0.00047392378260642645}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 | Epoch 20: Loss=0.5155, Accuracy=0.7028, Precision=0.7061, Recall=0.6950, F1=0.7005, ROC_AUC=0.8103\n",
      "Trial 17 | Epoch 1: Loss=0.6823, Accuracy=0.5629, Precision=0.5706, Recall=0.5080, F1=0.5375, ROC_AUC=0.5895\n",
      "Trial 17 | Epoch 2: Loss=0.6811, Accuracy=0.5708, Precision=0.5695, Recall=0.5807, F1=0.5750, ROC_AUC=0.5912\n",
      "Trial 17 | Epoch 3: Loss=0.6795, Accuracy=0.5713, Precision=0.5748, Recall=0.5481, F1=0.5611, ROC_AUC=0.6018\n",
      "Trial 17 | Epoch 4: Loss=0.6739, Accuracy=0.5777, Precision=0.5705, Recall=0.6288, F1=0.5982, ROC_AUC=0.6126\n",
      "Trial 17 | Epoch 5: Loss=0.6715, Accuracy=0.5827, Precision=0.5793, Recall=0.6046, F1=0.5916, ROC_AUC=0.6163\n",
      "Trial 17 | Epoch 6: Loss=0.6716, Accuracy=0.5862, Precision=0.5935, Recall=0.5472, F1=0.5694, ROC_AUC=0.6191\n",
      "Trial 17 | Epoch 7: Loss=0.6680, Accuracy=0.5886, Precision=0.5893, Recall=0.5844, F1=0.5869, ROC_AUC=0.6247\n",
      "Trial 17 | Epoch 8: Loss=0.6685, Accuracy=0.5902, Precision=0.5884, Recall=0.6002, F1=0.5942, ROC_AUC=0.6231\n",
      "Trial 17 | Epoch 9: Loss=0.6653, Accuracy=0.5964, Precision=0.5858, Recall=0.6581, F1=0.6199, ROC_AUC=0.6332\n",
      "Trial 17 | Epoch 10: Loss=0.6659, Accuracy=0.5908, Precision=0.5940, Recall=0.5734, F1=0.5836, ROC_AUC=0.6271\n",
      "Trial 17 | Epoch 11: Loss=0.6562, Accuracy=0.6099, Precision=0.6059, Recall=0.6286, F1=0.6170, ROC_AUC=0.6494\n",
      "Trial 17 | Epoch 12: Loss=0.6556, Accuracy=0.6087, Precision=0.6159, Recall=0.5779, F1=0.5963, ROC_AUC=0.6508\n",
      "Trial 17 | Epoch 13: Loss=0.6515, Accuracy=0.6142, Precision=0.6110, Recall=0.6286, F1=0.6196, ROC_AUC=0.6589\n",
      "Trial 17 | Epoch 14: Loss=0.6481, Accuracy=0.6143, Precision=0.6391, Recall=0.5251, F1=0.5765, ROC_AUC=0.6665\n",
      "Trial 17 | Epoch 15: Loss=0.6441, Accuracy=0.6213, Precision=0.6122, Recall=0.6616, F1=0.6359, ROC_AUC=0.6707\n",
      "Trial 17 | Epoch 16: Loss=0.6398, Accuracy=0.6243, Precision=0.6186, Recall=0.6483, F1=0.6331, ROC_AUC=0.6773\n",
      "Trial 17 | Epoch 17: Loss=0.6295, Accuracy=0.6328, Precision=0.6280, Recall=0.6516, F1=0.6396, ROC_AUC=0.6910\n",
      "Trial 17 | Epoch 18: Loss=0.6283, Accuracy=0.6322, Precision=0.6450, Recall=0.5879, F1=0.6151, ROC_AUC=0.6939\n",
      "Trial 17 | Epoch 19: Loss=0.6179, Accuracy=0.6386, Precision=0.6402, Recall=0.6329, F1=0.6365, ROC_AUC=0.7054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 18:36:00,027] Trial 17 finished with value: 0.649 and parameters: {'hidden_dim': 381, 'num_layers': 2, 'dropout': 0.26363972056833956, 'lr': 0.00017054787332927137}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 | Epoch 20: Loss=0.6094, Accuracy=0.6490, Precision=0.6629, Recall=0.6062, F1=0.6333, ROC_AUC=0.7187\n",
      "Trial 18 | Epoch 1: Loss=0.6920, Accuracy=0.5311, Precision=0.5208, Recall=0.7760, F1=0.6233, ROC_AUC=0.5569\n",
      "Trial 18 | Epoch 2: Loss=0.6887, Accuracy=0.5347, Precision=0.5421, Recall=0.4461, F1=0.4895, ROC_AUC=0.5535\n",
      "Trial 18 | Epoch 3: Loss=0.6889, Accuracy=0.5309, Precision=0.5211, Recall=0.7646, F1=0.6197, ROC_AUC=0.5514\n",
      "Trial 18 | Epoch 4: Loss=0.6889, Accuracy=0.5348, Precision=0.5530, Recall=0.3632, F1=0.4385, ROC_AUC=0.5536\n",
      "Trial 18 | Epoch 5: Loss=0.6910, Accuracy=0.5344, Precision=0.5242, Recall=0.7466, F1=0.6159, ROC_AUC=0.5499\n",
      "Trial 18 | Epoch 6: Loss=0.6921, Accuracy=0.5224, Precision=0.5401, Recall=0.3018, F1=0.3872, ROC_AUC=0.5239\n",
      "Trial 18 | Epoch 7: Loss=0.6919, Accuracy=0.5219, Precision=0.5459, Recall=0.2612, F1=0.3533, ROC_AUC=0.5229\n",
      "Trial 18 | Epoch 8: Loss=0.6913, Accuracy=0.5270, Precision=0.5505, Recall=0.2944, F1=0.3837, ROC_AUC=0.5232\n",
      "Trial 18 | Epoch 9: Loss=0.6910, Accuracy=0.5206, Precision=0.5318, Recall=0.3442, F1=0.4179, ROC_AUC=0.5252\n",
      "Trial 18 | Epoch 10: Loss=0.6914, Accuracy=0.5255, Precision=0.5501, Recall=0.2799, F1=0.3710, ROC_AUC=0.5219\n",
      "Trial 18 | Epoch 11: Loss=0.6910, Accuracy=0.5254, Precision=0.5614, Recall=0.2327, F1=0.3290, ROC_AUC=0.5268\n",
      "Trial 18 | Epoch 12: Loss=0.6918, Accuracy=0.5257, Precision=0.5561, Recall=0.2548, F1=0.3495, ROC_AUC=0.5337\n",
      "Trial 18 | Epoch 13: Loss=0.6910, Accuracy=0.5256, Precision=0.5628, Recall=0.2297, F1=0.3262, ROC_AUC=0.5261\n",
      "Trial 18 | Epoch 14: Loss=0.6908, Accuracy=0.5242, Precision=0.5658, Recall=0.2082, F1=0.3044, ROC_AUC=0.5290\n",
      "Trial 18 | Epoch 15: Loss=0.6909, Accuracy=0.5265, Precision=0.5571, Recall=0.2586, F1=0.3532, ROC_AUC=0.5281\n",
      "Trial 18 | Epoch 16: Loss=0.6910, Accuracy=0.5238, Precision=0.5497, Recall=0.2638, F1=0.3565, ROC_AUC=0.5270\n",
      "Trial 18 | Epoch 17: Loss=0.6911, Accuracy=0.5249, Precision=0.5564, Recall=0.2462, F1=0.3414, ROC_AUC=0.5325\n",
      "Trial 18 | Epoch 18: Loss=0.6909, Accuracy=0.5247, Precision=0.5774, Recall=0.1840, F1=0.2791, ROC_AUC=0.5236\n",
      "Trial 18 | Epoch 19: Loss=0.6908, Accuracy=0.5242, Precision=0.5788, Recall=0.1780, F1=0.2723, ROC_AUC=0.5436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 19:05:20,097] Trial 18 finished with value: 0.5242777777777777 and parameters: {'hidden_dim': 271, 'num_layers': 5, 'dropout': 0.3492031506606737, 'lr': 0.0005742043827273779}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 | Epoch 20: Loss=0.6911, Accuracy=0.5243, Precision=0.5711, Recall=0.1950, F1=0.2907, ROC_AUC=0.5232\n",
      "Trial 19 | Epoch 1: Loss=0.6895, Accuracy=0.5441, Precision=0.5500, Recall=0.4853, F1=0.5156, ROC_AUC=0.5579\n",
      "Trial 19 | Epoch 2: Loss=0.6814, Accuracy=0.5698, Precision=0.5671, Recall=0.5906, F1=0.5786, ROC_AUC=0.5914\n",
      "Trial 19 | Epoch 3: Loss=0.6759, Accuracy=0.5845, Precision=0.5848, Recall=0.5826, F1=0.5837, ROC_AUC=0.6138\n",
      "Trial 19 | Epoch 4: Loss=0.6784, Accuracy=0.5773, Precision=0.5798, Recall=0.5621, F1=0.5708, ROC_AUC=0.6065\n",
      "Trial 19 | Epoch 5: Loss=0.6752, Accuracy=0.5836, Precision=0.5902, Recall=0.5467, F1=0.5676, ROC_AUC=0.6150\n",
      "Trial 19 | Epoch 6: Loss=0.6603, Accuracy=0.6009, Precision=0.5836, Recall=0.7040, F1=0.6382, ROC_AUC=0.6477\n",
      "Trial 19 | Epoch 7: Loss=0.6514, Accuracy=0.6114, Precision=0.6067, Recall=0.6337, F1=0.6199, ROC_AUC=0.6601\n",
      "Trial 19 | Epoch 8: Loss=0.6429, Accuracy=0.6187, Precision=0.6315, Recall=0.5698, F1=0.5991, ROC_AUC=0.6754\n",
      "Trial 19 | Epoch 9: Loss=0.6339, Accuracy=0.6279, Precision=0.6087, Recall=0.7166, F1=0.6582, ROC_AUC=0.6857\n",
      "Trial 19 | Epoch 10: Loss=0.6183, Accuracy=0.6420, Precision=0.6261, Recall=0.7052, F1=0.6633, ROC_AUC=0.7083\n",
      "Trial 19 | Epoch 11: Loss=0.6016, Accuracy=0.6479, Precision=0.6383, Recall=0.6827, F1=0.6597, ROC_AUC=0.7250\n",
      "Trial 19 | Epoch 12: Loss=0.5750, Accuracy=0.6709, Precision=0.6786, Recall=0.6493, F1=0.6636, ROC_AUC=0.7530\n",
      "Trial 19 | Epoch 13: Loss=0.5577, Accuracy=0.6808, Precision=0.6827, Recall=0.6754, F1=0.6791, ROC_AUC=0.7708\n",
      "Trial 19 | Epoch 14: Loss=0.5490, Accuracy=0.6932, Precision=0.6970, Recall=0.6837, F1=0.6903, ROC_AUC=0.7863\n",
      "Trial 19 | Epoch 15: Loss=0.5198, Accuracy=0.7063, Precision=0.7152, Recall=0.6857, F1=0.7001, ROC_AUC=0.8072\n",
      "Trial 19 | Epoch 16: Loss=0.5125, Accuracy=0.7074, Precision=0.6924, Recall=0.7464, F1=0.7184, ROC_AUC=0.8146\n",
      "Trial 19 | Epoch 17: Loss=0.4991, Accuracy=0.7168, Precision=0.7073, Recall=0.7398, F1=0.7232, ROC_AUC=0.8265\n",
      "Trial 19 | Epoch 18: Loss=0.4901, Accuracy=0.7209, Precision=0.7281, Recall=0.7051, F1=0.7164, ROC_AUC=0.8349\n",
      "Trial 19 | Epoch 19: Loss=0.4855, Accuracy=0.7244, Precision=0.7320, Recall=0.7079, F1=0.7198, ROC_AUC=0.8411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 19:51:49,197] Trial 19 finished with value: 0.7248333333333333 and parameters: {'hidden_dim': 444, 'num_layers': 4, 'dropout': 0.16293348166846625, 'lr': 0.00020987595426412333}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 | Epoch 20: Loss=0.4958, Accuracy=0.7248, Precision=0.7280, Recall=0.7179, F1=0.7229, ROC_AUC=0.8433\n",
      "Trial 20 | Epoch 1: Loss=0.6824, Accuracy=0.5688, Precision=0.5719, Recall=0.5469, F1=0.5591, ROC_AUC=0.5885\n",
      "Trial 20 | Epoch 2: Loss=0.6803, Accuracy=0.5693, Precision=0.5662, Recall=0.5926, F1=0.5791, ROC_AUC=0.5995\n",
      "Trial 20 | Epoch 3: Loss=0.6764, Accuracy=0.5781, Precision=0.5764, Recall=0.5893, F1=0.5828, ROC_AUC=0.6079\n",
      "Trial 20 | Epoch 4: Loss=0.6747, Accuracy=0.5763, Precision=0.5693, Recall=0.6263, F1=0.5965, ROC_AUC=0.6092\n",
      "Trial 20 | Epoch 5: Loss=0.6692, Accuracy=0.5892, Precision=0.5905, Recall=0.5817, F1=0.5861, ROC_AUC=0.6224\n",
      "Trial 20 | Epoch 6: Loss=0.6700, Accuracy=0.5862, Precision=0.5962, Recall=0.5340, F1=0.5634, ROC_AUC=0.6219\n",
      "Trial 20 | Epoch 7: Loss=0.6661, Accuracy=0.5959, Precision=0.5954, Recall=0.5987, F1=0.5970, ROC_AUC=0.6329\n",
      "Trial 20 | Epoch 8: Loss=0.6627, Accuracy=0.5972, Precision=0.6098, Recall=0.5399, F1=0.5727, ROC_AUC=0.6384\n",
      "Trial 20 | Epoch 9: Loss=0.6467, Accuracy=0.6129, Precision=0.6167, Recall=0.5967, F1=0.6065, ROC_AUC=0.6651\n",
      "Trial 20 | Epoch 10: Loss=0.6428, Accuracy=0.6182, Precision=0.6317, Recall=0.5669, F1=0.5976, ROC_AUC=0.6725\n",
      "Trial 20 | Epoch 11: Loss=0.6313, Accuracy=0.6299, Precision=0.6247, Recall=0.6508, F1=0.6375, ROC_AUC=0.6892\n",
      "Trial 20 | Epoch 12: Loss=0.6160, Accuracy=0.6475, Precision=0.6461, Recall=0.6523, F1=0.6492, ROC_AUC=0.7116\n",
      "Trial 20 | Epoch 13: Loss=0.6020, Accuracy=0.6586, Precision=0.6699, Recall=0.6252, F1=0.6468, ROC_AUC=0.7305\n",
      "Trial 20 | Epoch 14: Loss=0.5707, Accuracy=0.6760, Precision=0.6742, Recall=0.6811, F1=0.6776, ROC_AUC=0.7610\n",
      "Trial 20 | Epoch 15: Loss=0.5561, Accuracy=0.6879, Precision=0.6921, Recall=0.6769, F1=0.6844, ROC_AUC=0.7767\n",
      "Trial 20 | Epoch 16: Loss=0.5446, Accuracy=0.6956, Precision=0.6911, Recall=0.7073, F1=0.6991, ROC_AUC=0.7903\n",
      "Trial 20 | Epoch 17: Loss=0.5291, Accuracy=0.7011, Precision=0.7003, Recall=0.7029, F1=0.7016, ROC_AUC=0.8029\n",
      "Trial 20 | Epoch 18: Loss=0.5232, Accuracy=0.7067, Precision=0.7161, Recall=0.6848, F1=0.7001, ROC_AUC=0.8109\n",
      "Trial 20 | Epoch 19: Loss=0.5129, Accuracy=0.7132, Precision=0.7160, Recall=0.7069, F1=0.7114, ROC_AUC=0.8207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 20:28:40,177] Trial 20 finished with value: 0.7145 and parameters: {'hidden_dim': 455, 'num_layers': 3, 'dropout': 0.10927523766611896, 'lr': 0.00018405740979625078}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 | Epoch 20: Loss=0.5029, Accuracy=0.7145, Precision=0.7251, Recall=0.6909, F1=0.7076, ROC_AUC=0.8284\n",
      "Trial 21 | Epoch 1: Loss=0.6844, Accuracy=0.5652, Precision=0.5571, Recall=0.6362, F1=0.5940, ROC_AUC=0.5833\n",
      "Trial 21 | Epoch 2: Loss=0.6764, Accuracy=0.5744, Precision=0.5859, Recall=0.5079, F1=0.5441, ROC_AUC=0.6071\n",
      "Trial 21 | Epoch 3: Loss=0.6764, Accuracy=0.5758, Precision=0.5893, Recall=0.5007, F1=0.5414, ROC_AUC=0.6068\n",
      "Trial 21 | Epoch 4: Loss=0.6730, Accuracy=0.5878, Precision=0.5793, Recall=0.6409, F1=0.6086, ROC_AUC=0.6181\n",
      "Trial 21 | Epoch 5: Loss=0.6702, Accuracy=0.5857, Precision=0.5964, Recall=0.5300, F1=0.5612, ROC_AUC=0.6235\n",
      "Trial 21 | Epoch 6: Loss=0.6674, Accuracy=0.5889, Precision=0.5943, Recall=0.5600, F1=0.5767, ROC_AUC=0.6264\n",
      "Trial 21 | Epoch 7: Loss=0.6643, Accuracy=0.5978, Precision=0.5980, Recall=0.5964, F1=0.5972, ROC_AUC=0.6351\n",
      "Trial 21 | Epoch 8: Loss=0.6594, Accuracy=0.6012, Precision=0.6029, Recall=0.5929, F1=0.5979, ROC_AUC=0.6427\n",
      "Trial 21 | Epoch 9: Loss=0.6576, Accuracy=0.6031, Precision=0.6085, Recall=0.5778, F1=0.5928, ROC_AUC=0.6485\n",
      "Trial 21 | Epoch 10: Loss=0.6464, Accuracy=0.6169, Precision=0.6253, Recall=0.5838, F1=0.6038, ROC_AUC=0.6682\n",
      "Trial 21 | Epoch 11: Loss=0.6460, Accuracy=0.6158, Precision=0.6414, Recall=0.5251, F1=0.5775, ROC_AUC=0.6729\n",
      "Trial 21 | Epoch 12: Loss=0.6307, Accuracy=0.6294, Precision=0.6412, Recall=0.5876, F1=0.6132, ROC_AUC=0.6927\n",
      "Trial 21 | Epoch 13: Loss=0.6214, Accuracy=0.6411, Precision=0.6414, Recall=0.6401, F1=0.6408, ROC_AUC=0.7048\n",
      "Trial 21 | Epoch 14: Loss=0.6139, Accuracy=0.6451, Precision=0.6522, Recall=0.6217, F1=0.6366, ROC_AUC=0.7147\n",
      "Trial 21 | Epoch 15: Loss=0.6052, Accuracy=0.6570, Precision=0.6598, Recall=0.6483, F1=0.6540, ROC_AUC=0.7274\n",
      "Trial 21 | Epoch 16: Loss=0.5893, Accuracy=0.6626, Precision=0.6711, Recall=0.6377, F1=0.6539, ROC_AUC=0.7448\n",
      "Trial 21 | Epoch 17: Loss=0.5702, Accuracy=0.6759, Precision=0.6806, Recall=0.6631, F1=0.6717, ROC_AUC=0.7631\n",
      "Trial 21 | Epoch 18: Loss=0.5537, Accuracy=0.6844, Precision=0.6892, Recall=0.6719, F1=0.6804, ROC_AUC=0.7784\n",
      "Trial 21 | Epoch 19: Loss=0.5452, Accuracy=0.6929, Precision=0.6750, Recall=0.7442, F1=0.7079, ROC_AUC=0.7891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 21:01:48,600] Trial 21 finished with value: 0.6930555555555555 and parameters: {'hidden_dim': 443, 'num_layers': 3, 'dropout': 0.10537963085428154, 'lr': 0.00014706322188363607}. Best is trial 4 with value: 0.7318888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 | Epoch 20: Loss=0.5427, Accuracy=0.6931, Precision=0.6999, Recall=0.6759, F1=0.6877, ROC_AUC=0.7958\n",
      "Trial 22 | Epoch 1: Loss=0.6940, Accuracy=0.5460, Precision=0.5337, Recall=0.7290, F1=0.6162, ROC_AUC=0.5753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-16 21:04:13,750] Trial 22 failed with parameters: {'hidden_dim': 507, 'num_layers': 3, 'dropout': 0.167456972037691, 'lr': 0.00018031239600952288} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_24628\\939395415.py\", line 148, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test, train_loader, test_loader, device), n_trials=50)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_24628\\939395415.py\", line 105, in objective\n",
      "    train_model(model, train_loader, criterion, optimizer, device)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_24628\\939395415.py\", line 53, in train_model\n",
      "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-16 21:04:13,774] Trial 22 failed with value None.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE  # 用于类别不平衡\n",
    "\n",
    "# 定义双向 LSTM 模型\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers, dropout, output_dim):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # 双向 LSTM 的输出维度是 hidden_dim * 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # 取最后一个时间步的输出\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df, target_column, sequence_length):\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "    \n",
    "    # 处理类别不平衡\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # 将数据转换为 LSTM 的输入格式 (batch_size, sequence_length, input_size)\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    if num_features % sequence_length != 0:\n",
    "        print(num_features)\n",
    "        raise ValueError(\"特征数不能被序列长度整除，请调整 sequence_length。\")\n",
    "    input_size = num_features // sequence_length\n",
    "    X = X.reshape(num_samples, sequence_length, input_size)\n",
    "    return X, y\n",
    "\n",
    "# 模型训练函数\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 模型评估函数\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]  # 取正类的概率\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0\n",
    "    return val_loss / len(test_loader), accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "# Optuna 目标函数\n",
    "def objective(trial, X_train, y_train, X_test, y_test, train_loader, test_loader, device):\n",
    "    global best_accuracy, best_model_state\n",
    "\n",
    "    # 超参数搜索空间\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 64, 512)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    # 模型初始化\n",
    "    model = BiLSTMModel(input_size=X_train.shape[2], hidden_dim=hidden_dim, num_layers=num_layers, \n",
    "                        dropout=dropout, output_dim=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 模型训练和评估\n",
    "    for epoch in range(20):  # 每次试验训练 20 个 epoch\n",
    "        train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, accuracy, precision, recall, f1, roc_auc = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "        # 打印每轮指标\n",
    "        print(f\"Trial {trial.number} | Epoch {epoch + 1}: Loss={val_loss:.4f}, \"\n",
    "              f\"Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, \"\n",
    "              f\"F1={f1:.4f}, ROC_AUC={roc_auc:.4f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    global train_loader, test_loader, device, best_accuracy, best_model_state\n",
    "\n",
    "    # 初始化全局变量\n",
    "    best_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    # 加载数据\n",
    "    target_column = 'match_flag'\n",
    "    sequence_length = 34  # 请根据特征数调整此参数\n",
    "    df = pd.read_csv('cleaned_aki_data3.csv')\n",
    "    X, y = preprocess_data(df, target_column, sequence_length)\n",
    "\n",
    "    # 检查预处理后的数据形状\n",
    "    print(\"X shape after preprocessing:\", X.shape)  # 应为 (num_samples, sequence_length, input_size)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # 数据加载器\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 贝叶斯优化\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test, train_loader, test_loader, device), n_trials=50)\n",
    "\n",
    "    # 输出最佳参数\n",
    "    print(\"Best parameters:\", study.best_params)\n",
    "    print(\"Best accuracy:\", best_accuracy)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    torch.save(best_model_state, \"best_bilstm_model.pth\")\n",
    "    print(\"Best model saved as 'best_bilstm_model.pth'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3218b2e-80be-4db5-be4a-97b00a3d440a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3aebcc-2b24-4f30-bf77-cb770bec7d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ReduceLROnPlateau' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 177\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 162\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m    161\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mReduceLROnPlateau\u001b[49m(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# 学习率调度器\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# 训练和评估\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ReduceLROnPlateau' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# 定义 LSTM 模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers, dropout, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # 取最后一个时间步的输出\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df, target_column, sequence_length):\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # 将数据转换为 LSTM 的输入格式 (batch_size, sequence_length, input_size)\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    if num_features % sequence_length != 0:\n",
    "        raise ValueError(\"特征数不能被序列长度整除，请调整 sequence_length。\")\n",
    "    input_size = num_features // sequence_length\n",
    "    X = X.reshape(num_samples, sequence_length, input_size)\n",
    "    return X, y\n",
    "\n",
    "# 模型训练函数\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 模型评估函数\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]  # 取正类的概率\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, \"\n",
    "          f\"F1: {f1:.4f}, ROC_AUC: {roc_auc:.4f}\")\n",
    "    return accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 加载新数据集\n",
    "    target_column = 'match_flag'  # 根据新数据集调整目标列名\n",
    "    sequence_length = 7         # 根据新数据集调整序列长度\n",
    "    df = pd.read_csv('cleaned_blood_gas_data.csv')\n",
    "    # df = pd.read_csv('cleaned_aki_urine_labs_sampled_with_flags.csv')  # 替换为新的数据集路径\n",
    "    X, y = preprocess_data(df, target_column, sequence_length)\n",
    "\n",
    "    # 数据集划分\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # 数据加载器\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载保存的模型\n",
    "    saved_model_path = \"best_lstm_model.pth\"\n",
    "    if not os.path.exists(saved_model_path):\n",
    "        raise FileNotFoundError(f\"Saved model not found at {saved_model_path}\")\n",
    "    \n",
    "    # 重新加载模型\n",
    "    input_size = X_train.shape[2]\n",
    "    hidden_dim = 236  # 从保存的模型中获取\n",
    "    num_layers = 1    # 从保存的模型中获取\n",
    "    dropout = 0.0     # 从保存的模型中获取\n",
    "    output_dim = 2    # 从保存的模型中获取\n",
    "    model = LSTMModel(input_size, hidden_dim, num_layers, dropout, output_dim).to(device)\n",
    "    \n",
    "    # 加载保存的权重\n",
    "    saved_state_dict = torch.load(saved_model_path)\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # 部分迁移权重\n",
    "    for name, param in saved_state_dict.items():\n",
    "        if name in model_dict and param.shape == model_dict[name].shape:\n",
    "            model_dict[name] = param\n",
    "        else:\n",
    "            print(f\"Skipping weight {name} due to shape mismatch.\")\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    # 再训练\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    train_model(model, train_loader, criterion, optimizer, device, epochs=10)\n",
    "\n",
    "    # 评估模型\n",
    "    evaluate_model(model, test_loader, device)\n",
    "\n",
    "    # 保存新的模型\n",
    "    # torch.save(model.state_dict(), \"new_best_lstm_model.pth\")\n",
    "    # print(\"Updated model saved as 'new_best_lstm_model.pth'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca1c5de-e771-4a4b-bde3-1b1d95e687b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0: torch.Size([944, 4])\n",
      "lstm.weight_hh_l0: torch.Size([944, 236])\n",
      "lstm.bias_ih_l0: torch.Size([944])\n",
      "lstm.bias_hh_l0: torch.Size([944])\n",
      "fc.weight: torch.Size([2, 236])\n",
      "fc.bias: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "saved_state_dict = torch.load('best_lstm_model.pth')\n",
    "for key, value in saved_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e9107-8645-475f-a13b-533f9f73558b",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48cd3d35-ddcc-4fdd-966b-561005775fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([40000, 1])) must be the same as input size (torch.Size([]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     97\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data1, data2)\n\u001b[1;32m---> 98\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    100\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:713\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\functional.py:2958\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2955\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   2957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 2958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m   2960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([40000, 1])) must be the same as input size (torch.Size([]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# 检测设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 定义单个 GAT 模块\n",
    "class GATBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):\n",
    "        super(GATBlock, self).__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.gat2 = GATConv(hidden_channels * heads, out_channels, heads=heads, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gat1(x, edge_index))\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 定义多模态组合模型\n",
    "class CombinedGATModel(nn.Module):\n",
    "    def __init__(self, in_channels1, in_channels2, hidden_channels, out_channels, heads=1):\n",
    "        super(CombinedGATModel, self).__init__()\n",
    "        self.gat1 = GATBlock(in_channels1, hidden_channels, out_channels, heads)\n",
    "        self.gat2 = GATBlock(in_channels2, hidden_channels, out_channels, heads)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        x1 = self.gat1(data1.x, data1.edge_index)\n",
    "        x2 = self.gat2(data2.x, data2.edge_index)\n",
    "        combined = torch.cat([x1.mean(dim=0), x2.mean(dim=0)], dim=0)  # 节点特征聚合\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "\n",
    "# 数据加载函数\n",
    "def load_graph_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    x = torch.tensor(df.iloc[:, :-1].values, dtype=torch.float32)\n",
    "    y = torch.tensor(df['match_flag'].values, dtype=torch.float32)\n",
    "    edge_index = torch.randint(0, x.size(0), (2, x.size(0) * 2))  # 随机生成边\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    data = load_graph_data(file_path)\n",
    "    num_nodes = data.x.size(0)\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_indices, test_indices = train_test_split(range(num_nodes), test_size=0.3, random_state=42)\n",
    "    train_mask[train_indices] = True\n",
    "    test_mask[test_indices] = True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.test_mask = test_mask\n",
    "    return data\n",
    "\n",
    "# 加载两个数据集\n",
    "data1 = prepare_data('cleaned_aki_data1.csv')\n",
    "data2 = prepare_data('cleaned_aki_data2.csv')\n",
    "\n",
    "train_loader1 = DataLoader([data1], batch_size=1, shuffle=False)\n",
    "train_loader2 = DataLoader([data2], batch_size=1, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "in_channels = data1.x.shape[1]\n",
    "hidden_channels = 16\n",
    "out_channels = 32\n",
    "model = CombinedGATModel(in_channels, in_channels, hidden_channels, out_channels, heads=2).to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 100\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data1, data2 in zip(train_loader1, train_loader2):\n",
    "        data1, data2 = data1[0].to(device), data2[0].to(device)  # data1 和 data2 是 Data 对象\n",
    "        labels = data1.y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data1, data2)\n",
    "        loss = loss_fn(output.squeeze(), labels.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # 评估模型性能\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for data1, data2 in zip(train_loader1, train_loader2):\n",
    "            data1, data2 = data1[0].to(device), data2[0].to(device)\n",
    "            labels = data1.y.to(device)\n",
    "\n",
    "            outputs = model(data1, data2).squeeze()\n",
    "            predictions = torch.sigmoid(outputs) > 0.5\n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "            y_pred.extend(predictions.cpu().tolist())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Accuracy after Epoch {epoch+1}: {accuracy:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "model_path = 'gat_model_combined.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n",
    "\n",
    "# 可视化损失与精度\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), losses, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), accuracies, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582b3b5b-9c8b-4185-8dc5-cf2ff39f3526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 0.6933, Val Loss: 0.6930\n",
      "Accuracy: 0.5000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, AUC: 0.5764\n",
      "Epoch 2:\n",
      "Train Loss: 0.6929, Val Loss: 0.6921\n",
      "Accuracy: 0.5333, Precision: 0.6406, Recall: 0.1518, F1: 0.2455, AUC: 0.5746\n",
      "Epoch 3:\n",
      "Train Loss: 0.6922, Val Loss: 0.6916\n",
      "Accuracy: 0.5337, Precision: 0.5199, Recall: 0.8813, F1: 0.6540, AUC: 0.5735\n",
      "Epoch 4:\n",
      "Train Loss: 0.6917, Val Loss: 0.6908\n",
      "Accuracy: 0.5333, Precision: 0.5254, Recall: 0.6878, F1: 0.5957, AUC: 0.5771\n",
      "Epoch 5:\n",
      "Train Loss: 0.6907, Val Loss: 0.6901\n",
      "Accuracy: 0.5580, Precision: 0.6023, Recall: 0.3415, F1: 0.4359, AUC: 0.5786\n",
      "Epoch 6:\n",
      "Train Loss: 0.6902, Val Loss: 0.6892\n",
      "Accuracy: 0.5582, Precision: 0.5641, Recall: 0.5118, F1: 0.5367, AUC: 0.5780\n",
      "Epoch 7:\n",
      "Train Loss: 0.6892, Val Loss: 0.6882\n",
      "Accuracy: 0.5367, Precision: 0.5298, Recall: 0.6520, F1: 0.5846, AUC: 0.5774\n",
      "Epoch 8:\n",
      "Train Loss: 0.6882, Val Loss: 0.6871\n",
      "Accuracy: 0.5482, Precision: 0.5445, Recall: 0.5897, F1: 0.5662, AUC: 0.5794\n",
      "Epoch 9:\n",
      "Train Loss: 0.6870, Val Loss: 0.6860\n",
      "Accuracy: 0.5607, Precision: 0.5675, Recall: 0.5108, F1: 0.5377, AUC: 0.5816\n",
      "Epoch 10:\n",
      "Train Loss: 0.6856, Val Loss: 0.6848\n",
      "Accuracy: 0.5457, Precision: 0.5394, Recall: 0.6253, F1: 0.5792, AUC: 0.5828\n",
      "Epoch 11:\n",
      "Train Loss: 0.6848, Val Loss: 0.6835\n",
      "Accuracy: 0.5597, Precision: 0.5644, Recall: 0.5240, F1: 0.5434, AUC: 0.5858\n",
      "Epoch 12:\n",
      "Train Loss: 0.6838, Val Loss: 0.6822\n",
      "Accuracy: 0.5548, Precision: 0.5509, Recall: 0.5935, F1: 0.5714, AUC: 0.5879\n",
      "Epoch 13:\n",
      "Train Loss: 0.6826, Val Loss: 0.6810\n",
      "Accuracy: 0.5637, Precision: 0.5696, Recall: 0.5212, F1: 0.5443, AUC: 0.5919\n",
      "Epoch 14:\n",
      "Train Loss: 0.6813, Val Loss: 0.6799\n",
      "Accuracy: 0.5610, Precision: 0.5547, Recall: 0.6187, F1: 0.5849, AUC: 0.5951\n",
      "Epoch 15:\n",
      "Train Loss: 0.6802, Val Loss: 0.6796\n",
      "Accuracy: 0.5713, Precision: 0.6054, Recall: 0.4098, F1: 0.4888, AUC: 0.5998\n",
      "Epoch 16:\n",
      "Train Loss: 0.6809, Val Loss: 0.6798\n",
      "Accuracy: 0.5653, Precision: 0.5488, Recall: 0.7343, F1: 0.6281, AUC: 0.6012\n",
      "Epoch 17:\n",
      "Train Loss: 0.6813, Val Loss: 0.6772\n",
      "Accuracy: 0.5764, Precision: 0.5880, Recall: 0.5108, F1: 0.5467, AUC: 0.6048\n",
      "Epoch 18:\n",
      "Train Loss: 0.6777, Val Loss: 0.6776\n",
      "Accuracy: 0.5724, Precision: 0.6077, Recall: 0.4087, F1: 0.4887, AUC: 0.6076\n",
      "Epoch 19:\n",
      "Train Loss: 0.6783, Val Loss: 0.6764\n",
      "Accuracy: 0.5770, Precision: 0.5678, Recall: 0.6452, F1: 0.6040, AUC: 0.6092\n",
      "Epoch 20:\n",
      "Train Loss: 0.6774, Val Loss: 0.6757\n",
      "Accuracy: 0.5801, Precision: 0.5707, Recall: 0.6465, F1: 0.6062, AUC: 0.6113\n",
      "Epoch 21:\n",
      "Train Loss: 0.6764, Val Loss: 0.6752\n",
      "Accuracy: 0.5814, Precision: 0.6061, Recall: 0.4652, F1: 0.5264, AUC: 0.6138\n",
      "Epoch 22:\n",
      "Train Loss: 0.6769, Val Loss: 0.6740\n",
      "Accuracy: 0.5865, Precision: 0.5924, Recall: 0.5545, F1: 0.5728, AUC: 0.6155\n",
      "Epoch 23:\n",
      "Train Loss: 0.6747, Val Loss: 0.6743\n",
      "Accuracy: 0.5819, Precision: 0.5683, Recall: 0.6818, F1: 0.6199, AUC: 0.6172\n",
      "Epoch 24:\n",
      "Train Loss: 0.6756, Val Loss: 0.6729\n",
      "Accuracy: 0.5929, Precision: 0.6066, Recall: 0.5287, F1: 0.5650, AUC: 0.6195\n",
      "Epoch 25:\n",
      "Train Loss: 0.6743, Val Loss: 0.6727\n",
      "Accuracy: 0.5867, Precision: 0.6101, Recall: 0.4802, F1: 0.5374, AUC: 0.6216\n",
      "Epoch 26:\n",
      "Train Loss: 0.6742, Val Loss: 0.6730\n",
      "Accuracy: 0.5818, Precision: 0.5658, Recall: 0.7038, F1: 0.6273, AUC: 0.6233\n",
      "Epoch 27:\n",
      "Train Loss: 0.6744, Val Loss: 0.6709\n",
      "Accuracy: 0.5958, Precision: 0.6017, Recall: 0.5663, F1: 0.5835, AUC: 0.6253\n",
      "Epoch 28:\n",
      "Train Loss: 0.6723, Val Loss: 0.6710\n",
      "Accuracy: 0.5940, Precision: 0.6186, Recall: 0.4903, F1: 0.5470, AUC: 0.6271\n",
      "Epoch 29:\n",
      "Train Loss: 0.6729, Val Loss: 0.6706\n",
      "Accuracy: 0.5898, Precision: 0.5766, Recall: 0.6760, F1: 0.6224, AUC: 0.6288\n",
      "Epoch 30:\n",
      "Train Loss: 0.6713, Val Loss: 0.6692\n",
      "Accuracy: 0.5996, Precision: 0.6021, Recall: 0.5872, F1: 0.5945, AUC: 0.6303\n",
      "Epoch 31:\n",
      "Train Loss: 0.6707, Val Loss: 0.6694\n",
      "Accuracy: 0.5955, Precision: 0.6237, Recall: 0.4815, F1: 0.5435, AUC: 0.6318\n",
      "Epoch 32:\n",
      "Train Loss: 0.6708, Val Loss: 0.6697\n",
      "Accuracy: 0.5903, Precision: 0.5729, Recall: 0.7100, F1: 0.6341, AUC: 0.6334\n",
      "Epoch 33:\n",
      "Train Loss: 0.6711, Val Loss: 0.6675\n",
      "Accuracy: 0.6008, Precision: 0.6128, Recall: 0.5477, F1: 0.5784, AUC: 0.6347\n",
      "Epoch 34:\n",
      "Train Loss: 0.6692, Val Loss: 0.6673\n",
      "Accuracy: 0.5995, Precision: 0.6212, Recall: 0.5098, F1: 0.5601, AUC: 0.6362\n",
      "Epoch 35:\n",
      "Train Loss: 0.6679, Val Loss: 0.6683\n",
      "Accuracy: 0.5962, Precision: 0.5772, Recall: 0.7188, F1: 0.6403, AUC: 0.6375\n",
      "Epoch 36:\n",
      "Train Loss: 0.6690, Val Loss: 0.6664\n",
      "Accuracy: 0.6010, Precision: 0.6248, Recall: 0.5057, F1: 0.5590, AUC: 0.6386\n",
      "Epoch 37:\n",
      "Train Loss: 0.6680, Val Loss: 0.6653\n",
      "Accuracy: 0.6038, Precision: 0.6100, Recall: 0.5760, F1: 0.5925, AUC: 0.6398\n",
      "Epoch 38:\n",
      "Train Loss: 0.6668, Val Loss: 0.6655\n",
      "Accuracy: 0.6004, Precision: 0.5894, Recall: 0.6618, F1: 0.6235, AUC: 0.6411\n",
      "Epoch 39:\n",
      "Train Loss: 0.6673, Val Loss: 0.6650\n",
      "Accuracy: 0.6024, Precision: 0.6284, Recall: 0.5013, F1: 0.5577, AUC: 0.6421\n",
      "Epoch 40:\n",
      "Train Loss: 0.6668, Val Loss: 0.6644\n",
      "Accuracy: 0.6008, Precision: 0.5917, Recall: 0.6507, F1: 0.6198, AUC: 0.6434\n",
      "Epoch 41:\n",
      "Train Loss: 0.6661, Val Loss: 0.6634\n",
      "Accuracy: 0.6059, Precision: 0.6055, Recall: 0.6078, F1: 0.6067, AUC: 0.6445\n",
      "Epoch 42:\n",
      "Train Loss: 0.6644, Val Loss: 0.6634\n",
      "Accuracy: 0.6042, Precision: 0.6274, Recall: 0.5135, F1: 0.5648, AUC: 0.6455\n",
      "Epoch 43:\n",
      "Train Loss: 0.6648, Val Loss: 0.6647\n",
      "Accuracy: 0.6043, Precision: 0.5839, Recall: 0.7260, F1: 0.6473, AUC: 0.6469\n",
      "Epoch 44:\n",
      "Train Loss: 0.6651, Val Loss: 0.6639\n",
      "Accuracy: 0.6027, Precision: 0.6441, Recall: 0.4588, F1: 0.5359, AUC: 0.6473\n",
      "Epoch 45:\n",
      "Train Loss: 0.6656, Val Loss: 0.6623\n",
      "Accuracy: 0.6082, Precision: 0.5966, Recall: 0.6688, F1: 0.6306, AUC: 0.6484\n",
      "Epoch 46:\n",
      "Train Loss: 0.6635, Val Loss: 0.6613\n",
      "Accuracy: 0.6118, Precision: 0.6085, Recall: 0.6265, F1: 0.6174, AUC: 0.6491\n",
      "Epoch 47:\n",
      "Train Loss: 0.6633, Val Loss: 0.6621\n",
      "Accuracy: 0.6017, Precision: 0.6331, Recall: 0.4837, F1: 0.5484, AUC: 0.6496\n",
      "Epoch 48:\n",
      "Train Loss: 0.6638, Val Loss: 0.6632\n",
      "Accuracy: 0.6062, Precision: 0.5840, Recall: 0.7383, F1: 0.6522, AUC: 0.6509\n",
      "Epoch 49:\n",
      "Train Loss: 0.6648, Val Loss: 0.6619\n",
      "Accuracy: 0.6021, Precision: 0.6385, Recall: 0.4707, F1: 0.5419, AUC: 0.6510\n",
      "Epoch 50:\n",
      "Train Loss: 0.6631, Val Loss: 0.6598\n",
      "Accuracy: 0.6153, Precision: 0.6110, Recall: 0.6350, F1: 0.6228, AUC: 0.6520\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv  # 使用 GATv2\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df, target_column):\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "    return X, y\n",
    "\n",
    "# 构建 KNN 图\n",
    "def build_knn_graph(X, y, k=10):\n",
    "    # 使用 KNN 构建稀疏邻接矩阵\n",
    "    edge_index = kneighbors_graph(X, k, mode='connectivity', include_self=False)\n",
    "    edge_index = edge_index.nonzero()  # 转换为 COO 格式\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # 构建图数据\n",
    "    x = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    graph = Data(x=x, edge_index=edge_index, y=y)\n",
    "    return graph\n",
    "\n",
    "# 加载数据\n",
    "def load_data(file_path, target_column, batch_size=64, k=5):\n",
    "    df = pd.read_csv(file_path)\n",
    "    X, y = preprocess_data(df, target_column)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # 构建训练图和测试图\n",
    "    train_graph = build_knn_graph(X_train, y_train, k=k)\n",
    "    test_graph = build_knn_graph(X_test, y_test, k=k)\n",
    "    \n",
    "    # 构建 DataLoader\n",
    "    train_loader = GeometricDataLoader([train_graph], batch_size=batch_size, shuffle=True)\n",
    "    test_loader = GeometricDataLoader([test_graph], batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# GATv2 模型\n",
    "class GATv2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.1):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.conv1 = GATv2Conv(input_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATv2Conv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),  # 增加全连接层维度\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 检查输入数据的形状和类型\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(f\"Expected input x to have 2 dimensions, but got {x.dim()}\")\n",
    "        if edge_index.dim() != 2:\n",
    "            raise ValueError(f\"Expected edge_index to have 2 dimensions, but got {edge_index.dim()}\")\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze(1)  # 输出 (num_nodes, )\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.x, data.edge_index)  # 单图训练\n",
    "        loss = criterion(outputs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 评估模型\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data.x, data.edge_index)  # 单图评估\n",
    "            loss = criterion(outputs, data.y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_pred_proba.extend(probs.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    return total_loss / len(test_loader), accuracy, precision, recall, f1, auc\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    file_path = 'cleaned_aki_data1.csv'  # 只使用一个 CSV 文件\n",
    "    target_column = 'match_flag'\n",
    "\n",
    "    # 加载数据\n",
    "    train_loader, test_loader = load_data(file_path, target_column, k=5)  # 使用 KNN 图，k=5\n",
    "\n",
    "    # 模型参数\n",
    "    input_dim = next(iter(train_loader))[0].x.shape[1]\n",
    "    hidden_dim = 128  # 增加隐藏层维度\n",
    "    output_dim = 1    # 输出维度\n",
    "    heads = 8         # GATv2 的注意力头数\n",
    "    dropout = 0.3\n",
    "    epochs = 50       # 增加训练轮数\n",
    "\n",
    "    # 模型初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GATv2(input_dim, hidden_dim, output_dim, heads=heads, dropout=dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)  # 学习率调度器\n",
    "\n",
    "    # 训练和评估\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, accuracy, precision, recall, f1, auc = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b56222-13e5-4cdf-92e1-bf0048c9f91f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 在第一个数据集上训练初始模型\u001b[39;00m\n\u001b[0;32m     64\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 在第一个数据集的验证集上评估模型\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m在第一个数据集的验证集上评估模型：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    607\u001b[0m         )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    250\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    258\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "# 定义评估模型的函数\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # 使用随机过采样处理类别不平衡\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    y_pred_resampled = model.predict(X_resampled)\n",
    "    y_pred_proba_resampled = model.predict_proba(X_resampled)[:, 1]\n",
    "\n",
    "    # 计算评估指标\n",
    "    fpr, tpr, thresholds = roc_curve(y_resampled, y_pred_proba_resampled)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    f1 = f1_score(y_resampled, y_pred_resampled)\n",
    "    precision = precision_score(y_resampled, y_pred_resampled)\n",
    "    recall = recall_score(y_resampled, y_pred_resampled)\n",
    "    mcc = matthews_corrcoef(y_resampled, y_pred_resampled)\n",
    "\n",
    "    # 计算校准曲线\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_resampled, y_pred_proba_resampled, n_bins=10)\n",
    "    reg = LinearRegression(fit_intercept=False).fit(mean_predicted_value.reshape(-1, 1), fraction_of_positives.reshape(-1, 1))\n",
    "    calibration_slope = reg.coef_[0][0]\n",
    "    calibration_intercept = reg.intercept_\n",
    "\n",
    "    print(f\"AUC: {auc_score:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, MCC: {mcc:.4f}\")\n",
    "    print(f\"Calibration Slope: {calibration_slope:.4f}, Calibration Intercept: {calibration_intercept:.4f}\")\n",
    "\n",
    "    return auc_score, f1, precision, recall, mcc, calibration_slope, calibration_intercept\n",
    "\n",
    "# 加载数据集\n",
    "file_path1 = 'cleaned_aki_data1.csv'\n",
    "file_path2 = 'cleaned_aki_data2.csv'\n",
    "target_column = 'match_flag'\n",
    "\n",
    "# 加载第一个数据集\n",
    "data1 = pd.read_csv(file_path1)\n",
    "X1 = data1.drop(columns=[target_column])  # 特征\n",
    "y1 = data1[target_column]  # 标签\n",
    "\n",
    "# 将第一个数据集分为训练集和验证集\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_train1 = scaler.fit_transform(X_train1)\n",
    "X_val1 = scaler.transform(X_val1)\n",
    "\n",
    "# 在第一个数据集上训练初始模型\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# 在第一个数据集的验证集上评估模型\n",
    "print(\"在第一个数据集的验证集上评估模型：\")\n",
    "evaluate_model(model, X_val1, y_val1)\n",
    "\n",
    "# 加载第二个数据集\n",
    "data2 = pd.read_csv(file_path2)\n",
    "X2 = data2.drop(columns=[target_column])  # 特征\n",
    "y2 = data2[target_column]  # 标签\n",
    "\n",
    "# 使用相同的标准化器对第二个数据集进行标准化\n",
    "X2 = scaler.transform(X2)\n",
    "\n",
    "# 在第二个数据集上继续训练模型\n",
    "model.fit(X2, y2)\n",
    "\n",
    "# 在第二个数据集上评估最终模型\n",
    "print(\"在第二个数据集上评估最终模型：\")\n",
    "evaluate_model(model, X2, y2)\n",
    "\n",
    "# 保存最终模型\n",
    "model_filename = 'final_aki_model.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(f\"最终模型已保存到 {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11f8a4ca-34a8-42c9-b804-78037f663750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26380\\2656226072.py:389: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  class_weights = 1. / class_counts\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 421\u001b[0m\n\u001b[0;32m    418\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 421\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 297\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    295\u001b[0m criterion1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39mcalc_pos_weight(y))\n\u001b[0;32m    296\u001b[0m scheduler1 \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer1, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m--> 297\u001b[0m model1, hist1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcriterion1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model1.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# 训练模型2\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Model 2...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 235\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs, save_path)\u001b[0m\n\u001b[0;32m    232\u001b[0m         y_probs\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39msigmoid(outputs)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# 计算指标\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(y_probs)\n\u001b[0;32m    237\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_true, preds)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:627\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    625\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[0;32m    626\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    636\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[0;32m    637\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score,\n",
    "                             recall_score, f1_score, roc_auc_score,\n",
    "                             roc_curve)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (DataLoader, TensorDataset,\n",
    "                              WeightedRandomSampler)\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 固定随机种子\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --------------------------\n",
    "# 时间序列增强模块\n",
    "# --------------------------\n",
    "class TemporalAugmentation:\n",
    "    \"\"\"时间序列数据增强\"\"\"\n",
    "    def __init__(self, sigma=0.1, p=0.5):\n",
    "        self.sigma = sigma  # 噪声强度\n",
    "        self.p = p  # 应用概率\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if np.random.rand() < self.p:\n",
    "            noise = torch.randn_like(x) * self.sigma\n",
    "            return x + noise\n",
    "        return x\n",
    "\n",
    "# --------------------------\n",
    "# 残差块模块（改进版）\n",
    "# --------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"带通道注意力机制的残差块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # 通道注意力机制\n",
    "        self.ca = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(out_channels, out_channels // 8, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels // 8, out_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # 通道注意力\n",
    "        ca_weight = self.ca(out)\n",
    "        out = out * ca_weight\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# --------------------------\n",
    "# 改进的CNN模型\n",
    "# --------------------------\n",
    "class DynamicCNN(nn.Module):\n",
    "    \"\"\"带数据增强和时间感知的CNN\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # 数据增强层\n",
    "        self.augment = Compose([TemporalAugmentation(sigma=0.05, p=0.3)])\n",
    "\n",
    "        # 特征预处理\n",
    "        self.preprocess = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        # 残差卷积模块\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 3, padding=1),\n",
    "            ResidualBlock(16, 16),\n",
    "            nn.MaxPool1d(2),\n",
    "            ResidualBlock(16, 32),\n",
    "            nn.AdaptiveAvgPool1d(8)\n",
    "        )\n",
    "\n",
    "        # 动态计算全连接输入维度\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(2, input_dim)\n",
    "            dummy = self.preprocess(dummy).unsqueeze(1)\n",
    "            dummy = self.conv_layers(dummy)\n",
    "            self.fc_input = dummy.view(dummy.size(0), -1).shape[1]\n",
    "\n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.fc_input, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, augment=True):\n",
    "        if self.training and augment:\n",
    "            x = self.augment(x)\n",
    "        x = self.preprocess(x).unsqueeze(1)\n",
    "        features = self.conv_layers(x).view(x.size(0), -1)\n",
    "        return self.classifier(features).squeeze(1)\n",
    "\n",
    "# --------------------------\n",
    "# 注意力融合模块\n",
    "# --------------------------\n",
    "class AttentionFusion(nn.Module):\n",
    "    \"\"\"基于注意力的模型融合\"\"\"\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super().__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        with torch.no_grad():\n",
    "            logitA = self.modelA(x1)\n",
    "            logitB = self.modelB(x2)\n",
    "        concat_logits = torch.stack([logitA, logitB], dim=1)\n",
    "        weights = self.attention(concat_logits)\n",
    "        probA = torch.sigmoid(logitA)\n",
    "        probB = torch.sigmoid(logitB)\n",
    "        return (weights[:, 0] * probA) + (weights[:, 1] * probB)\n",
    "\n",
    "# --------------------------\n",
    "# 模型解释模块\n",
    "# --------------------------\n",
    "def feature_importance(model, X, feature_names, n_samples=1000):\n",
    "    model.eval()\n",
    "    baseline = torch.mean(X, dim=0, keepdim=True)\n",
    "    delta_list = []\n",
    "\n",
    "    # 确保特征数量与特征名称数量一致\n",
    "    assert X.shape[1] == len(feature_names), \"特征数量与特征名称不匹配\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(X.shape[1])):\n",
    "            perturbed = X.clone()\n",
    "            perturbed[:, i] = baseline[0, i]\n",
    "            orig_output = torch.sigmoid(model(X))\n",
    "            perturbed_output = torch.sigmoid(model(perturbed))\n",
    "            delta = torch.mean(torch.abs(orig_output - perturbed_output)).item()\n",
    "            delta_list.append(delta)\n",
    "\n",
    "    # 动态确定显示数量\n",
    "    display_num = min(20, len(delta_list))  # 取特征数量和前20中的较小值\n",
    "    indices = np.argsort(delta_list)[::-1][:display_num]  # 只取实际存在的索引\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(display_num), [delta_list[i] for i in indices][::-1])\n",
    "    plt.yticks(range(display_num), [feature_names[i] for i in indices][::-1])\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.title(f'Top {display_num} Important Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 训练评估模块（优化版）\n",
    "# --------------------------\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, criterion,\n",
    "                       scheduler=None, epochs=30, save_path='best_model.pth'):\n",
    "    history = {'train_loss': [], 'val_auc': [], 'val_f1': [],\n",
    "               'val_accuracy': [], 'val_precision': []}\n",
    "    best_auc = 0\n",
    "    early_stop = EarlyStopper(patience=10)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        y_true, y_probs = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(device)\n",
    "                outputs = model(X)\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_probs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "        # 计算指标\n",
    "        auc = roc_auc_score(y_true, y_probs)\n",
    "        preds = np.round(y_probs)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        accuracy = accuracy_score(y_true, preds)\n",
    "        precision = precision_score(y_true, preds)\n",
    "\n",
    "        history['val_auc'].append(auc)\n",
    "        history['val_f1'].append(f1)\n",
    "        history['val_accuracy'].append(accuracy)\n",
    "        history['val_precision'].append(precision)\n",
    "\n",
    "        # 学习率调度\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(auc)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val AUC: {auc:.4f} | F1: {f1:.4f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved new best model with AUC: {auc:.4f}\")\n",
    "\n",
    "        if early_stop(auc):\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model, history\n",
    "\n",
    "# --------------------------\n",
    "# 主流程\n",
    "# --------------------------\n",
    "def main():\n",
    "    # 加载数据\n",
    "    df1 = pd.read_csv('./data/cleaned_jigan.csv')\n",
    "    df2 = pd.read_csv('./data/cleaned_labs_first_day.csv')\n",
    "    target = 'match_flag'\n",
    "\n",
    "    # 数据对齐\n",
    "    min_len = min(len(df1), len(df2))\n",
    "    df1 = df1.iloc[:min_len].reset_index(drop=True)\n",
    "    df2 = df2.iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "    # 特征工程\n",
    "    X1 = df1.drop(columns=target).values.astype(np.float32)\n",
    "    X2 = df2.drop(columns=target).values.astype(np.float32)\n",
    "    y = df1[target].values.astype(np.float32)\n",
    "\n",
    "    # 训练模型1\n",
    "    print(\"\\nTraining Model 1...\")\n",
    "    dataset1 = TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(y))\n",
    "    train_loader1, val_loader1 = create_loaders(dataset1)\n",
    "    model1 = DynamicCNN(X1.shape[1]).to(device)\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion1 = nn.BCEWithLogitsLoss(pos_weight=calc_pos_weight(y))\n",
    "    scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer1, mode='max', factor=0.1, patience=3)\n",
    "    model1, hist1 = train_and_evaluate(model1, train_loader1, val_loader1, optimizer1,\n",
    "                                     criterion1, scheduler=scheduler1, save_path='best_model1.pth')\n",
    "\n",
    "    # 训练模型2\n",
    "    print(\"\\nTraining Model 2...\")\n",
    "    dataset2 = TensorDataset(torch.FloatTensor(X2), torch.FloatTensor(y))\n",
    "    train_loader2, val_loader2 = create_loaders(dataset2)\n",
    "    model2 = DynamicCNN(X2.shape[1]).to(device)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion2 = nn.BCEWithLogitsLoss(pos_weight=calc_pos_weight(y))\n",
    "    scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='max', factor=0.1, patience=3)\n",
    "    model2, hist2 = train_and_evaluate(model2, train_loader2, val_loader2, optimizer2,\n",
    "                                     criterion2, scheduler=scheduler2, save_path='best_model2.pth')\n",
    "\n",
    "    # 加载最佳模型\n",
    "    model1.load_state_dict(torch.load('best_model1.pth'))\n",
    "    model2.load_state_dict(torch.load('best_model2.pth'))\n",
    "\n",
    "    # 训练融合模型\n",
    "    print(\"\\nTraining Fusion Model...\")\n",
    "    fusion_model = AttentionFusion(model1, model2).to(device)\n",
    "    optimizer = optim.Adam(fusion_model.parameters(), lr=0.001)\n",
    "    scheduler_fusion = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    dataset = TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(X2), torch.FloatTensor(y))\n",
    "    train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        fusion_model.train()\n",
    "        for X1_batch, X2_batch, y_batch in train_loader:\n",
    "            X1_batch, X2_batch = X1_batch.to(device), X2_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            probs = fusion_model(X1_batch, X2_batch)\n",
    "            loss = nn.BCELoss()(probs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler_fusion.step()\n",
    "        print(f\"Epoch {epoch + 1}: Loss={total_loss / len(train_loader):.4f}, LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # 模型解释与评估\n",
    "    print(\"\\nFeature Importance for Model 1:\")\n",
    "    feature_importance(model1, torch.FloatTensor(X1[:1000]).to(device), df1.drop(columns=target).columns.tolist())\n",
    "    print(\"\\nFeature Importance for Model 2:\")\n",
    "    feature_importance(model2, torch.FloatTensor(X2[:1000]).to(device), df2.drop(columns=target).columns.tolist())\n",
    "    evaluate_ensemble(fusion_model, X1, X2, y)\n",
    "\n",
    "# --------------------------\n",
    "# 辅助函数\n",
    "# --------------------------\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0.005):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "def create_loaders(dataset, val_ratio=0.2):\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    y_train = dataset[train_dataset.indices][1].numpy()\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=calc_sample_weights(y_train),\n",
    "        num_samples=len(train_dataset),\n",
    "        replacement=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def calc_pos_weight(y):\n",
    "    pos = np.sum(y)\n",
    "    neg = len(y) - pos\n",
    "    return torch.tensor([neg / pos]).to(device) if pos > 0 else torch.tensor([1.0]).to(device)\n",
    "\n",
    "def calc_sample_weights(y):\n",
    "    class_counts = np.bincount(y.astype(int))\n",
    "    class_weights = 1. / class_counts\n",
    "    return torch.tensor([class_weights[int(label)] for label in y])\n",
    "\n",
    "def evaluate_ensemble(model, X1, X2, y):\n",
    "    loader = DataLoader(TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(X2)), batch_size=256)\n",
    "    model.eval()\n",
    "    probs, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for X1_batch, X2_batch in loader:\n",
    "            X1_batch, X2_batch = X1_batch.to(device), X2_batch.to(device)\n",
    "            batch_probs = model(X1_batch, X2_batch).cpu().numpy()\n",
    "            probs.extend(batch_probs)\n",
    "            truths.extend(y[:len(X1_batch)])\n",
    "    preds = np.round(probs)\n",
    "    print(\"\\nFinal Ensemble Performance:\")\n",
    "    print(f\"AUC: {roc_auc_score(truths, probs):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(truths, preds):.4f}\")\n",
    "    print(f\"Precision: {precision_score(truths, preds):.4f}\")\n",
    "    print(f\"Recall: {recall_score(truths, preds):.4f}\")\n",
    "    print(f\"F1: {f1_score(truths, preds):.4f}\")\n",
    "    # 绘制ROC曲线\n",
    "    fpr, tpr, _ = roc_curve(truths, probs)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(truths, probs):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3435343-bd68-480f-99be-3c0f2aea0b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 1...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m criterion1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39mcalc_pos_weight(y))\n\u001b[0;32m     23\u001b[0m scheduler1 \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer1, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m model1, hist1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcriterion1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model1.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 训练模型2\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Model 2...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 235\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs, save_path)\u001b[0m\n\u001b[0;32m    232\u001b[0m         y_probs\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39msigmoid(outputs)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# 计算指标\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(y_probs)\n\u001b[0;32m    237\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_true, preds)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:606\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    604\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 606\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    609\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    610\u001b[0m ):\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_fpr \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "    # 加载数据\n",
    "    df1 = pd.read_csv('./data/cleaned_jigan1.csv')\n",
    "    df2 = pd.read_csv('./data/cleaned_labs_first_day1.csv')\n",
    "    target = 'match_flag'\n",
    "\n",
    "    # 数据对齐\n",
    "    min_len = min(len(df1), len(df2))\n",
    "    df1 = df1.iloc[:min_len].reset_index(drop=True)\n",
    "    df2 = df2.iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "    # 特征工程\n",
    "    X1 = df1.drop(columns=target).values.astype(np.float32)\n",
    "    X2 = df2.drop(columns=target).values.astype(np.float32)\n",
    "    y = df1[target].values.astype(np.float32)\n",
    "\n",
    "    # 训练模型1\n",
    "    print(\"\\nTraining Model 1...\")\n",
    "    dataset1 = TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(y))\n",
    "    train_loader1, val_loader1 = create_loaders(dataset1)\n",
    "    model1 = DynamicCNN(X1.shape[1]).to(device)\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion1 = nn.BCEWithLogitsLoss(pos_weight=calc_pos_weight(y))\n",
    "    scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer1, mode='max', factor=0.1, patience=3)\n",
    "    model1, hist1 = train_and_evaluate(model1, train_loader1, val_loader1, optimizer1,\n",
    "                                     criterion1, scheduler=scheduler1, save_path='best_model1.pth')\n",
    "\n",
    "    # 训练模型2\n",
    "    print(\"\\nTraining Model 2...\")\n",
    "    dataset2 = TensorDataset(torch.FloatTensor(X2), torch.FloatTensor(y))\n",
    "    train_loader2, val_loader2 = create_loaders(dataset2)\n",
    "    model2 = DynamicCNN(X2.shape[1]).to(device)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion2 = nn.BCEWithLogitsLoss(pos_weight=calc_pos_weight(y))\n",
    "    scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='max', factor=0.1, patience=3)\n",
    "    model2, hist2 = train_and_evaluate(model2, train_loader2, val_loader2, optimizer2,\n",
    "                                     criterion2, scheduler=scheduler2, save_path='best_model2.pth')\n",
    "\n",
    "    # 加载最佳模型\n",
    "    model1.load_state_dict(torch.load('best_model1.pth'))\n",
    "    model2.load_state_dict(torch.load('best_model2.pth'))\n",
    "\n",
    "    # 训练融合模型\n",
    "    print(\"\\nTraining Fusion Model...\")\n",
    "    fusion_model = AttentionFusion(model1, model2).to(device)\n",
    "    optimizer = optim.Adam(fusion_model.parameters(), lr=0.001)\n",
    "    scheduler_fusion = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    dataset = TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(X2), torch.FloatTensor(y))\n",
    "    train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        fusion_model.train()\n",
    "        for X1_batch, X2_batch, y_batch in train_loader:\n",
    "            X1_batch, X2_batch = X1_batch.to(device), X2_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            probs = fusion_model(X1_batch, X2_batch)\n",
    "            loss = nn.BCELoss()(probs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler_fusion.step()\n",
    "        print(f\"Epoch {epoch + 1}: Loss={total_loss / len(train_loader):.4f}, LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # 模型解释与评估\n",
    "    print(\"\\nFeature Importance for Model 1:\")\n",
    "    feature_importance(model1, torch.FloatTensor(X1[:1000]).to(device), df1.drop(columns=target).columns.tolist())\n",
    "    print(\"\\nFeature Importance for Model 2:\")\n",
    "    feature_importance(model2, torch.FloatTensor(X2[:1000]).to(device), df2.drop(columns=target).columns.tolist())\n",
    "    evaluate_ensemble(fusion_model, X1, X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de48e1c1-8f8d-41a3-966c-b8177e7eb32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(model, X1, X2, y):\n",
    "    loader = DataLoader(TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(X2)),\n",
    "                        batch_size=256, shuffle=False)  # 必须关闭shuffle\n",
    "    \n",
    "    model.eval()\n",
    "    probs, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X1_batch, X2_batch) in enumerate(loader):\n",
    "            X1_batch, X2_batch = X1_batch.to(device), X2_batch.to(device)\n",
    "            batch_probs = model(X1_batch, X2_batch).cpu().numpy()\n",
    "            probs.extend(batch_probs)\n",
    "            \n",
    "            # 正确获取对应batch的标签\n",
    "            start_idx = batch_idx * loader.batch_size\n",
    "            end_idx = start_idx + len(X1_batch)\n",
    "            truths.extend(y[start_idx:end_idx])  # 按位置精确截取\n",
    "    \n",
    "    # 转换为numpy数组并验证\n",
    "    truths = np.array(truths)\n",
    "    probs = np.array(probs)\n",
    "    print(f\"\\n最终验证集类别分布 - 负类: {np.sum(truths==0)}, 正类: {np.sum(truths==1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae01a580-09eb-47f6-acaa-608f69639d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "警告：验证集只包含单一类别，无法计算AUC\n",
      "类别分布: {0: 0, 1: 450420}\n"
     ]
    }
   ],
   "source": [
    "evaluate_ensemble(fusion_model, X1, X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1f867f-9072-4f6c-82e0-0c1039bf9d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据类别分布检查：\n",
      "总样本数: 450420\n",
      "正类比例: 50.00%\n",
      "负类数量: 225210\n",
      "正类数量: 225210\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed4baff4-4aaf-4f85-ae9a-e71151178f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['patient_id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 460\u001b[0m\n\u001b[0;32m    457\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 460\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 285\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    282\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df2[df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhadm_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(common_ids)]\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhadm_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# 特征工程\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m X1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatient_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# 移除标识列\u001b[39;00m\n\u001b[0;32m    286\u001b[0m X2 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[target, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    287\u001b[0m y \u001b[38;5;241m=\u001b[39m df1[target]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5260\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5265\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['patient_id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score,\n",
    "                             recall_score, f1_score, roc_auc_score,\n",
    "                             roc_curve)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (DataLoader, TensorDataset,\n",
    "                              WeightedRandomSampler)\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 固定随机种子\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --------------------------\n",
    "# 时间序列增强模块\n",
    "# --------------------------\n",
    "class TemporalAugmentation:\n",
    "    \"\"\"时间序列数据增强\"\"\"\n",
    "    def __init__(self, sigma=0.1, p=0.5):\n",
    "        self.sigma = sigma  # 噪声强度\n",
    "        self.p = p  # 应用概率\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if np.random.rand() < self.p:\n",
    "            noise = torch.randn_like(x) * self.sigma\n",
    "            return x + noise\n",
    "        return x\n",
    "\n",
    "# --------------------------\n",
    "# 残差块模块（改进版）\n",
    "# --------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"带通道注意力机制的残差块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # 通道注意力机制\n",
    "        self.ca = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(out_channels, out_channels // 8, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels // 8, out_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # 通道注意力\n",
    "        ca_weight = self.ca(out)\n",
    "        out = out * ca_weight\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# --------------------------\n",
    "# 改进的CNN模型\n",
    "# --------------------------\n",
    "class DynamicCNN(nn.Module):\n",
    "    \"\"\"带数据增强和时间感知的CNN\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # 数据增强层\n",
    "        self.augment = Compose([TemporalAugmentation(sigma=0.05, p=0.3)])\n",
    "\n",
    "        # 特征预处理\n",
    "        self.preprocess = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        # 残差卷积模块\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 3, padding=1),\n",
    "            ResidualBlock(16, 16),\n",
    "            nn.MaxPool1d(2),\n",
    "            ResidualBlock(16, 32),\n",
    "            nn.AdaptiveAvgPool1d(8)\n",
    "        )\n",
    "\n",
    "        # 动态计算全连接输入维度\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(2, input_dim)\n",
    "            dummy = self.preprocess(dummy).unsqueeze(1)\n",
    "            dummy = self.conv_layers(dummy)\n",
    "            self.fc_input = dummy.view(dummy.size(0), -1).shape[1]\n",
    "\n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.fc_input, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, augment=True):\n",
    "        if self.training and augment:\n",
    "            x = self.augment(x)\n",
    "        x = self.preprocess(x).unsqueeze(1)\n",
    "        features = self.conv_layers(x).view(x.size(0), -1)\n",
    "        return self.classifier(features).squeeze(1)\n",
    "\n",
    "# --------------------------\n",
    "# 注意力融合模块\n",
    "# --------------------------\n",
    "class AttentionFusion(nn.Module):\n",
    "    \"\"\"基于注意力的模型融合\"\"\"\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super().__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        with torch.no_grad():\n",
    "            logitA = self.modelA(x1)\n",
    "            logitB = self.modelB(x2)\n",
    "        concat_logits = torch.stack([logitA, logitB], dim=1)\n",
    "        weights = self.attention(concat_logits)\n",
    "        probA = torch.sigmoid(logitA)\n",
    "        probB = torch.sigmoid(logitB)\n",
    "        return (weights[:, 0] * probA) + (weights[:, 1] * probB)\n",
    "\n",
    "# --------------------------\n",
    "# 模型解释模块\n",
    "# --------------------------\n",
    "def feature_importance(model, X, feature_names, n_samples=1000):\n",
    "    model.eval()\n",
    "    baseline = torch.mean(X, dim=0, keepdim=True)\n",
    "    delta_list = []\n",
    "    \n",
    "    # 确保特征数量与特征名称数量一致\n",
    "    assert X.shape[1] == len(feature_names), \"特征数量与特征名称不匹配\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(X.shape[1])):\n",
    "            perturbed = X.clone()\n",
    "            perturbed[:, i] = baseline[0, i]\n",
    "            orig_output = torch.sigmoid(model(X))\n",
    "            perturbed_output = torch.sigmoid(model(perturbed))\n",
    "            delta = torch.mean(torch.abs(orig_output - perturbed_output)).item()\n",
    "            delta_list.append(delta)\n",
    "\n",
    "    # 动态确定显示数量\n",
    "    display_num = min(20, len(delta_list))  # 取特征数量和前20中的较小值\n",
    "    indices = np.argsort(delta_list)[::-1][:display_num]  # 只取实际存在的索引\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(display_num), [delta_list[i] for i in indices][::-1])\n",
    "    plt.yticks(range(display_num), [feature_names[i] for i in indices][::-1])\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.title(f'Top {display_num} Important Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 训练评估模块（优化版）\n",
    "# --------------------------\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, criterion,\n",
    "                       scheduler=None, epochs=30, save_path='best_model.pth'):\n",
    "    history = {'train_loss': [], 'val_auc': [], 'val_f1': [],\n",
    "               'val_accuracy': [], 'val_precision': []}\n",
    "    best_auc = 0\n",
    "    early_stop = EarlyStopper(patience=10)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        y_true, y_probs = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(device)\n",
    "                outputs = model(X)\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_probs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "        # 计算指标\n",
    "        auc = roc_auc_score(y_true, y_probs)\n",
    "        preds = np.round(y_probs)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        accuracy = accuracy_score(y_true, preds)\n",
    "        precision = precision_score(y_true, preds)\n",
    "\n",
    "        history['val_auc'].append(auc)\n",
    "        history['val_f1'].append(f1)\n",
    "        history['val_accuracy'].append(accuracy)\n",
    "        history['val_precision'].append(precision)\n",
    "\n",
    "        # 学习率调度\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(auc)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val AUC: {auc:.4f} | F1: {f1:.4f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved new best model with AUC: {auc:.4f}\")\n",
    "\n",
    "        if early_stop(auc):\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model, history\n",
    "\n",
    "# --------------------------\n",
    "# 主流程\n",
    "# --------------------------\n",
    "def main():\n",
    "    # 加载数据\n",
    "    # 加载数据\n",
    "    df1 = pd.read_csv('./data/cleaned_jigan1.csv')\n",
    "    df2 = pd.read_csv('./data/cleaned_labs_first_day1.csv')\n",
    "    target = 'match_flag'\n",
    "\n",
    "    # 安全数据对齐（按patient_id对齐）\n",
    "    common_ids = np.intersect1d(df1['hadm_id'], df2['hadm_id'])  # 假设存在唯一标识列\n",
    "    df1 = df1[df1['hadm_id'].isin(common_ids)].sort_values('hadm_id').reset_index(drop=True)\n",
    "    df2 = df2[df2['hadm_id'].isin(common_ids)].sort_values('hadm_id').reset_index(drop=True)\n",
    "\n",
    "    # 特征工程\n",
    "    X1 = df1.drop(columns=[target, 'hadm_id']).values.astype(np.float32)  # 移除标识列\n",
    "    X2 = df2.drop(columns=[target, 'hadm_id']).values.astype(np.float32)\n",
    "    y = df1[target].values.astype(np.float32)\n",
    "\n",
    "    # 添加数据完整性检查\n",
    "    print(\"\\n数据完整性验证：\")\n",
    "    print(f\"X1样本数: {len(X1)}, X2样本数: {len(X2)}, y样本数: {len(y)}\")\n",
    "    print(f\"正类比例: {np.mean(y):.2%}\")\n",
    "    assert len(X1) == len(X2) == len(y), \"特征与标签数量不匹配\"\n",
    "\n",
    "    # 训练模型1\n",
    "    print(\"\\nTraining Model 1...\")\n",
    "    dataset1 = TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(y))\n",
    "    train_loader1, val_loader1 = create_loaders(dataset1)\n",
    "    model1 = DynamicCNN(X1.shape[1]).to(device)\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion1 = nn.BCEWithLogitsLoss(pos_weight=calc_pos_weight(y))\n",
    "    scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer1, mode='max', factor=0.1, patience=3)\n",
    "    model1, hist1 = train_and_evaluate(model1, train_loader1, val_loader1, optimizer1,\n",
    "                                     criterion1, scheduler=scheduler1, save_path='best_model1.pth')\n",
    "\n",
    "    # 训练模型2\n",
    "    print(\"\\nTraining Model 2...\")\n",
    "    dataset2 = TensorDataset(torch.FloatTensor(X2), torch.FloatTensor(y))\n",
    "    train_loader2, val_loader2 = create_loaders(dataset2)\n",
    "    model2 = DynamicCNN(X2.shape[1]).to(device)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion2 = nn.BCEWithLogitsLoss(pos_weight=calc_pos_weight(y))\n",
    "    scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='max', factor=0.1, patience=3)\n",
    "    model2, hist2 = train_and_evaluate(model2, train_loader2, val_loader2, optimizer2,\n",
    "                                     criterion2, scheduler=scheduler2, save_path='best_model2.pth')\n",
    "\n",
    "    # 加载最佳模型\n",
    "    model1.load_state_dict(torch.load('best_model1.pth'))\n",
    "    model2.load_state_dict(torch.load('best_model2.pth'))\n",
    "\n",
    "    # 训练融合模型\n",
    "    print(\"\\nTraining Fusion Model...\")\n",
    "    fusion_model = AttentionFusion(model1, model2).to(device)\n",
    "    optimizer = optim.Adam(fusion_model.parameters(), lr=0.001)\n",
    "    scheduler_fusion = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    dataset = TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(X2), torch.FloatTensor(y))\n",
    "    train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        fusion_model.train()\n",
    "        for X1_batch, X2_batch, y_batch in train_loader:\n",
    "            X1_batch, X2_batch = X1_batch.to(device), X2_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            probs = fusion_model(X1_batch, X2_batch)\n",
    "            loss = nn.BCELoss()(probs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler_fusion.step()\n",
    "        print(f\"Epoch {epoch + 1}: Loss={total_loss / len(train_loader):.4f}, LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # 模型解释与评估\n",
    "    print(\"\\nFeature Importance for Model 1:\")\n",
    "    feature_importance(model1, torch.FloatTensor(X1[:1000]).to(device), df1.drop(columns=target).columns.tolist())\n",
    "    print(\"\\nFeature Importance for Model 2:\")\n",
    "    feature_importance(model2, torch.FloatTensor(X2[:1000]).to(device), df2.drop(columns=target).columns.tolist())\n",
    "    evaluate_ensemble(fusion_model, X1, X2, y)\n",
    "\n",
    "# --------------------------\n",
    "# 辅助函数\n",
    "# --------------------------\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0.005):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "def create_loaders(dataset, val_ratio=0.2):\n",
    "    # 使用分层划分\n",
    "    y = dataset.tensors[1].numpy()\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_idx, val_idx = next(skf.split(np.zeros(len(y)), y))\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    \n",
    "    # 类别平衡采样\n",
    "    y_train = y[train_idx]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=calc_sample_weights(y_train),\n",
    "        num_samples=len(train_dataset),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def calc_pos_weight(y):\n",
    "    pos = np.sum(y)\n",
    "    neg = len(y) - pos\n",
    "    return torch.tensor([neg / pos]).to(device) if pos > 0 else torch.tensor([1.0]).to(device)\n",
    "\n",
    "def calc_sample_weights(y):\n",
    "    class_counts = np.bincount(y.astype(int))\n",
    "    class_weights = 1. / class_counts\n",
    "    return torch.tensor([class_weights[int(label)] for label in y])\n",
    "\n",
    "def evaluate_ensemble(model, X1, X2, y):\n",
    "    # 创建数据集时确保顺序一致\n",
    "    dataset = TensorDataset(torch.FloatTensor(X1), torch.FloatTensor(X2), torch.FloatTensor(y))\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False)  # 必须关闭shuffle\n",
    "    \n",
    "    model.eval()\n",
    "    probs, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for X1_batch, X2_batch, y_batch in loader:\n",
    "            X1_batch, X2_batch = X1_batch.to(device), X2_batch.to(device)\n",
    "            batch_probs = model(X1_batch, X2_batch).cpu().numpy()\n",
    "            probs.extend(batch_probs)\n",
    "            truths.extend(y_batch.cpu().numpy())  # 直接使用loader提供的标签\n",
    "    \n",
    "    # 转换为numpy数组并验证\n",
    "    truths = np.array(truths)\n",
    "    probs = np.array(probs)\n",
    "    print(f\"\\n最终验证集类别分布 - 负类: {np.sum(truths==0)}, 正类: {np.sum(truths==1)}\")\n",
    "    \n",
    "    # 检查类别分布\n",
    "    unique_classes = np.unique(truths)\n",
    "    if len(unique_classes) == 1:\n",
    "        print(\"\\n警告：验证集只包含单一类别，无法计算AUC\")\n",
    "        class_dist = {0: np.sum(truths==0), 1: np.sum(truths==1)}\n",
    "        print(f\"类别分布: {class_dist}\")\n",
    "        return\n",
    "    \n",
    "    # 计算评估指标\n",
    "    preds = np.round(probs)\n",
    "    print(\"\\nFinal Ensemble Performance:\")\n",
    "    try:\n",
    "        auc = roc_auc_score(truths, probs)\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"AUC计算失败: {str(e)}\")\n",
    "        auc = 0\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(truths, preds):.4f}\")\n",
    "    print(f\"Precision: {precision_score(truths, preds):.4f}\")\n",
    "    print(f\"Recall: {recall_score(truths, preds):.4f}\")\n",
    "    print(f\"F1: {f1_score(truths, preds):.4f}\")\n",
    "\n",
    "    # 绘制ROC曲线\n",
    "    if auc > 0:\n",
    "        fpr, tpr, _ = roc_curve(truths, probs)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc718c5b-4049-4fd9-90c2-ba752017cac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
